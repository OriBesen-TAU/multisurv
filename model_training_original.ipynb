{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Top'></a>\n",
    "\n",
    "# Multisurv model training<a class='tocSkip'></a>\n",
    "\n",
    "Train MultiSurv models with different combinations of input data modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> PyTorch detected CUDA <<<\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext watermark\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('>>> PyTorch detected CUDA <<<')\n",
    "\n",
    "# Make modules in \"src\" dir visible\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "import utils\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#DataLoader\" data-toc-modified-id=\"DataLoader-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span><code>DataLoader</code></a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Different-intervals\" data-toc-modified-id=\"Different-intervals-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Different intervals</a></span><ul class=\"toc-item\"><li><span><a href=\"#Equidistant-times\" data-toc-modified-id=\"Equidistant-times-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Equidistant times</a></span></li><li><span><a href=\"#By-duration-quantiles\" data-toc-modified-id=\"By-duration-quantiles-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>By duration quantiles</a></span></li></ul></li><li><span><a href=\"#Pick-learning-rate\" data-toc-modified-id=\"Pick-learning-rate-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Pick learning rate</a></span></li><li><span><a href=\"#Fit\" data-toc-modified-id=\"Fit-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Fit</a></span><ul class=\"toc-item\"><li><span><a href=\"#Save-model-weights\" data-toc-modified-id=\"Save-model-weights-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Save model weights</a></span></li></ul></li><li><span><a href=\"#Check-validation-metrics\" data-toc-modified-id=\"Check-validation-metrics-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Check validation metrics</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = utils.INPUT_DATA_DIR\n",
    "MODELS = utils.TRAINED_MODEL_DIR\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6ff94090f84a7586175654b3c63d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Input data', index=(0, 1), options=('clinical', 'mRNA', 'DNAm', 'miRNA', 'CNV', 'wâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_modalities = widgets.SelectMultiple(\n",
    "    options=['clinical', 'mRNA', 'DNAm', 'miRNA', 'CNV', 'wsi'],\n",
    "    index=[0, 1],\n",
    "    rows=6,\n",
    "    description='Input data',\n",
    "    disabled=False\n",
    ")\n",
    "display(data_modalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   submitter_id project_id      time  event  group\n",
      "0  TCGA-Z7-A8R6  TCGA-BRCA  8.920548      0  train\n",
      "1  TCGA-C8-A1HE  TCGA-BRCA  1.027397      0  train\n",
      "2  TCGA-A8-A07B  TCGA-BRCA  3.583562      0  train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9917"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------#\n",
    "#                             20-CANCER SUBSET                                #\n",
    "#                 (to compare to Cheerla and Gevaert 2019)                    #\n",
    "#-----------------------------------------------------------------------------#\n",
    "\n",
    "#cancers = ['BLCA', 'BRCA', 'CESC', 'COAD', 'READ',\n",
    " #          'HNSC', 'KICH', 'KIRC', 'KIRP', 'LAML',\n",
    " #          'LGG', 'LIHC', 'LUAD', 'LUSC', 'OV',\n",
    " #          'PAAD', 'PRAD', 'SKCM', 'STAD', 'THCA', 'UCEC']\n",
    "\n",
    "cancers = ['TCGA-BRCA']\n",
    "\n",
    "labels = pd.read_csv('data/labels.tsv', sep='\\t')\n",
    "print(labels.head(3))\n",
    "\n",
    "# List of patients to exclude: patients with cancers that are not in the subset\n",
    "only_BRCA = list(labels.loc[~labels['project_id'].isin(cancers), 'submitter_id'])\n",
    "len(only_BRCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Event counts for ALL 29 CANCERS (excluding OV, LUSC, SARC):\n",
      "  train: 2267 events\n",
      "  val: 282 events\n",
      "  test: 284 events\n",
      "\n",
      "Event counts for TCGA-BRCA ONLY:\n",
      "  train: 121 events\n",
      "  val: 15 events\n",
      "  test: 15 events\n"
     ]
    }
   ],
   "source": [
    "#counts events per group\n",
    "\n",
    "# Load the TSV file\n",
    "df = labels\n",
    "\n",
    "# Define excluded projects\n",
    "excluded_projects = {'TCGA-OV', 'TCGA-LUSC', 'TCGA-SARC'}\n",
    "\n",
    "# Filter for the 29 cancer types (i.e., all except the excluded)\n",
    "all_cancers_df = df[~df['project_id'].isin(excluded_projects)]\n",
    "\n",
    "# Filter only BRCA\n",
    "brca_df = df[df['project_id'] == 'TCGA-BRCA']\n",
    "\n",
    "# Count number of events (event == 1) per group\n",
    "def count_events_per_group(sub_df, label):\n",
    "    print(f\"\\nEvent counts for {label}:\")\n",
    "    grouped = sub_df[sub_df['event'] == 1].groupby('group').size()\n",
    "    for group_name in ['train', 'val', 'test']:\n",
    "        count = grouped.get(group_name, 0)\n",
    "        print(f\"  {group_name}: {count} events\")\n",
    "\n",
    "# Output\n",
    "count_events_per_group(all_cancers_df, 'ALL 29 CANCERS (excluding OV, LUSC, SARC)')\n",
    "count_events_per_group(brca_df, 'TCGA-BRCA ONLY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   submitter_id project_id      time  event  group\n",
      "0  TCGA-Z7-A8R6  TCGA-BRCA  8.920548      0  train\n",
      "1  TCGA-C8-A1HE  TCGA-BRCA  1.027397      0  train\n",
      "2  TCGA-A8-A07B  TCGA-BRCA  3.583562      0  train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1341"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cancers = ['TCGA-SARC', 'TCGA-LUSC', 'TCGA-OV']\n",
    "\n",
    "labels = pd.read_csv('data/labels.tsv', sep='\\t')\n",
    "print(labels.head(3))\n",
    "\n",
    "# List of patients to exclude: patients with cancers that are not in the subset\n",
    "exclude_cancers = list(labels.loc[labels['project_id'].isin(cancers), 'submitter_id'])\n",
    "len(exclude_cancers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping 7734 patient(s) not in exclude list.\n",
      "Keeping 968 patient(s) not in exclude list.\n",
      "Keeping 968 patient(s) not in exclude list.\n",
      "Data modalities:\n",
      "   clinical\n",
      "\n",
      "Dataset sizes (# patients):\n",
      "   train: 7734\n",
      "   val: 968\n",
      "   test: 968\n",
      "\n",
      "Batch size: 128\n"
     ]
    }
   ],
   "source": [
    "dataloaders = utils.get_dataloaders(data_location=DATA,\n",
    "                                    labels_file='/app/data/labels.tsv',\n",
    "                                    modalities=data_modalities.value,\n",
    "                                    wsi_patch_size=299,\n",
    "                                    n_wsi_patches=5,\n",
    "#                                     batch_size=20,\n",
    "#                                     batch_size=128,\n",
    "                                     #batch_size=64,\n",
    "                                     exclude_patients=exclude_cancers,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different intervals\n",
    "\n",
    "If trying out different time interval outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equidistant times"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "interval_cuts = torch.arange(0., 365 * 5.1, 365 / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By duration quantiles"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labels = [(t, e) for t, e in dataloaders['train'].dataset.label_map.values()]\n",
    "durations = [t for t, _ in labels]\n",
    "events = [e for _, e in labels]\n",
    "\n",
    "interval_cuts = utils.discretize_time_by_duration_quantiles(durations, events, 20)\n",
    "interval_cuts = torch.from_numpy(interval_cuts)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#-----------------------------------------------------------------------------#\n",
    "#                       PRE-TRAINED UNIMODAL MODELS                           #\n",
    "#-----------------------------------------------------------------------------#\n",
    "\n",
    "unimodal_weigths = {'clinical': 'clinical_lr0.007run2_epoch78_concord0.78.pth', #'clinical_mRNA_lr0.005-no-SARC-LUSC-OV_no-pretrained-weights_epoch25_concord0.82.pth',\n",
    "                    'mRNA': #'clinical_mRNA_lr0.005-no-SARC-LUSC-OV_no-pretrained-weights_epoch25_concord0.82.pth'\n",
    "                    'DNAm': None,\n",
    "                    'miRNA': None,\n",
    "                    'CNV': None,\n",
    "                    'wsi': None,}\n",
    "\n",
    "unimodal_weigths = {k: os.path.join(MODELS, v) if v is not None else None\n",
    "                    for k, v in unimodal_weigths.items()}\n",
    "\n",
    "multisurv = Model(dataloaders=dataloaders,\n",
    "                  unimodal_state_files=unimodal_weigths,\n",
    "#                  freeze_up_to='aggregator',\n",
    "#                  output_intervals=interval_cuts,\n",
    "                  device=device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#-----------------------------------------------------------------------------#\n",
    "#                              AUXILIARY LOSS                                 #\n",
    "#-----------------------------------------------------------------------------#\n",
    "\n",
    "# cosine_embedding_margin = 1e-5\n",
    "# auxiliary_criterion = torch.nn.CosineEmbeddingLoss(margin=cosine_embedding_margin)\n",
    "\n",
    "multisurv = Model(dataloaders=dataloaders,\n",
    "                  auxiliary_criterion=auxiliary_criterion,\n",
    "                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating MultiSurv model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/src/multisurv.py:84: UserWarning: Input data is unimodal: no fusion procedure.\n",
      "  warnings.warn('Input data is unimodal: no fusion procedure.')\n"
     ]
    }
   ],
   "source": [
    "multisurv = Model(dataloaders=dataloaders,\n",
    "#                   fusion_method='attention',\n",
    "#                   output_intervals=interval_cuts,\n",
    "                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating MultiSurv model...\n"
     ]
    }
   ],
   "source": [
    "multisurv = Model(dataloaders=dataloaders,\n",
    "#                   fusion_method='attention',\n",
    "#                  unimodal_state_files=unimodal_weigths,\n",
    "                   output_intervals=multisurv.output_intervals.to(device),\n",
    "                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model weights:\n",
      "/mnt/data/multisurv_models/clinical_lr0.0075-no-SARC-LUSC-OV_epoch85_concord0.79.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/src/model.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "multisurv.load_weights(os.path.join(MODELS, 'clinical_lr0.0075-no-SARC-LUSC-OV_epoch85_concord0.79.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output intervals (in years):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
       "        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\n",
       "        28., 29., 30.], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Output intervals (in years):')\n",
    "multisurv.output_intervals / 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clinical_submodel', 'mRNA_submodel', 'aggregator', 'fc_block', 'risk_layer']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multisurv.model_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable blocks:\n",
      "   clinical_submodel: True\n",
      "   mRNA_submodel: True\n",
      "   fc_block: True\n",
      "   risk_layer: True\n"
     ]
    }
   ],
   "source": [
    "print('Trainable blocks:')\n",
    "layer = None\n",
    "\n",
    "for name, child in multisurv.model.named_children():\n",
    "    for name_2, params in child.named_parameters():\n",
    "        if name is not layer:\n",
    "            print(f'   {name}: {params.requires_grad}')\n",
    "        layer = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiSurv(\n",
       "  (clinical_submodel): ClinicalNet(\n",
       "    (embedding_layers): ModuleList(\n",
       "      (0): Embedding(32, 16)\n",
       "      (1): Embedding(2, 1)\n",
       "      (2): Embedding(9, 5)\n",
       "      (3-5): 3 x Embedding(3, 2)\n",
       "      (6): Embedding(16, 8)\n",
       "      (7): Embedding(10, 5)\n",
       "      (8): Embedding(21, 11)\n",
       "    )\n",
       "    (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "    (bn_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (linear): Linear(in_features=53, out_features=256, bias=True)\n",
       "    (output_layer): FC(\n",
       "      (fc): Sequential(\n",
       "        (0): Dropout(p=0.5, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mRNA_submodel): FC(\n",
       "    (fc): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Linear(in_features=1000, out_features=4096, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): Dropout(p=0.5, inplace=False)\n",
       "      (5): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (6): ReLU()\n",
       "      (7): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): Dropout(p=0.5, inplace=False)\n",
       "      (9): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      (10): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (aggregator): Fusion()\n",
       "  (fc_block): FC(\n",
       "    (fc): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): Dropout(p=0.5, inplace=False)\n",
       "      (5): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (6): ReLU()\n",
       "      (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): Dropout(p=0.5, inplace=False)\n",
       "      (9): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (10): ReLU()\n",
       "      (11): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): Dropout(p=0.5, inplace=False)\n",
       "      (13): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (14): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (risk_layer): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=30, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multisurv.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied GPU-compatible fix to LRRangeTest.run\n"
     ]
    }
   ],
   "source": [
    "# GPU-compatible version of the completely fixed LR test\n",
    "\n",
    "def gpu_fixed_lr_test_run(self, init_value=1e-8, final_value=10., beta=0.98):\n",
    "    \"\"\"GPU-compatible LR test that ensures all tensors are on the same device.\"\"\"\n",
    "    print(\">>> Using GPU-COMPATIBLE FIXED lr_test.run method\")\n",
    "    \n",
    "    power = (1 / (len(self.dataloader) - 1))\n",
    "    mult = (final_value / init_value) ** power\n",
    "    lr = init_value\n",
    "    self.optimizer.param_groups[0]['lr'] = lr\n",
    "    avg_loss = 0.\n",
    "    best_loss = 0.\n",
    "    batch_num = 0\n",
    "\n",
    "    print('>>> Compute loss at increasing LR values')\n",
    "    \n",
    "    # Clear losses and lrs lists\n",
    "    self.losses = []\n",
    "    self.lrs = []\n",
    "\n",
    "    for data in self.dataloader:\n",
    "        batch_num += 1\n",
    "        print('\\r' + f'    Iterate over mini-batches: {str(batch_num)}', end='')\n",
    "\n",
    "        try:\n",
    "            # Unpack data\n",
    "            if len(data) == 3:\n",
    "                modality_data, time, event = data\n",
    "            elif len(data) == 4:\n",
    "                modality_data, time, event, pid = data\n",
    "            \n",
    "            # CRITICAL: Move ALL data to the same device as the model\n",
    "            target_device = next(self.model.parameters()).device\n",
    "            \n",
    "            # Move modality data to device\n",
    "            for key, value in modality_data.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    modality_data[key] = value.to(target_device)\n",
    "                elif isinstance(value, (list, tuple)):\n",
    "                    modality_data[key] = tuple(v.to(target_device) if isinstance(v, torch.Tensor) else v for v in value)\n",
    "            \n",
    "            # Move time and event to device\n",
    "            time = time.to(target_device)\n",
    "            event = event.to(target_device)\n",
    "            \n",
    "            # Set model to train mode\n",
    "            self.model.train()\n",
    "            \n",
    "            # Clear gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.set_grad_enabled(True):\n",
    "                feature_representations, risk = self.model(modality_data)\n",
    "                modality_features = feature_representations['modalities']\n",
    "                \n",
    "                # Compute loss with proper device handling\n",
    "                try:\n",
    "                    # Ensure output_intervals are on the same device\n",
    "                    if hasattr(self, 'output_intervals'):\n",
    "                        breaks = self.output_intervals.to(target_device)\n",
    "                    else:\n",
    "                        breaks = torch.linspace(0, 10, 19, device=target_device)  # Fallback\n",
    "                    \n",
    "                    loss = self.criterion(risk, times=time, events=event, breaks=breaks, device=target_device)\n",
    "                    \n",
    "                except Exception as loss_error:\n",
    "                    print(f\"\\nPrimary criterion failed: {loss_error}\")\n",
    "                    # Fallback to simple MSE loss on the same device\n",
    "                    dummy_target = torch.ones_like(risk)  # This will be on the same device as risk\n",
    "                    loss = torch.nn.functional.mse_loss(risk, dummy_target)\n",
    "                    print(\"Using fallback MSE loss\")\n",
    "                \n",
    "                # Add auxiliary loss if needed\n",
    "                if self.aux_criterion is not None:\n",
    "                    try:\n",
    "                        if len(modality_features) >= 2:\n",
    "                            target = torch.ones(modality_features[0].shape[0], device=target_device)\n",
    "                            aux_loss = self.aux_criterion(modality_features[0], modality_features[1], target)\n",
    "                            loss = loss + 0.1 * aux_loss\n",
    "                    except Exception as aux_error:\n",
    "                        print(f\"\\nAuxiliary criterion failed: {aux_error}\")\n",
    "            \n",
    "            # Store the loss value\n",
    "            loss_value = loss.item()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError at batch {batch_num}: {e}\")\n",
    "            break\n",
    "\n",
    "        # Compute the smoothed loss\n",
    "        avg_loss = (beta * avg_loss + (1 - beta) * loss_value)\n",
    "        smoothed_loss = avg_loss / (1 - beta ** batch_num)\n",
    "\n",
    "        # Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "            print()\n",
    "            print('    Exploding loss; finish test.')\n",
    "            break\n",
    "\n",
    "        # Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num == 1:\n",
    "            best_loss = smoothed_loss\n",
    "            \n",
    "        # Store the values\n",
    "        self.losses.append(smoothed_loss)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # Do the backward pass and optimizer step\n",
    "        try:\n",
    "            loss.backward(retain_graph=False)\n",
    "            self.optimizer.step()\n",
    "        except Exception as e:\n",
    "            print(f\"\\nBackward pass failed at batch {batch_num}: {e}\")\n",
    "            break\n",
    "\n",
    "        # Update the lr for the next step\n",
    "        lr *= mult\n",
    "        self.optimizer.param_groups[0]['lr'] = lr\n",
    "        \n",
    "        # Stop after reasonable number of batches\n",
    "        if batch_num >= 100:\n",
    "            print(\"\\nStopping after 100 batches\")\n",
    "            break\n",
    "\n",
    "    print()\n",
    "    print('    Completed test.')\n",
    "    return self\n",
    "\n",
    "# Apply the GPU-compatible fix\n",
    "import lr_range_test\n",
    "lr_range_test.LRRangeTest.run = gpu_fixed_lr_test_run\n",
    "print(\"Applied GPU-compatible fix to LRRangeTest.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Using GPU-COMPATIBLE FIXED lr_test.run method\n",
      ">>> Compute loss at increasing LR values\n",
      "    Iterate over mini-batches: 66\n",
      "    Exploding loss; finish test.\n",
      "\n",
      "    Completed test.\n",
      "\n",
      "CPU times: user 17.7 s, sys: 575 ms, total: 18.3 s\n",
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "multisurv.test_lr_range()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multisurv.plot_lr_range(trim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Applied GPU-compatible fix to ModelCoach._predictions_to_pycox method\n",
      "Model output intervals shape: 31\n",
      "Model output intervals device: cuda:0\n",
      "Model risk layer output: 30\n",
      "Expected: 30 intervals for 31 breakpoints\n"
     ]
    }
   ],
   "source": [
    "# GPU-compatible fix for _predictions_to_pycox method\n",
    "def gpu_fixed_predictions_to_pycox(self, preds, time_points=None):\n",
    "    \"\"\"GPU-compatible version that properly handles device transfers.\"\"\"\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    \n",
    "    # preds shape is [num_patients, num_intervals] e.g., [864, 18]\n",
    "    # Convert predictions to DataFrame (move to CPU first)\n",
    "    df = pd.DataFrame(preds.detach().cpu().numpy())  # Added .detach() for GPU tensors\n",
    "    \n",
    "    if time_points is None:\n",
    "        # Get the number of intervals from the model output\n",
    "        n_intervals = preds.shape[1]  # Should be 18 for your model\n",
    "        \n",
    "        # Use the model's actual output intervals\n",
    "        if hasattr(self, 'output_intervals') and self.output_intervals is not None:\n",
    "            # Handle GPU tensors properly\n",
    "            intervals = self.output_intervals\n",
    "            if torch.is_tensor(intervals):\n",
    "                intervals = intervals.detach().cpu()  # Move to CPU for calculations\n",
    "            \n",
    "            if len(intervals) > n_intervals:\n",
    "                # Take the first n_intervals midpoints\n",
    "                midpoints = (intervals[:-1] + intervals[1:]) / 2\n",
    "                time_points = midpoints[:n_intervals]\n",
    "            else:\n",
    "                # Fallback to evenly spaced points\n",
    "                last_interval = intervals[-1].item() if torch.is_tensor(intervals) else intervals[-1]\n",
    "                time_points = torch.linspace(0.5, last_interval / 365, n_intervals)\n",
    "        else:\n",
    "            # Fallback: create time points that match the output size\n",
    "            time_points = torch.arange(0.5, 0.5 + n_intervals, 1.0)\n",
    "    \n",
    "    # Ensure time_points matches the prediction dimensions\n",
    "    if len(time_points) != preds.shape[1]:\n",
    "        print(f\"Warning: Adjusting time_points from {len(time_points)} to {preds.shape[1]}\")\n",
    "        first_point = time_points[0].item() if torch.is_tensor(time_points) else time_points[0]\n",
    "        last_point = time_points[-1].item() if torch.is_tensor(time_points) else time_points[-1]\n",
    "        time_points = torch.linspace(first_point, last_point, preds.shape[1])\n",
    "    \n",
    "    # FIXED: The DataFrame structure should be transposed\n",
    "    # We want columns to be time points, rows to be patients\n",
    "    df = df.T  # Transpose so shape becomes [num_intervals, num_patients]\n",
    "    \n",
    "    # Convert time_points to numpy if it's a tensor (ensure CPU)\n",
    "    if torch.is_tensor(time_points):\n",
    "        time_points = time_points.detach().cpu().numpy()\n",
    "    \n",
    "    # Set the index to time points\n",
    "    df.index = time_points\n",
    "    df.index.name = 'time'\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the GPU-compatible fix to ModelCoach\n",
    "from coach import ModelCoach\n",
    "ModelCoach._predictions_to_pycox = gpu_fixed_predictions_to_pycox\n",
    "print(\"âœ… Applied GPU-compatible fix to ModelCoach._predictions_to_pycox method\")\n",
    "\n",
    "# Debug info with GPU handling\n",
    "print(f\"Model output intervals shape: {len(multisurv.output_intervals)}\")\n",
    "print(f\"Model output intervals device: {multisurv.output_intervals.device}\")\n",
    "print(f\"Model risk layer output: {multisurv.model.risk_layer[0].out_features}\")\n",
    "print(f\"Expected: {len(multisurv.output_intervals) - 1} intervals for {len(multisurv.output_intervals)} breakpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run tag: \"clinical_lr0.005-Events_stratified_Labels-no-SARC-LUSC-OV_6\"\n"
     ]
    }
   ],
   "source": [
    "picked_lr = 5e-3\n",
    "\n",
    "run_tag = utils.compose_run_tag(model=multisurv, lr=picked_lr,\n",
    "                                dataloaders=dataloaders,\n",
    "                                log_dir='.training_logs/',\n",
    "                                suffix='-Events_stratified_Labels-no-SARC-LUSC-OV_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------\n",
      "             Training        Validation\n",
      "           ------------     ------------\n",
      " Epoch     Loss     Ctd     Loss     Ctd\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/multisurv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005]\n",
      " 1/80     0.0232   0.603   0.0167   0.688\n",
      "[0.005]\n",
      " 3/80     0.0158   0.682   0.0150   0.754\n",
      "[0.005]\n",
      " 6/80     0.0147   0.725   0.0146   0.775\n",
      "[0.005]\n",
      " 9/80     0.0145   0.748   0.0143   0.785\n",
      "[0.005]\n",
      " 12/80    0.0142   0.758   0.0142   0.791\n",
      "[0.005]\n",
      " 15/80    0.0143   0.757   0.0141   0.791\n",
      "[0.005]\n",
      " 18/80    0.0140   0.770   0.0138   0.796\n",
      "[0.005]\n",
      " 21/80    0.0140   0.770   0.0138   0.796\n",
      "\n",
      ">>> Keyboard interrupt! <<<\n",
      "(trained for 0h 9m 33s)\n",
      "\n",
      "Best validation concordance values:\n",
      "     0.7926 (epoch19)\n",
      "     0.7963 (epoch20)\n",
      "     0.7963 (epoch21)\n"
     ]
    }
   ],
   "source": [
    "fit_args = {\n",
    "    'lr': picked_lr,\n",
    "    'num_epochs': 80,\n",
    "    'info_freq': 3,\n",
    "#     'info_freq': None,\n",
    "#     'lr_factor': 0.25,\n",
    "#     'scheduler_patience': 5,\n",
    "    'lr_factor': 0.5,\n",
    "    'scheduler_patience': 8,\n",
    "    'log_dir': os.path.join('.training_logs/', run_tag),\n",
    "    'weight_decay': 0,\n",
    "}\n",
    "\n",
    "multisurv.fit(**fit_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model weights\n",
    "\n",
    "If desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using event file: /app/.training_logs/mRNA_lr0.005-Events_stratified_Labels-no-SARC-LUSC-OV_4/events.out.tfevents.1753089526.8cda6e761041.28.0\n",
      "Available scalar tags: ['train_loss', 'train_concord', 'val_loss', 'val_concord']\n",
      "Best smoothed val_concord = 0.7749 at epoch 16\n",
      "\n",
      " epoch â”‚ raw val_concord â”‚ smoothed\n",
      "â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   3   â”‚      0.7598     â”‚  0.7326\n",
      "   4   â”‚      0.7493     â”‚  0.7479\n",
      "   5   â”‚      0.7560     â”‚  0.7550\n",
      "   6   â”‚      0.7571     â”‚  0.7541\n",
      "   7   â”‚      0.7637     â”‚  0.7589\n",
      "   8   â”‚      0.7621     â”‚  0.7610\n",
      "   9   â”‚      0.7597     â”‚  0.7618\n",
      "  10   â”‚      0.7721     â”‚  0.7646\n",
      "  11   â”‚      0.7619     â”‚  0.7645\n",
      "  12   â”‚      0.7724     â”‚  0.7688\n",
      "  13   â”‚      0.7724     â”‚  0.7689\n",
      "  14   â”‚      0.7709     â”‚  0.7719\n",
      "  15   â”‚      0.7775     â”‚  0.7736\n",
      "  16   â”‚      0.7763     â”‚  0.7749\n",
      "  17   â”‚      0.7668     â”‚  0.7735\n",
      "  18   â”‚      0.7723     â”‚  0.7718\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# 1) Path to your events file:\n",
    "events_path = os.path.join('/app', '.training_logs/' ,run_tag)\n",
    "event_files = glob.glob(os.path.join(events_path, \"events.out.tfevents.*\"))\n",
    "if not event_files:\n",
    "    raise FileNotFoundError(f\"No TensorBoard event files found in {events_path}\")\n",
    "event_file = sorted(event_files, key=os.path.getmtime)[-1]  # use the newest\n",
    "print(f\"Using event file: {event_file}\")\n",
    "\n",
    "# 2) Load all scalar data\n",
    "ea = EventAccumulator(event_file, size_guidance={\"scalars\": 0})\n",
    "ea.Reload()\n",
    "\n",
    "# 3) Check available scalar tags\n",
    "print(\"Available scalar tags:\", ea.Tags()[\"scalars\"])\n",
    "# => ['train_loss', 'train_concord', 'val_loss', 'val_concord']\n",
    "\n",
    "# 4) Extract (epoch, value) pairs for val_concord\n",
    "val_concord = [(e.step, e.value) for e in ea.Scalars(\"val_concord\")]\n",
    "\n",
    "# 5) Separate epochs and values\n",
    "epochs = [step for step, _ in val_concord]\n",
    "vals   = np.array([v for _, v in val_concord])\n",
    "\n",
    "# 6) Compute 3-epoch moving average\n",
    "window = 3\n",
    "# 'valid' mode produces len(vals) - window + 1 points\n",
    "smoothed = np.convolve(vals, np.ones(window)/window, mode=\"valid\")\n",
    "smoothed_epochs = epochs[window - 1 :]  # first smoothed point corresponds to epoch=window\n",
    "\n",
    "# 7) Find epoch with highest smoothed val_concord\n",
    "best_idx = np.argmax(smoothed)\n",
    "best_epoch = smoothed_epochs[best_idx]\n",
    "best_smoothed_concord = smoothed[best_idx]\n",
    "print(f\"Best smoothed val_concord = {best_smoothed_concord:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "# 8) (Optional) print raw vs smoothed for inspection\n",
    "print(\"\\n epoch â”‚ raw val_concord â”‚ smoothed\")\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "for ep, raw, sm in zip(smoothed_epochs, vals[window - 1 :], smoothed):\n",
    "    print(f\"  {ep:>2d}   â”‚      {raw:.4f}     â”‚  {sm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using event file: /app/.training_logs/events.out.tfevents.1580718775.luis-MS-7B61.746.0\n",
      "Available scalar tags: ['train_loss', 'train_Cindex', 'val_loss', 'val_Cindex']\n",
      "Best smoothed val_concord = 0.8094 at epoch 43\n",
      "\n",
      " epoch â”‚ raw val_concord â”‚ smoothed\n",
      "â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   3   â”‚      0.7592     â”‚  0.7384\n",
      "   4   â”‚      0.7766     â”‚  0.7615\n",
      "   5   â”‚      0.7773     â”‚  0.7710\n",
      "   6   â”‚      0.7843     â”‚  0.7794\n",
      "   7   â”‚      0.7885     â”‚  0.7834\n",
      "   8   â”‚      0.7741     â”‚  0.7823\n",
      "   9   â”‚      0.7934     â”‚  0.7853\n",
      "  10   â”‚      0.7877     â”‚  0.7851\n",
      "  11   â”‚      0.7896     â”‚  0.7902\n",
      "  12   â”‚      0.7922     â”‚  0.7898\n",
      "  13   â”‚      0.7914     â”‚  0.7911\n",
      "  14   â”‚      0.7956     â”‚  0.7931\n",
      "  15   â”‚      0.7974     â”‚  0.7948\n",
      "  16   â”‚      0.7991     â”‚  0.7974\n",
      "  17   â”‚      0.8010     â”‚  0.7992\n",
      "  18   â”‚      0.7970     â”‚  0.7990\n",
      "  19   â”‚      0.7988     â”‚  0.7989\n",
      "  20   â”‚      0.8006     â”‚  0.7988\n",
      "  21   â”‚      0.8016     â”‚  0.8003\n",
      "  22   â”‚      0.7985     â”‚  0.8002\n",
      "  23   â”‚      0.8033     â”‚  0.8011\n",
      "  24   â”‚      0.8019     â”‚  0.8012\n",
      "  25   â”‚      0.8053     â”‚  0.8035\n",
      "  26   â”‚      0.8007     â”‚  0.8027\n",
      "  27   â”‚      0.8026     â”‚  0.8029\n",
      "  28   â”‚      0.8049     â”‚  0.8027\n",
      "  29   â”‚      0.8035     â”‚  0.8036\n",
      "  30   â”‚      0.8053     â”‚  0.8046\n",
      "  31   â”‚      0.8050     â”‚  0.8046\n",
      "  32   â”‚      0.8093     â”‚  0.8065\n",
      "  33   â”‚      0.8091     â”‚  0.8078\n",
      "  34   â”‚      0.8088     â”‚  0.8090\n",
      "  35   â”‚      0.8077     â”‚  0.8085\n",
      "  36   â”‚      0.8080     â”‚  0.8082\n",
      "  37   â”‚      0.8069     â”‚  0.8075\n",
      "  38   â”‚      0.8092     â”‚  0.8080\n",
      "  39   â”‚      0.8093     â”‚  0.8085\n",
      "  40   â”‚      0.8092     â”‚  0.8092\n",
      "  41   â”‚      0.8088     â”‚  0.8091\n",
      "  42   â”‚      0.8094     â”‚  0.8091\n",
      "  43   â”‚      0.8100     â”‚  0.8094\n",
      "  44   â”‚      0.8088     â”‚  0.8094\n",
      "  45   â”‚      0.8089     â”‚  0.8092\n",
      "  46   â”‚      0.8086     â”‚  0.8088\n",
      "  47   â”‚      0.8092     â”‚  0.8089\n",
      "  48   â”‚      0.8094     â”‚  0.8091\n",
      "  49   â”‚      0.8087     â”‚  0.8091\n",
      "  50   â”‚      0.8085     â”‚  0.8089\n",
      "  51   â”‚      0.8085     â”‚  0.8086\n",
      "  52   â”‚      0.8087     â”‚  0.8086\n",
      "  53   â”‚      0.8087     â”‚  0.8086\n",
      "  54   â”‚      0.8090     â”‚  0.8088\n",
      "  55   â”‚      0.8085     â”‚  0.8087\n",
      "  56   â”‚      0.8085     â”‚  0.8087\n",
      "  57   â”‚      0.8083     â”‚  0.8084\n",
      "  58   â”‚      0.8088     â”‚  0.8086\n"
     ]
    }
   ],
   "source": [
    "#events.out.tfevents.1580718775.luis-MS-7B61.746\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# 1) Path to your events file:\n",
    "events_path = os.path.join('/app', '.training_logs/')\n",
    "event_file = os.path.join(events_path, \"events.out.tfevents.1580718775.luis-MS-7B61.746.0\")\n",
    "print(f\"Using event file: {event_file}\")\n",
    "\n",
    "# 2) Load all scalar data\n",
    "ea = EventAccumulator(event_file, size_guidance={\"scalars\": 0})\n",
    "ea.Reload()\n",
    "\n",
    "# 3) Check available scalar tags\n",
    "print(\"Available scalar tags:\", ea.Tags()[\"scalars\"])\n",
    "# => ['train_loss', 'train_concord', 'val_loss', 'val_concord']\n",
    "\n",
    "# 4) Extract (epoch, value) pairs for val_concord\n",
    "val_concord = [(e.step, e.value) for e in ea.Scalars(\"val_loss\")]\n",
    "\n",
    "# 5) Separate epochs and values\n",
    "epochs = [step for step, _ in val_concord]\n",
    "vals   = np.array([v for _, v in val_concord])\n",
    "\n",
    "# 6) Compute 3-epoch moving average\n",
    "window = 3\n",
    "# 'valid' mode produces len(vals) - window + 1 points\n",
    "smoothed = np.convolve(vals, np.ones(window)/window, mode=\"valid\")\n",
    "smoothed_epochs = epochs[window - 1 :]  # first smoothed point corresponds to epoch=window\n",
    "\n",
    "# 7) Find epoch with highest smoothed val_concord\n",
    "best_idx = np.argmax(smoothed)\n",
    "best_epoch = smoothed_epochs[best_idx]\n",
    "best_smoothed_concord = smoothed[best_idx]\n",
    "print(f\"Best smoothed val_concord = {best_smoothed_concord:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "# 8) (Optional) print raw vs smoothed for inspection\n",
    "print(\"\\n epoch â”‚ raw val_concord â”‚ smoothed\")\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "for ep, raw, sm in zip(smoothed_epochs, vals[window - 1 :], smoothed):\n",
    "    print(f\"  {ep:>2d}   â”‚      {raw:.4f}     â”‚  {sm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch15', 'epoch16', 'epoch18'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multisurv.best_model_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch15': 0.7774812787868027,\n",
       " 'epoch16': 0.7762540862899099,\n",
       " 'epoch18': 0.7723009598211954}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multisurv.best_concord_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch31': 0.8239799846035412}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multisurv.current_concord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model weights to file:\n",
      "    /mnt/data/multisurv_models/mRNA_lr0.005-Events_stratified_Labels-no-SARC-LUSC-OV_4_epoch15_concord0.78.pth\n"
     ]
    }
   ],
   "source": [
    "multisurv.save_weights(saved_epoch='epoch15', prefix=run_tag, weight_dir=MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping 876 patient(s) not in exclude list.\n",
      "Keeping 109 patient(s) not in exclude list.\n",
      "Keeping 109 patient(s) not in exclude list.\n",
      "Data modalities:\n",
      "   clinical\n",
      "\n",
      "Dataset sizes (# patients):\n",
      "   train: 876\n",
      "   val: 109\n",
      "   test: 109\n",
      "\n",
      "Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "dataloaders = utils.get_dataloaders(data_location=DATA,\n",
    "                                    labels_file='/app/data/labels.tsv',\n",
    "                                    modalities=data_modalities.value,\n",
    "                                    wsi_patch_size=299,\n",
    "                                    n_wsi_patches=5,\n",
    "                                    batch_size=32,\n",
    "                                    exclude_patients=only_BRCA,\n",
    "                                    #return_patient_id=True,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect patient predictions: 109/109\n",
      "\n",
      "C-index   0.704\n",
      "Ctd       0.705\n",
      "IBS       0.179\n",
      "INBLL     0.52\n"
     ]
    }
   ],
   "source": [
    "performance = utils.Evaluation(\n",
    "    model=multisurv, dataset=dataloaders['val'].dataset,\n",
    "    device=device)\n",
    "performance.compute_metrics()\n",
    "performance.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect patient predictions: 109/109\n",
      "\n",
      "C-index   0.786\n",
      "Ctd       0.786\n",
      "IBS       0.194\n",
      "INBLL     0.557\n"
     ]
    }
   ],
   "source": [
    "performance = utils.Evaluation(\n",
    "    model=multisurv, dataset=dataloaders['test'].dataset,\n",
    "    device=device)\n",
    "performance.compute_metrics()\n",
    "performance.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watermark <a class='tocSkip'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch      1.4.0\n",
      "ipywidgets 7.5.1\n",
      "pandas     1.0.1\n",
      "\n",
      "CPython 3.6.7\n",
      "IPython 7.11.1\n",
      "\n",
      "last updated: Tue Jul 28 2020\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions\n",
    "%watermark -v\n",
    "print()\n",
    "%watermark -u -n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of the page](#Top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "197px",
    "width": "372px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "236px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
