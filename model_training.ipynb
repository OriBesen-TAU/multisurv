{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Top'></a>\n",
    "\n",
    "# Multisurv model training<a class='tocSkip'></a>\n",
    "\n",
    "Train MultiSurv models with different combinations of input data modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> PyTorch detected CUDA <<<\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext watermark\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('>>> PyTorch detected CUDA <<<')\n",
    "\n",
    "# Make modules in \"src\" dir visible\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "import utils\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#DataLoader\" data-toc-modified-id=\"DataLoader-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span><code>DataLoader</code></a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Different-intervals\" data-toc-modified-id=\"Different-intervals-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Different intervals</a></span><ul class=\"toc-item\"><li><span><a href=\"#Equidistant-times\" data-toc-modified-id=\"Equidistant-times-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Equidistant times</a></span></li><li><span><a href=\"#By-duration-quantiles\" data-toc-modified-id=\"By-duration-quantiles-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>By duration quantiles</a></span></li></ul></li><li><span><a href=\"#Pick-learning-rate\" data-toc-modified-id=\"Pick-learning-rate-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Pick learning rate</a></span></li><li><span><a href=\"#Fit\" data-toc-modified-id=\"Fit-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Fit</a></span><ul class=\"toc-item\"><li><span><a href=\"#Save-model-weights\" data-toc-modified-id=\"Save-model-weights-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Save model weights</a></span></li></ul></li><li><span><a href=\"#Check-validation-metrics\" data-toc-modified-id=\"Check-validation-metrics-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Check validation metrics</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ PyTorch is using: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "âœ… NVIDIA GPU detected - good for training!\n",
      "\n",
      "==================================================\n",
      "=== GPU DETECTION REPORT ===\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "PyTorch version: 2.4.0\n",
      "Number of CUDA devices: 1\n",
      "\n",
      "=== AVAILABLE GPUS ===\n",
      "GPU 0: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "  Memory: 8.0 GB\n",
      "  Compute Capability: 8.9\n",
      "  Multi Processors: 24\n",
      "\n",
      "=== CURRENT SELECTION ===\n",
      "Current CUDA device: 0\n",
      "Current GPU name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "\n",
      "=== MEMORY USAGE ===\n",
      "GPU 0 (NVIDIA GeForce RTX 4060 Laptop GPU):\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 8.0 GB\n",
      "\n",
      "=== GPU FUNCTIONALITY TEST ===\n",
      "âœ… GPU computation successful\n",
      "Test tensor device: cuda:0\n",
      "Result tensor device: cuda:0\n",
      "\n",
      "=== NVIDIA SYSTEM INFO ===\n",
      "NVIDIA-SMI Output:\n",
      "Mon Jun 30 19:47:11 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.64.01              Driver Version: 576.80         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   43C    P0              7W /   84W |     123MiB /   8188MiB |     13%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           10313      C   /python3.8                            N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "âœ… Using single GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "\n",
      "=== PYTORCH GPU VERIFICATION ===\n",
      "âœ… PyTorch using: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "âœ… Confirmed: Using NVIDIA GPU\n",
      "\n",
      "=== RECOMMENDATION ===\n",
      "âœ… Ready for GPU training with device: cuda:0\n",
      "Use: multisurv.device = cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "def check_gpu_setup():\n",
    "    \"\"\"Complete GPU detection and verification.\"\"\"\n",
    "    \n",
    "    print(\"=== GPU DETECTION REPORT ===\")\n",
    "    \n",
    "    # 1. Check CUDA availability\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"âŒ CUDA not available - will use CPU\")\n",
    "        return\n",
    "    \n",
    "    # 2. Check number of GPUs\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Number of CUDA devices: {gpu_count}\")\n",
    "    \n",
    "    # 3. List all available GPUs\n",
    "    print(\"\\n=== AVAILABLE GPUS ===\")\n",
    "    for i in range(gpu_count):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i}: {props.name}\")\n",
    "        print(f\"  Memory: {props.total_memory / 1024**3:.1f} GB\")\n",
    "        print(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
    "        print(f\"  Multi Processors: {props.multi_processor_count}\")\n",
    "    \n",
    "    # 4. Check current device\n",
    "    current_device = torch.cuda.current_device()\n",
    "    current_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"\\n=== CURRENT SELECTION ===\")\n",
    "    print(f\"Current CUDA device: {current_device}\")\n",
    "    print(f\"Current GPU name: {current_name}\")\n",
    "    \n",
    "    # 5. Memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\n=== MEMORY USAGE ===\")\n",
    "        for i in range(gpu_count):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "            print(f\"GPU {i} ({torch.cuda.get_device_name(i)}):\")\n",
    "            print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "            print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "            print(f\"  Total: {total:.1f} GB\")\n",
    "    \n",
    "    # 6. Test tensor creation on GPU\n",
    "    print(f\"\\n=== GPU FUNCTIONALITY TEST ===\")\n",
    "    try:\n",
    "        # Create test tensor on GPU\n",
    "        test_tensor = torch.randn(1000, 1000).cuda()\n",
    "        result = torch.mm(test_tensor, test_tensor)\n",
    "        print(f\"âœ… GPU computation successful\")\n",
    "        print(f\"Test tensor device: {test_tensor.device}\")\n",
    "        print(f\"Result tensor device: {result.device}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del test_tensor, result\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ GPU computation failed: {e}\")\n",
    "\n",
    "def check_nvidia_system():\n",
    "    \"\"\"Check NVIDIA system information.\"\"\"\n",
    "    print(\"\\n=== NVIDIA SYSTEM INFO ===\")\n",
    "    \n",
    "    try:\n",
    "        # Run nvidia-smi command\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"NVIDIA-SMI Output:\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"âŒ nvidia-smi command failed\")\n",
    "            print(result.stderr)\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ nvidia-smi not found - NVIDIA drivers may not be installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error running nvidia-smi: {e}\")\n",
    "\n",
    "def set_gpu_preference():\n",
    "    \"\"\"Set GPU preference if multiple GPUs available.\"\"\"\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No CUDA GPUs available\")\n",
    "        return None\n",
    "    \n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    \n",
    "    if gpu_count == 1:\n",
    "        device = torch.device('cuda:0')\n",
    "        print(f\"âœ… Using single GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        return device\n",
    "    \n",
    "    print(f\"\\n=== MULTIPLE GPUS DETECTED ({gpu_count}) ===\")\n",
    "    \n",
    "    # Show GPU options\n",
    "    for i in range(gpu_count):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i}: {props.name} ({props.total_memory / 1024**3:.1f} GB)\")\n",
    "    \n",
    "    # Find the most powerful GPU (by memory)\n",
    "    best_gpu = 0\n",
    "    best_memory = 0\n",
    "    \n",
    "    for i in range(gpu_count):\n",
    "        memory = torch.cuda.get_device_properties(i).total_memory\n",
    "        if memory > best_memory:\n",
    "            best_memory = memory\n",
    "            best_gpu = i\n",
    "    \n",
    "    device = torch.device(f'cuda:{best_gpu}')\n",
    "    print(f\"âœ… Auto-selected most powerful GPU: {torch.cuda.get_device_name(best_gpu)}\")\n",
    "    \n",
    "    # Set as current device\n",
    "    torch.cuda.set_device(best_gpu)\n",
    "    \n",
    "    return device\n",
    "\n",
    "def verify_pytorch_gpu_usage():\n",
    "    \"\"\"Verify PyTorch is actually using the NVIDIA GPU (not Intel).\"\"\"\n",
    "    \n",
    "    print(\"\\n=== PYTORCH GPU VERIFICATION ===\")\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"âŒ PyTorch not using GPU - will use CPU\")\n",
    "        return False\n",
    "    \n",
    "    # Create tensor and check device\n",
    "    x = torch.randn(100, 100).cuda()\n",
    "    gpu_name = torch.cuda.get_device_name(x.device)\n",
    "    \n",
    "    print(f\"âœ… PyTorch using: {gpu_name}\")\n",
    "    \n",
    "    # Check if it's NVIDIA (not Intel)\n",
    "    if 'nvidia' in gpu_name.lower() or 'geforce' in gpu_name.lower() or 'rtx' in gpu_name.lower() or 'gtx' in gpu_name.lower():\n",
    "        print(f\"âœ… Confirmed: Using NVIDIA GPU\")\n",
    "        return True\n",
    "    elif 'intel' in gpu_name.lower():\n",
    "        print(f\"âš ï¸  Warning: Using Intel GPU - this may be slow for deep learning\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"â“ Unknown GPU type: {gpu_name}\")\n",
    "        return True\n",
    "\n",
    "# Run all checks\n",
    "def complete_gpu_check():\n",
    "    \"\"\"Run complete GPU diagnostic.\"\"\"\n",
    "    check_gpu_setup()\n",
    "    check_nvidia_system()\n",
    "    device = set_gpu_preference()\n",
    "    is_nvidia = verify_pytorch_gpu_usage()\n",
    "    \n",
    "    print(f\"\\n=== RECOMMENDATION ===\")\n",
    "    if is_nvidia and device:\n",
    "        print(f\"âœ… Ready for GPU training with device: {device}\")\n",
    "        print(f\"Use: multisurv.device = {device}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Consider using CPU training: multisurv.device = torch.device('cpu')\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Quick check function\n",
    "def quick_gpu_check():\n",
    "    \"\"\"Quick GPU check for immediate feedback.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"ðŸŽ¯ PyTorch is using: {gpu_name}\")\n",
    "        \n",
    "        if any(keyword in gpu_name.lower() for keyword in ['nvidia', 'geforce', 'rtx', 'gtx']):\n",
    "            print(\"âœ… NVIDIA GPU detected - good for training!\")\n",
    "        else:\n",
    "            print(\"âš ï¸  Non-NVIDIA GPU detected\")\n",
    "    else:\n",
    "        print(\"âŒ No GPU available - will use CPU\")\n",
    "\n",
    "# Run the checks\n",
    "quick_gpu_check()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "complete_gpu_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = utils.INPUT_DATA_DIR\n",
    "MODELS = utils.TRAINED_MODEL_DIR\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d242e51a784559a9262262b0cdaa8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Input data', index=(0, 1), options=('clinical', 'mRNA', 'DNAm', 'miRNA', 'CNV', 'wâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_modalities = widgets.SelectMultiple(\n",
    "    options=['clinical', 'mRNA', 'DNAm', 'miRNA', 'CNV', 'wsi'],\n",
    "    index=[0, 1],\n",
    "    rows=6,\n",
    "    description='Input data',\n",
    "    disabled=False\n",
    ")\n",
    "display(data_modalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   submitter_id      time  event  group\n",
      "0  TCGA-Z7-A8R6  8.920548      0   test\n",
      "1  TCGA-C8-A1HE  1.027397      0   test\n",
      "2  TCGA-A8-A07B  3.583562      0  train\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------#\n",
    "#                             20-CANCER SUBSET                                #\n",
    "#                 (to compare to Cheerla and Gevaert 2019)                    #\n",
    "#-----------------------------------------------------------------------------#\n",
    "\n",
    "cancers = ['BRCA']\n",
    "\n",
    "labels = pd.read_csv('/app/data/labels.tsv', sep='\\t')\n",
    "print(labels.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding 1 patient(s) missing all data.\n",
      "Data modalities:\n",
      "   DNAm\n",
      "\n",
      "Dataset sizes (# patients):\n",
      "   train: 709\n",
      "   val: 165\n",
      "   test: 219\n",
      "\n",
      "Batch size: 16\n"
     ]
    }
   ],
   "source": [
    "dataloaders = utils.get_dataloaders(data_location=DATA,\n",
    "                                    labels_file='/app/data/labels.tsv',\n",
    "                                    modalities=data_modalities.value,\n",
    "                                    wsi_patch_size=299,\n",
    "                                    n_wsi_patches=5,\n",
    "#                                     batch_size=20,\n",
    "#                                    batch_size=64,\n",
    "                                     batch_size=16,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train event rate: 13.80%\n",
      "Val event rate: 13.94%\n",
      "Test event rate: 13.70%\n"
     ]
    }
   ],
   "source": [
    "# Compare event rates across splits\n",
    "def get_event_rate(dataset):\n",
    "    events = [e for _, e in dataset.label_map.values()]\n",
    "    return sum(events) / len(events)\n",
    "\n",
    "print(f\"Train event rate: {get_event_rate(dataloaders['train'].dataset):.2%}\")\n",
    "print(f\"Val event rate: {get_event_rate(dataloaders['val'].dataset):.2%}\")\n",
    "print(f\"Test event rate: {get_event_rate(dataloaders['test'].dataset):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN set (n=709):\n",
      "Clinical features shape: ({'DNAm': tensor([0.0813, 0.7809, 0.8450,  ..., 0.6343, 0.8889, 0.8638])}, 2.5506849315068494, 0)\n",
      "\n",
      "VAL set (n=165):\n",
      "Clinical features shape: ({'DNAm': tensor([0.1192, 0.5168, 0.7130,  ..., 0.0115, 0.6108, 0.7005])}, 1.5424657534246575, 1)\n",
      "\n",
      "TEST set (n=219):\n",
      "Clinical features shape: ({'DNAm': tensor([0.4664, 0.0514, 0.7464,  ..., 0.7416, 0.1982, 0.4422])}, 0.589041095890411, 0)\n"
     ]
    }
   ],
   "source": [
    "# Check if clinical features are similar across splits\n",
    "# Look for any systematic differences\n",
    "for split in ['train', 'val', 'test']:\n",
    "    data = dataloaders[split].dataset\n",
    "    print(f\"\\n{split.upper()} set (n={len(data)}):\")\n",
    "    # Get first patient's clinical data to see structure\n",
    "    if len(data) > 0:\n",
    "        clinical = data[0]\n",
    "        print(f\"Clinical features shape: {clinical}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 1. First, check the continuous feature distribution for clinical data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract continuous feature values for all patients\n",
    "def get_continuous_feature(dataset):\n",
    "    values = []\n",
    "    for i in range(len(dataset)):\n",
    "        clinical_data = dataset[i][0]  # Get the data part\n",
    "        continuous_val = clinical_data['clinical'][1].item()  # The continuous feature\n",
    "        values.append(continuous_val)\n",
    "    return np.array(values)\n",
    "\n",
    "train_cont = get_continuous_feature(dataloaders['train'].dataset)\n",
    "val_cont = get_continuous_feature(dataloaders['val'].dataset)\n",
    "test_cont = get_continuous_feature(dataloaders['test'].dataset)\n",
    "\n",
    "# Plot distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_cont, alpha=0.5, label=f'Train (mean={train_cont.mean():.3f})', bins=30)\n",
    "plt.hist(val_cont, alpha=0.5, label=f'Val (mean={val_cont.mean():.3f})', bins=30)\n",
    "plt.hist(test_cont, alpha=0.5, label=f'Test (mean={test_cont.mean():.3f})', bins=30)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Continuous Clinical Feature')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Train range: [{train_cont.min():.3f}, {train_cont.max():.3f}]\")\n",
    "print(f\"Val range: [{val_cont.min():.3f}, {val_cont.max():.3f}]\")\n",
    "print(f\"Test range: [{test_cont.min():.3f}, {test_cont.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different intervals\n",
    "\n",
    "If trying out different time interval outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equidistant times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_cuts = torch.arange(0., 365 * 5.1, 365 / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By duration quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [(t, e) for t, e in dataloaders['train'].dataset.label_map.values()]\n",
    "durations = [t for t, _ in labels]\n",
    "events = [e for _, e in labels]\n",
    "\n",
    "interval_cuts = utils.discretize_time_by_duration_quantiles(durations, events, 20)\n",
    "interval_cuts = torch.from_numpy(interval_cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------#\n",
    "#                       PRE-TRAINED UNIMODAL MODELS                           #\n",
    "#-----------------------------------------------------------------------------#\n",
    "\n",
    "unimodal_weigths = {'clinical': None,\n",
    "                    'mRNA': None,\n",
    "                    'DNAm': None,\n",
    "                    'miRNA': None,\n",
    "                    'CNV': None,\n",
    "                    'wsi': None,}\n",
    "\n",
    "unimodal_weigths = {k: os.path.join(MODELS, v) if v is not None else None\n",
    "                    for k, v in unimodal_weigths.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating MultiSurv model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "multisurv = Model(\n",
    "    dataloaders=dataloaders,\n",
    "#    auxiliary_criterion=None,  # No auxiliary loss needed\n",
    "    output_intervals=interval_cuts,\n",
    "#    unimodal_state_files = unimodal_weigths,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gpu_enabled_multisurv():\n",
    "    \"\"\"Create a fresh MultiSurv instance with proper GPU setup.\"\"\"\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Creating MultiSurv with device: {device}\")\n",
    "    \n",
    "    \n",
    "    # Create new model with GPU device\n",
    "    gpu_multisurv = Model(\n",
    "        dataloaders=multisurv.dataloaders,  # Reuse existing dataloaders\n",
    "        fusion_method=multisurv.fusion_method,\n",
    "        output_intervals=multisurv.output_intervals.to(device),  # Move to GPU\n",
    "        unimodal_state_files = multisurv.unimodal_state_files,\n",
    "        device=device  # Set device properly\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… GPU-enabled MultiSurv created!\")\n",
    "    return gpu_multisurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MultiSurv with device: cuda:0\n",
      "Instantiating MultiSurv model...\n",
      "âœ… GPU-enabled MultiSurv created!\n"
     ]
    }
   ],
   "source": [
    "gpu_multisurv = create_gpu_enabled_multisurv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output intervals (in years):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  1.6767,  2.7123,  3.7342,  4.6247,  6.2712,  6.8849,  7.5808,\n",
       "         8.0000,  8.9342, 10.0274, 10.6110, 10.7973, 11.7397, 17.2384, 17.6877,\n",
       "        19.5233, 23.5753], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Output intervals (in years):')\n",
    "gpu_multisurv.output_intervals / 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DNAm_submodel', 'fc_block', 'risk_layer']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_multisurv.model_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable blocks:\n",
      "   DNAm_submodel: True\n",
      "   fc_block: True\n",
      "   risk_layer: True\n"
     ]
    }
   ],
   "source": [
    "print('Trainable blocks:')\n",
    "layer = None\n",
    "\n",
    "for name, child in gpu_multisurv.model.named_children():\n",
    "    for name_2, params in child.named_parameters():\n",
    "        if name is not layer:\n",
    "            print(f'   {name}: {params.requires_grad}')\n",
    "        layer = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiSurv(\n",
       "  (DNAm_submodel): FC(\n",
       "    (fc): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Linear(in_features=3764, out_features=8192, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): Dropout(p=0.5, inplace=False)\n",
       "      (5): Linear(in_features=8192, out_features=8192, bias=True)\n",
       "      (6): ReLU()\n",
       "      (7): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): Dropout(p=0.5, inplace=False)\n",
       "      (9): Linear(in_features=8192, out_features=8192, bias=True)\n",
       "      (10): ReLU()\n",
       "      (11): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): Dropout(p=0.5, inplace=False)\n",
       "      (13): Linear(in_features=8192, out_features=8192, bias=True)\n",
       "      (14): ReLU()\n",
       "      (15): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (16): Dropout(p=0.5, inplace=False)\n",
       "      (17): Linear(in_features=8192, out_features=512, bias=True)\n",
       "      (18): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (fc_block): FC(\n",
       "    (fc): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): Dropout(p=0.5, inplace=False)\n",
       "      (5): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (6): ReLU()\n",
       "      (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): Dropout(p=0.5, inplace=False)\n",
       "      (9): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (10): ReLU()\n",
       "      (11): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): Dropout(p=0.5, inplace=False)\n",
       "      (13): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (14): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (risk_layer): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=17, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_multisurv.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ GPU Memory: 1.85 GB allocated, 1.87 GB reserved\n",
      "ðŸ” GPU cache size: 1.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10313/1937229576.py:9: FutureWarning: `torch.cuda.memory_cached` has been renamed to `torch.cuda.memory_reserved`\n",
      "  print(f\"ðŸ” GPU cache size: {torch.cuda.memory_cached(0) / 1024**3:.2f} GB\")\n"
     ]
    }
   ],
   "source": [
    "# Check GPU usage after LR test\n",
    "def monitor_gpu_real_time():\n",
    "    import torch\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"ðŸŽ¯ GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "    \n",
    "    # Check if there are any tensors on GPU\n",
    "    print(f\"ðŸ” GPU cache size: {torch.cuda.memory_cached(0) / 1024**3:.2f} GB\")\n",
    "monitor_gpu_real_time()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_example, _, _ = dataloaders['train'].dataset[0]\n",
    "print(\"Categorical features:\\n\", data_example['clinical'][0])\n",
    "print(\"Continuous features:\\n\", data_example['clinical'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied GPU-compatible fix to LRRangeTest.run\n"
     ]
    }
   ],
   "source": [
    "# GPU-compatible version of the completely fixed LR test\n",
    "\n",
    "def gpu_fixed_lr_test_run(self, init_value=1e-8, final_value=10., beta=0.98):\n",
    "    \"\"\"GPU-compatible LR test that ensures all tensors are on the same device.\"\"\"\n",
    "    print(\">>> Using GPU-COMPATIBLE FIXED lr_test.run method\")\n",
    "    \n",
    "    power = (1 / (len(self.dataloader) - 1))\n",
    "    mult = (final_value / init_value) ** power\n",
    "    lr = init_value\n",
    "    self.optimizer.param_groups[0]['lr'] = lr\n",
    "    avg_loss = 0.\n",
    "    best_loss = 0.\n",
    "    batch_num = 0\n",
    "\n",
    "    print('>>> Compute loss at increasing LR values')\n",
    "    \n",
    "    # Clear losses and lrs lists\n",
    "    self.losses = []\n",
    "    self.lrs = []\n",
    "\n",
    "    for data in self.dataloader:\n",
    "        batch_num += 1\n",
    "        print('\\r' + f'    Iterate over mini-batches: {str(batch_num)}', end='')\n",
    "\n",
    "        try:\n",
    "            # Unpack data\n",
    "            if len(data) == 3:\n",
    "                modality_data, time, event = data\n",
    "            elif len(data) == 4:\n",
    "                modality_data, time, event, pid = data\n",
    "            \n",
    "            # CRITICAL: Move ALL data to the same device as the model\n",
    "            target_device = next(self.model.parameters()).device\n",
    "            \n",
    "            # Move modality data to device\n",
    "            for key, value in modality_data.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    modality_data[key] = value.to(target_device)\n",
    "                elif isinstance(value, (list, tuple)):\n",
    "                    modality_data[key] = tuple(v.to(target_device) if isinstance(v, torch.Tensor) else v for v in value)\n",
    "            \n",
    "            # Move time and event to device\n",
    "            time = time.to(target_device)\n",
    "            event = event.to(target_device)\n",
    "            \n",
    "            # Set model to train mode\n",
    "            self.model.train()\n",
    "            \n",
    "            # Clear gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.set_grad_enabled(True):\n",
    "                feature_representations, risk = self.model(modality_data)\n",
    "                modality_features = feature_representations['modalities']\n",
    "                \n",
    "                # Compute loss with proper device handling\n",
    "                try:\n",
    "                    # Ensure output_intervals are on the same device\n",
    "                    if hasattr(self, 'output_intervals'):\n",
    "                        breaks = self.output_intervals.to(target_device)\n",
    "                    else:\n",
    "                        breaks = torch.linspace(0, 10, 19, device=target_device)  # Fallback\n",
    "                    \n",
    "                    loss = self.criterion(risk, times=time, events=event, breaks=breaks, device=target_device)\n",
    "                    \n",
    "                except Exception as loss_error:\n",
    "                    print(f\"\\nPrimary criterion failed: {loss_error}\")\n",
    "                    # Fallback to simple MSE loss on the same device\n",
    "                    dummy_target = torch.ones_like(risk)  # This will be on the same device as risk\n",
    "                    loss = torch.nn.functional.mse_loss(risk, dummy_target)\n",
    "                    print(\"Using fallback MSE loss\")\n",
    "                \n",
    "                # Add auxiliary loss if needed\n",
    "                if self.aux_criterion is not None:\n",
    "                    try:\n",
    "                        if len(modality_features) >= 2:\n",
    "                            target = torch.ones(modality_features[0].shape[0], device=target_device)\n",
    "                            aux_loss = self.aux_criterion(modality_features[0], modality_features[1], target)\n",
    "                            loss = loss + 0.1 * aux_loss\n",
    "                    except Exception as aux_error:\n",
    "                        print(f\"\\nAuxiliary criterion failed: {aux_error}\")\n",
    "            \n",
    "            # Store the loss value\n",
    "            loss_value = loss.item()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError at batch {batch_num}: {e}\")\n",
    "            break\n",
    "\n",
    "        # Compute the smoothed loss\n",
    "        avg_loss = (beta * avg_loss + (1 - beta) * loss_value)\n",
    "        smoothed_loss = avg_loss / (1 - beta ** batch_num)\n",
    "\n",
    "        # Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "            print()\n",
    "            print('    Exploding loss; finish test.')\n",
    "            break\n",
    "\n",
    "        # Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num == 1:\n",
    "            best_loss = smoothed_loss\n",
    "            \n",
    "        # Store the values\n",
    "        self.losses.append(smoothed_loss)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # Do the backward pass and optimizer step\n",
    "        try:\n",
    "            loss.backward(retain_graph=False)\n",
    "            self.optimizer.step()\n",
    "        except Exception as e:\n",
    "            print(f\"\\nBackward pass failed at batch {batch_num}: {e}\")\n",
    "            break\n",
    "\n",
    "        # Update the lr for the next step\n",
    "        lr *= mult\n",
    "        self.optimizer.param_groups[0]['lr'] = lr\n",
    "        \n",
    "        # Stop after reasonable number of batches\n",
    "        if batch_num >= 100:\n",
    "            print(\"\\nStopping after 100 batches\")\n",
    "            break\n",
    "\n",
    "    print()\n",
    "    print('    Completed test.')\n",
    "    return self\n",
    "\n",
    "# Apply the GPU-compatible fix\n",
    "import lr_range_test\n",
    "lr_range_test.LRRangeTest.run = gpu_fixed_lr_test_run\n",
    "print(\"Applied GPU-compatible fix to LRRangeTest.run\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Ultimate fix: Replace the entire lr_range_test.py run method\n",
    "\n",
    "def completely_fixed_lr_test_run(self, init_value=1e-8, final_value=10., beta=0.98):\n",
    "    \"\"\"Completely rewritten LR test that avoids the inplace operation issue.\"\"\"\n",
    "    print(\">>> Using COMPLETELY FIXED lr_test.run method\")\n",
    "    \n",
    "    power = (1 / (len(self.dataloader) - 1))\n",
    "    mult = (final_value / init_value) ** power\n",
    "    lr = init_value\n",
    "    self.optimizer.param_groups[0]['lr'] = lr\n",
    "    avg_loss = 0.\n",
    "    best_loss = 0.\n",
    "    batch_num = 0\n",
    "\n",
    "    print('>>> Compute loss at increasing LR values')\n",
    "    \n",
    "    # Clear losses and lrs lists\n",
    "    self.losses = []\n",
    "    self.lrs = []\n",
    "\n",
    "    for data in self.dataloader:\n",
    "        batch_num += 1\n",
    "        print('\\r' + f'    Iterate over mini-batches: {str(batch_num)}', end='')\n",
    "\n",
    "        try:\n",
    "            # COMPLETELY MANUAL APPROACH - avoid ModelCoach entirely\n",
    "            \n",
    "            # Unpack data\n",
    "            if len(data) == 3:\n",
    "                modality_data, time, event = data\n",
    "            elif len(data) == 4:\n",
    "                modality_data, time, event, pid = data\n",
    "            \n",
    "            # Move to device manually\n",
    "            for key, value in modality_data.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    modality_data[key] = value.to(self.device)\n",
    "                elif isinstance(value, (list, tuple)):\n",
    "                    modality_data[key] = tuple(v.to(self.device) if isinstance(v, torch.Tensor) else v for v in value)\n",
    "            \n",
    "            time = time.to(self.device)\n",
    "            event = event.to(self.device)\n",
    "            \n",
    "            # Set model to train mode\n",
    "            self.model.train()\n",
    "            \n",
    "            # Clear gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.set_grad_enabled(True):\n",
    "                feature_representations, risk = self.model(modality_data)\n",
    "                modality_features = feature_representations['modalities']\n",
    "                \n",
    "                # Compute loss manually (fix the arguments)\n",
    "                try:\n",
    "                    # The loss function signature is: forward(risk, times, events, breaks, device)\n",
    "                    # NOT forward(risk, times, events, modality_features, breaks)\n",
    "                    breaks = self.output_intervals\n",
    "                    if isinstance(breaks, (list, tuple)):\n",
    "                        breaks = torch.tensor(breaks, dtype=torch.float32, device=self.device)\n",
    "                    \n",
    "                    loss = self.criterion(risk, times=time, events=event, breaks=breaks, device=self.device)\n",
    "                        \n",
    "                except Exception as loss_error:\n",
    "                    print(f\"\\nPrimary criterion failed: {loss_error}\")\n",
    "                    # Fallback to simple MSE loss for LR range test\n",
    "                    dummy_target = torch.ones_like(risk)\n",
    "                    loss = torch.nn.functional.mse_loss(risk, dummy_target)\n",
    "                    print(\"Using fallback MSE loss\")\n",
    "                \n",
    "                # Add auxiliary loss if needed (fix the argument mismatch)\n",
    "                if self.aux_criterion is not None:\n",
    "                    try:\n",
    "                        # CosineEmbeddingLoss expects (input1, input2, target)\n",
    "                        # Let's use the two modality features\n",
    "                        if len(modality_features) >= 2:\n",
    "                            # Create a dummy target (1 for similar, -1 for dissimilar)\n",
    "                            target = torch.ones(modality_features[0].shape[0], device=self.device)\n",
    "                            aux_loss = self.aux_criterion(modality_features[0], modality_features[1], target)\n",
    "                            loss = loss + 0.1 * aux_loss  # Scale down aux loss\n",
    "                        else:\n",
    "                            print(\"Skipping aux loss - insufficient modality features\")\n",
    "                    except Exception as aux_error:\n",
    "                        print(f\"\\nAuxiliary criterion failed: {aux_error}\")\n",
    "                        print(\"Skipping auxiliary loss\")\n",
    "            \n",
    "            # Store the loss value\n",
    "            loss_value = loss.item()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError at batch {batch_num}: {e}\")\n",
    "            break\n",
    "\n",
    "        # Compute the smoothed loss\n",
    "        avg_loss = (beta * avg_loss + (1 - beta) * loss_value)\n",
    "        smoothed_loss = avg_loss / (1 - beta ** batch_num)\n",
    "\n",
    "        # Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "            print()\n",
    "            print('    Exploding loss; finish test.')\n",
    "            break\n",
    "\n",
    "        # Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num == 1:\n",
    "            best_loss = smoothed_loss\n",
    "            \n",
    "        # Store the values\n",
    "        self.losses.append(smoothed_loss)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # Do the backward pass and optimizer step\n",
    "        try:\n",
    "            # CRITICAL: Use retain_graph=False and no double backward pass\n",
    "            loss.backward(retain_graph=False)\n",
    "            self.optimizer.step()\n",
    "        except Exception as e:\n",
    "            print(f\"\\nBackward pass failed at batch {batch_num}: {e}\")\n",
    "            break\n",
    "\n",
    "        # Update the lr for the next step\n",
    "        lr *= mult\n",
    "        self.optimizer.param_groups[0]['lr'] = lr\n",
    "        \n",
    "        # Stop after reasonable number of batches to avoid infinite loops\n",
    "        if batch_num >= 100:\n",
    "            print(\"\\nStopping after 100 batches\")\n",
    "            break\n",
    "\n",
    "    print()\n",
    "    print('    Completed test.')\n",
    "    return self\n",
    "\n",
    "# Apply the complete fix\n",
    "import lr_range_test\n",
    "lr_range_test.LRRangeTest.run = completely_fixed_lr_test_run\n",
    "print(\"Applied COMPLETE fix to LRRangeTest.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Using GPU-COMPATIBLE FIXED lr_test.run method\n",
      ">>> Compute loss at increasing LR values\n",
      "    Iterate over mini-batches: 44\n",
      "    Completed test.\n",
      "CPU times: user 2.98 s, sys: 229 ms, total: 3.2 s\n",
      "Wall time: 6.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)  # Disable anomaly detection too\n",
    "gpu_multisurv.test_lr_range()  # Use the new GPU instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loss values collected: 44\n",
      "Number of learning rates tested: 44\n",
      "LR range: 1.00e-06 to 1.00e+01\n",
      "Loss range: 0.041379 to 0.056957\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the results\n",
    "print(f\"Number of loss values collected: {len(gpu_multisurv.lr_test.losses)}\")\n",
    "print(f\"Number of learning rates tested: {len(gpu_multisurv.lr_test.lrs)}\")\n",
    "print(f\"LR range: {min(gpu_multisurv.lr_test.lrs):.2e} to {max(gpu_multisurv.lr_test.lrs):.2e}\")\n",
    "print(f\"Loss range: {min(gpu_multisurv.lr_test.losses):.6f} to {max(gpu_multisurv.lr_test.losses):.6f}\")\n",
    "\n",
    "# Plot the results to find optimal learning rate\n",
    "gpu_multisurv.plot_lr_range(trim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Fix for the _predictions_to_pycox method in coach.py\n",
    "\n",
    "def fixed_predictions_to_pycox(self, preds, time_points=None):\n",
    "    \"\"\"Fixed version that properly handles the DataFrame structure.\"\"\"\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    \n",
    "    # preds shape is [num_patients, num_intervals] e.g., [864, 18]\n",
    "    # Convert predictions to DataFrame\n",
    "    df = pd.DataFrame(preds.cpu().numpy())\n",
    "    \n",
    "    if time_points is None:\n",
    "        # Get the number of intervals from the model output\n",
    "        n_intervals = preds.shape[1]  # Should be 18 for your model\n",
    "        \n",
    "        # Use the model's actual output intervals\n",
    "        if hasattr(self, 'output_intervals') and self.output_intervals is not None:\n",
    "            # Use the midpoints of the output intervals\n",
    "            intervals = self.output_intervals\n",
    "            if len(intervals) > n_intervals:\n",
    "                # Take the first n_intervals midpoints\n",
    "                midpoints = (intervals[:-1] + intervals[1:]) / 2\n",
    "                time_points = midpoints[:n_intervals]\n",
    "            else:\n",
    "                # Fallback to evenly spaced points\n",
    "                time_points = torch.linspace(0.5, intervals[-1].item() / 365, n_intervals)\n",
    "        else:\n",
    "            # Fallback: create time points that match the output size\n",
    "            time_points = torch.arange(0.5, 0.5 + n_intervals, 1.0)\n",
    "    \n",
    "    # Ensure time_points matches the prediction dimensions\n",
    "    if len(time_points) != preds.shape[1]:\n",
    "        print(f\"Warning: Adjusting time_points from {len(time_points)} to {preds.shape[1]}\")\n",
    "        time_points = torch.linspace(time_points[0], time_points[-1], preds.shape[1])\n",
    "    \n",
    "    # FIXED: The DataFrame structure should be transposed\n",
    "    # We want columns to be time points, rows to be patients\n",
    "    df = df.T  # Transpose so shape becomes [num_intervals, num_patients]\n",
    "    \n",
    "    # Convert time_points to numpy if it's a tensor\n",
    "    if torch.is_tensor(time_points):\n",
    "        time_points = time_points.cpu().numpy()\n",
    "    \n",
    "    # Set the index to time points\n",
    "    df.index = time_points\n",
    "    df.index.name = 'time'\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the fix to ModelCoach\n",
    "from coach import ModelCoach\n",
    "ModelCoach._predictions_to_pycox = fixed_predictions_to_pycox\n",
    "print(\"Applied corrected fix to ModelCoach._predictions_to_pycox method\")\n",
    "\n",
    "# Debug info\n",
    "print(f\"Model output intervals shape: {len(gpu_multisurv.output_intervals)}\")\n",
    "print(f\"Model risk layer output: {gpu_multisurv.model.risk_layer[0].out_features}\")\n",
    "print(f\"Expected: {len(gpu_multisurv.output_intervals) - 1} intervals for {len(gpu_multisurv.output_intervals)} breakpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Applied GPU-compatible fix to ModelCoach._predictions_to_pycox method\n",
      "Model output intervals shape: 18\n",
      "Model output intervals device: cuda:0\n",
      "Model risk layer output: 17\n",
      "Expected: 17 intervals for 18 breakpoints\n"
     ]
    }
   ],
   "source": [
    "# GPU-compatible fix for _predictions_to_pycox method\n",
    "def gpu_fixed_predictions_to_pycox(self, preds, time_points=None):\n",
    "    \"\"\"GPU-compatible version that properly handles device transfers.\"\"\"\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    \n",
    "    # preds shape is [num_patients, num_intervals] e.g., [864, 18]\n",
    "    # Convert predictions to DataFrame (move to CPU first)\n",
    "    df = pd.DataFrame(preds.detach().cpu().numpy())  # Added .detach() for GPU tensors\n",
    "    \n",
    "    if time_points is None:\n",
    "        # Get the number of intervals from the model output\n",
    "        n_intervals = preds.shape[1]  # Should be 18 for your model\n",
    "        \n",
    "        # Use the model's actual output intervals\n",
    "        if hasattr(self, 'output_intervals') and self.output_intervals is not None:\n",
    "            # Handle GPU tensors properly\n",
    "            intervals = self.output_intervals\n",
    "            if torch.is_tensor(intervals):\n",
    "                intervals = intervals.detach().cpu()  # Move to CPU for calculations\n",
    "            \n",
    "            if len(intervals) > n_intervals:\n",
    "                # Take the first n_intervals midpoints\n",
    "                midpoints = (intervals[:-1] + intervals[1:]) / 2\n",
    "                time_points = midpoints[:n_intervals]\n",
    "            else:\n",
    "                # Fallback to evenly spaced points\n",
    "                last_interval = intervals[-1].item() if torch.is_tensor(intervals) else intervals[-1]\n",
    "                time_points = torch.linspace(0.5, last_interval / 365, n_intervals)\n",
    "        else:\n",
    "            # Fallback: create time points that match the output size\n",
    "            time_points = torch.arange(0.5, 0.5 + n_intervals, 1.0)\n",
    "    \n",
    "    # Ensure time_points matches the prediction dimensions\n",
    "    if len(time_points) != preds.shape[1]:\n",
    "        print(f\"Warning: Adjusting time_points from {len(time_points)} to {preds.shape[1]}\")\n",
    "        first_point = time_points[0].item() if torch.is_tensor(time_points) else time_points[0]\n",
    "        last_point = time_points[-1].item() if torch.is_tensor(time_points) else time_points[-1]\n",
    "        time_points = torch.linspace(first_point, last_point, preds.shape[1])\n",
    "    \n",
    "    # FIXED: The DataFrame structure should be transposed\n",
    "    # We want columns to be time points, rows to be patients\n",
    "    df = df.T  # Transpose so shape becomes [num_intervals, num_patients]\n",
    "    \n",
    "    # Convert time_points to numpy if it's a tensor (ensure CPU)\n",
    "    if torch.is_tensor(time_points):\n",
    "        time_points = time_points.detach().cpu().numpy()\n",
    "    \n",
    "    # Set the index to time points\n",
    "    df.index = time_points\n",
    "    df.index.name = 'time'\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the GPU-compatible fix to ModelCoach\n",
    "from coach import ModelCoach\n",
    "ModelCoach._predictions_to_pycox = gpu_fixed_predictions_to_pycox\n",
    "print(\"âœ… Applied GPU-compatible fix to ModelCoach._predictions_to_pycox method\")\n",
    "\n",
    "# Debug info with GPU handling\n",
    "print(f\"Model output intervals shape: {len(gpu_multisurv.output_intervals)}\")\n",
    "print(f\"Model output intervals device: {gpu_multisurv.output_intervals.device}\")\n",
    "print(f\"Model risk layer output: {gpu_multisurv.model.risk_layer[0].out_features}\")\n",
    "print(f\"Expected: {len(gpu_multisurv.output_intervals) - 1} intervals for {len(gpu_multisurv.output_intervals)} breakpoints\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import utils\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# 2. Set up training parameters\n",
    "picked_lr = 5e-3\n",
    "\n",
    "# Add timestamp to make unique run tags\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "run_tag = utils.compose_run_tag(\n",
    "    model=gpu_multisurv, \n",
    "    lr=picked_lr,\n",
    "    dataloaders=gpu_multisurv.dataloaders,\n",
    "    log_dir='./training_logs/',\n",
    "    suffix=f'_breast_cancer_{timestamp}'  # Add timestamp\n",
    ")\n",
    "\n",
    "# 3. Create log directory (now it will be unique)\n",
    "log_dir = os.path.join('./training_logs/', run_tag)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "print(f\"Log directory created: {log_dir}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "fit_args = {\n",
    "    'lr': picked_lr,\n",
    "    'num_epochs': 75,\n",
    "    'info_freq': 5,\n",
    "    'lr_factor': 0.5,\n",
    "    'scheduler_patience': 10,\n",
    "    'log_dir': log_dir,  # Use the verified directory\n",
    "}\n",
    "\n",
    "print(f\"Starting training with LR: {picked_lr}\")\n",
    "\n",
    "# 4. Train the model\n",
    "gpu_multisurv.fit(**fit_args)\n",
    "\n",
    "# 5. Calculate and display elapsed time\n",
    "hrs, mins, secs = utils.elapsed_time(start_time)\n",
    "print(f\"Training completed in {hrs}h {mins}m {secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRE-TRAINING GPU VERIFICATION ===\n",
      "GPU model device: cuda:0\n",
      "GPU intervals device: cuda:0\n",
      "GPU device setting: cuda:0\n",
      "GPU memory before training: 4.62 GB\n",
      "Run tag: \"DNAm_lr0.00035_breast_cancer_gpu_20250630_194833\"\n",
      "âœ… Log directory created: ./training_logs/DNAm_lr0.00035_breast_cancer_gpu_20250630_194833\n",
      "âœ… Setup complete - ready for GPU training!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup with GPU verification\n",
    "import os\n",
    "import time\n",
    "import utils\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# 2. Verify GPU setup before training\n",
    "print(\"=== PRE-TRAINING GPU VERIFICATION ===\")\n",
    "print(f\"GPU model device: {next(gpu_multisurv.model.parameters()).device}\")\n",
    "print(f\"GPU intervals device: {gpu_multisurv.output_intervals.device}\")\n",
    "print(f\"GPU device setting: {gpu_multisurv.device}\")\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"GPU memory before training: {allocated:.2f} GB\")\n",
    "\n",
    "# 3. Set up training parameters\n",
    "picked_lr = 3.5e-4\n",
    "picked_num_epochs = 60\n",
    "picked_info_freq = 5\n",
    "picked_lr_factor = 0.5\n",
    "picked_scheduler_patience = 5\n",
    "picked_weight_decay = 1e-4\n",
    "\n",
    "# Add timestamp to make unique run tags\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "run_tag = utils.compose_run_tag(\n",
    "    model=gpu_multisurv, \n",
    "    lr=picked_lr,\n",
    "    dataloaders=gpu_multisurv.dataloaders,\n",
    "    log_dir='./training_logs/',\n",
    "    suffix=f'_breast_cancer_gpu_{timestamp}'  # Added 'gpu' to indicate GPU training\n",
    ")\n",
    "\n",
    "# 4. Create log directory\n",
    "log_dir = os.path.join('./training_logs/', run_tag)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "print(f\"âœ… Log directory created: {log_dir}\")\n",
    "\n",
    "print(\"âœ… Setup complete - ready for GPU training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting GPU training with LR: 0.00035\n",
      "ðŸ“Š Expected to see GPU utilization spike during training\n",
      "ðŸ“ Logs will be saved to: ./training_logs/DNAm_lr0.00035_breast_cancer_gpu_20250630_194833\n",
      "\n",
      "=== GPU STATUS BEFORE TRAINING ===\n",
      "ðŸŽ¯ GPU Memory: 4.62 GB allocated, 5.57 GB reserved\n",
      "\n",
      "â° Training started at: 19:48:44\n",
      "Instantiating MultiSurv model...\n",
      "\n",
      "------------------------------------------\n",
      "             Training        Validation\n",
      "           ------------     ------------\n",
      " Epoch     Loss     Ctd     Loss     Ctd\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/multisurv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/60     0.0337   0.505   0.0172   0.695\n",
      " 5/60     0.0179   0.512   0.0178   0.637\n",
      " 10/60    0.0153   0.607   0.0180   0.615\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Training execution with GPU monitoring\n",
    "fit_args = {\n",
    "    'lr': picked_lr,\n",
    "    'num_epochs': picked_num_epochs,\n",
    "    'info_freq': picked_info_freq,\n",
    "    'lr_factor': picked_lr_factor,\n",
    "    'scheduler_patience': picked_scheduler_patience,\n",
    "    'log_dir': log_dir,\n",
    "    'weight_decay':picked_weight_decay\n",
    "}\n",
    "\n",
    "print(f\"ðŸš€ Starting GPU training with LR: {picked_lr}\")\n",
    "print(f\"ðŸ“Š Expected to see GPU utilization spike during training\")\n",
    "print(f\"ðŸ“ Logs will be saved to: {log_dir}\")\n",
    "\n",
    "# Function to monitor GPU during training\n",
    "def check_gpu_status():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        print(f\"ðŸŽ¯ GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "    else:\n",
    "        print(\"âŒ CUDA not available\")\n",
    "\n",
    "\n",
    "# Check GPU before training\n",
    "print(\"\\n=== GPU STATUS BEFORE TRAINING ===\")\n",
    "check_gpu_status()\n",
    "\n",
    "# Clear any cached memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Start training\n",
    "print(f\"\\nâ° Training started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "try:\n",
    "    # This is where the actual training happens\n",
    "    gpu_multisurv.fit(**fit_args)\n",
    "    \n",
    "    print(f\"\\nâœ… Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Training failed with error: {e}\")\n",
    "    print(\"This might be due to GPU memory issues or device mismatches\")\n",
    "    # Print more debug info\n",
    "    print(f\"Model device: {next(gpu_multisurv.model.parameters()).device}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Check GPU after training\n",
    "print(\"\\n=== GPU STATUS AFTER TRAINING ===\")\n",
    "check_gpu_status()\n",
    "\n",
    "# Calculate and display elapsed time\n",
    "hrs, mins, secs = utils.elapsed_time(start_time)\n",
    "print(f\"\\nâ° Training completed in {hrs}h {mins}m {secs}s\")\n",
    "print(f\"ðŸ Training finished at: {datetime.now().strftime('%H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "from collections import deque\n",
    "import utils\n",
    "\n",
    "class AutoTrainer:\n",
    "    \"\"\"Automated training system with intelligent decision making.\"\"\"\n",
    "    \n",
    "    def __init__(self, gpu_model, device, models_dir='./models/', log_base_dir='./training_logs/'):\n",
    "        self.model = gpu_model\n",
    "        self.device = device\n",
    "        self.models_dir = models_dir\n",
    "        self.log_base_dir = log_base_dir\n",
    "        \n",
    "        # Training parameters\n",
    "        self.base_lr = 1e-5\n",
    "        self.num_epochs = 60\n",
    "        self.info_freq = 5\n",
    "        self.lr_factor = 0.5\n",
    "        self.scheduler_patience = 5\n",
    "        self.weight_decay = 1e-4\n",
    "        \n",
    "        # Decision thresholds\n",
    "        self.overfitting_gap_warning = 0.15\n",
    "        self.overfitting_gap_critical = 0.25\n",
    "        self.overfitting_loss_ratio = 2.0\n",
    "        self.convergence_variance = 0.02\n",
    "        self.convergence_loss_variance = 0.001\n",
    "        self.min_acceptable_performance = 0.55\n",
    "        self.smoothing_window = 3\n",
    "        \n",
    "        # Training phases\n",
    "        self.exploration_epochs = 20\n",
    "        self.refinement_epochs = 40\n",
    "        \n",
    "        # Tracking\n",
    "        self.checkpoint_candidates = {}  # epoch -> metrics\n",
    "        self.training_history = {\n",
    "            'train_loss': [], 'train_concord': [],\n",
    "            'val_loss': [], 'val_concord': [],\n",
    "            'smoothed_val_concord': []\n",
    "        }\n",
    "        self.lr_reductions = 0\n",
    "        self.overfitting_warnings = 0\n",
    "        \n",
    "    def train(self, dataloaders):\n",
    "        \"\"\"Main training orchestration with automated decisions.\"\"\"\n",
    "        print(\"ðŸš€ Starting Automated Training Process\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Setup training\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        run_tag = utils.compose_run_tag(\n",
    "            model=self.model, \n",
    "            lr=self.base_lr,\n",
    "            dataloaders=dataloaders,\n",
    "            log_dir=self.log_base_dir,\n",
    "            suffix=f'_auto_train_{timestamp}'\n",
    "        )\n",
    "        log_dir = os.path.join(self.log_base_dir, run_tag)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "        # Initial setup\n",
    "        current_lr = self.base_lr\n",
    "        patience_counter = 0\n",
    "        best_smoothed_concord = 0\n",
    "        convergence_history = deque(maxlen=10)\n",
    "        \n",
    "        print(f\"ðŸ“ Logs: {log_dir}\")\n",
    "        print(f\"ðŸŽ¯ Initial LR: {current_lr}\")\n",
    "        \n",
    "        # Training loop with checkpoints\n",
    "        epoch = 0\n",
    "        while epoch < self.num_epochs:\n",
    "            # Determine training phase\n",
    "            phase = self._get_training_phase(epoch)\n",
    "            \n",
    "            # Train for a batch of epochs\n",
    "            batch_epochs = min(self.info_freq, self.num_epochs - epoch)\n",
    "            \n",
    "            fit_args = {\n",
    "                'lr': current_lr,\n",
    "                'num_epochs': batch_epochs,\n",
    "                'info_freq': self.info_freq,\n",
    "                'lr_factor': self.lr_factor,\n",
    "                'scheduler_patience': self.scheduler_patience,\n",
    "                'log_dir': log_dir,\n",
    "                'weight_decay': self.weight_decay\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                print(f\"\\nðŸ“Š Training epochs {epoch+1}-{epoch+batch_epochs} (Phase: {phase})\")\n",
    "                self.model.fit(**fit_args)\n",
    "                \n",
    "                # Extract metrics from tensorboard logs\n",
    "                metrics = self._extract_metrics_from_logs(log_dir, up_to_epoch=epoch+batch_epochs)\n",
    "                \n",
    "                # Update tracking\n",
    "                self._update_tracking(metrics)\n",
    "                \n",
    "                # Get current performance\n",
    "                current_metrics = self._get_current_metrics(epoch+batch_epochs)\n",
    "                smoothed_concord = current_metrics['smoothed_val_concord']\n",
    "                \n",
    "                print(f\"\\nðŸ“ˆ Epoch {epoch+batch_epochs} Summary:\")\n",
    "                print(f\"   Val Concord: {current_metrics['val_concord']:.4f} (smoothed: {smoothed_concord:.4f})\")\n",
    "                print(f\"   Generalization gap: {current_metrics['gap']:.4f}\")\n",
    "                print(f\"   Loss ratio: {current_metrics['loss_ratio']:.2f}\")\n",
    "                \n",
    "                # Make decisions\n",
    "                decision = self._make_decision(\n",
    "                    current_metrics, phase, patience_counter, \n",
    "                    best_smoothed_concord, convergence_history\n",
    "                )\n",
    "                \n",
    "                # Execute decision\n",
    "                if decision['action'] == 'stop':\n",
    "                    print(f\"\\nðŸ›‘ Stopping: {decision['reason']}\")\n",
    "                    break\n",
    "                    \n",
    "                elif decision['action'] == 'reduce_lr':\n",
    "                    current_lr *= self.lr_factor\n",
    "                    self.lr_reductions += 1\n",
    "                    patience_counter = 0\n",
    "                    print(f\"\\nðŸ“‰ Reducing LR to {current_lr:.6f}\")\n",
    "                    \n",
    "                elif decision['action'] == 'continue':\n",
    "                    if smoothed_concord > best_smoothed_concord:\n",
    "                        best_smoothed_concord = smoothed_concord\n",
    "                        patience_counter = 0\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                \n",
    "                # Track checkpoint candidates\n",
    "                if self._is_good_checkpoint(current_metrics, phase):\n",
    "                    self._save_checkpoint_candidate(epoch+batch_epochs, current_metrics)\n",
    "                \n",
    "                # Update convergence history\n",
    "                convergence_history.append(smoothed_concord)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nâŒ Training error: {e}\")\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(\"ðŸ’¡ Consider reducing batch size\")\n",
    "                break\n",
    "            \n",
    "            epoch += batch_epochs\n",
    "        \n",
    "        # Final checkpoint selection and evaluation\n",
    "        print(\"\\nðŸŽ¯ Selecting best checkpoint...\")\n",
    "        best_checkpoint = self._select_best_checkpoint(dataloaders)\n",
    "        \n",
    "        # Final evaluation\n",
    "        print(\"\\nðŸ“Š Final Evaluation:\")\n",
    "        final_results = self._final_evaluation(best_checkpoint, dataloaders)\n",
    "        \n",
    "        # Training summary\n",
    "        hrs, mins, secs = utils.elapsed_time(start_time)\n",
    "        print(f\"\\nâœ… Training completed in {hrs}h {mins}m {secs}s\")\n",
    "        print(f\"ðŸ† Best checkpoint: Epoch {best_checkpoint['epoch']}\")\n",
    "        print(f\"   Val Ctd: {final_results['val_ctd']:.4f}\")\n",
    "        print(f\"   Test Ctd: {final_results['test_ctd']:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'best_checkpoint': best_checkpoint,\n",
    "            'final_results': final_results,\n",
    "            'run_tag': run_tag,\n",
    "            'training_history': self.training_history\n",
    "        }\n",
    "    \n",
    "    def _get_training_phase(self, epoch):\n",
    "        \"\"\"Determine current training phase.\"\"\"\n",
    "        if epoch < self.exploration_epochs:\n",
    "            return 'exploration'\n",
    "        elif epoch < self.refinement_epochs:\n",
    "            return 'refinement'\n",
    "        else:\n",
    "            return 'fine-tuning'\n",
    "    \n",
    "    def _extract_metrics_from_logs(self, log_dir, up_to_epoch):\n",
    "        \"\"\"Extract metrics from tensorboard logs.\"\"\"\n",
    "        event_files = glob.glob(os.path.join(log_dir, \"events.out.tfevents.*\"))\n",
    "        if not event_files:\n",
    "            return None\n",
    "            \n",
    "        event_file = sorted(event_files, key=os.path.getmtime)[-1]\n",
    "        ea = EventAccumulator(event_file, size_guidance={\"scalars\": 0})\n",
    "        ea.Reload()\n",
    "        \n",
    "        metrics = {}\n",
    "        for tag in ['train_loss', 'train_concord', 'val_loss', 'val_concord']:\n",
    "            data = [(e.step, e.value) for e in ea.Scalars(tag) if e.step <= up_to_epoch]\n",
    "            metrics[tag] = data\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def _update_tracking(self, metrics):\n",
    "        \"\"\"Update training history tracking.\"\"\"\n",
    "        if not metrics:\n",
    "            return\n",
    "            \n",
    "        for key in self.training_history:\n",
    "            if key != 'smoothed_val_concord' and key in metrics:\n",
    "                # Add new values\n",
    "                for step, value in metrics[key]:\n",
    "                    if step >= len(self.training_history[key]):\n",
    "                        self.training_history[key].append(value)\n",
    "        \n",
    "        # Update smoothed values\n",
    "        if len(self.training_history['val_concord']) >= self.smoothing_window:\n",
    "            vals = np.array(self.training_history['val_concord'])\n",
    "            smoothed = np.convolve(vals, np.ones(self.smoothing_window)/self.smoothing_window, mode='valid')\n",
    "            self.training_history['smoothed_val_concord'] = smoothed.tolist()\n",
    "    \n",
    "    def _get_current_metrics(self, epoch):\n",
    "        \"\"\"Get current training metrics.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Get latest values\n",
    "        if self.training_history['train_concord']:\n",
    "            metrics['train_concord'] = self.training_history['train_concord'][-1]\n",
    "        else:\n",
    "            metrics['train_concord'] = 0.5\n",
    "            \n",
    "        if self.training_history['val_concord']:\n",
    "            metrics['val_concord'] = self.training_history['val_concord'][-1]\n",
    "        else:\n",
    "            metrics['val_concord'] = 0.5\n",
    "            \n",
    "        if self.training_history['train_loss']:\n",
    "            metrics['train_loss'] = self.training_history['train_loss'][-1]\n",
    "        else:\n",
    "            metrics['train_loss'] = 1.0\n",
    "            \n",
    "        if self.training_history['val_loss']:\n",
    "            metrics['val_loss'] = self.training_history['val_loss'][-1]\n",
    "        else:\n",
    "            metrics['val_loss'] = 1.0\n",
    "        \n",
    "        # Smoothed concordance\n",
    "        if self.training_history['smoothed_val_concord']:\n",
    "            metrics['smoothed_val_concord'] = self.training_history['smoothed_val_concord'][-1]\n",
    "        else:\n",
    "            metrics['smoothed_val_concord'] = metrics['val_concord']\n",
    "        \n",
    "        # Derived metrics\n",
    "        metrics['gap'] = metrics['train_concord'] - metrics['val_concord']\n",
    "        metrics['loss_ratio'] = metrics['val_loss'] / max(metrics['train_loss'], 1e-6)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _make_decision(self, metrics, phase, patience_counter, best_smoothed, convergence_history):\n",
    "        \"\"\"Make training decision based on current metrics and phase.\"\"\"\n",
    "        decision = {'action': 'continue', 'reason': ''}\n",
    "        \n",
    "        # Check for convergence\n",
    "        if len(convergence_history) >= 10:\n",
    "            variance = np.var(list(convergence_history))\n",
    "            if variance < self.convergence_variance and self.lr_reductions > 0:\n",
    "                decision = {'action': 'stop', 'reason': 'Model converged'}\n",
    "                return decision\n",
    "        \n",
    "        # Check for severe overfitting\n",
    "        if metrics['gap'] > self.overfitting_gap_critical:\n",
    "            decision = {'action': 'stop', 'reason': f'Severe overfitting (gap={metrics[\"gap\"]:.3f})'}\n",
    "            return decision\n",
    "        \n",
    "        # Check for poor performance\n",
    "        if phase != 'exploration' and metrics['smoothed_val_concord'] < self.min_acceptable_performance:\n",
    "            decision = {'action': 'stop', 'reason': f'Performance below threshold ({metrics[\"smoothed_val_concord\"]:.3f})'}\n",
    "            return decision\n",
    "        \n",
    "        # Phase-specific decisions\n",
    "        if phase == 'exploration':\n",
    "            # Be patient in exploration\n",
    "            if patience_counter > 10:\n",
    "                decision = {'action': 'reduce_lr', 'reason': 'No improvement in exploration'}\n",
    "                \n",
    "        elif phase == 'refinement':\n",
    "            # More aggressive in refinement\n",
    "            if patience_counter > 5:\n",
    "                decision = {'action': 'reduce_lr', 'reason': 'No improvement in refinement'}\n",
    "            elif metrics['gap'] > self.overfitting_gap_warning:\n",
    "                self.overfitting_warnings += 1\n",
    "                if self.overfitting_warnings > 3:\n",
    "                    decision = {'action': 'reduce_lr', 'reason': 'Repeated overfitting warnings'}\n",
    "                    \n",
    "        else:  # fine-tuning\n",
    "            # Very aggressive in fine-tuning\n",
    "            if patience_counter > 3:\n",
    "                decision = {'action': 'stop', 'reason': 'No improvement in fine-tuning'}\n",
    "            elif metrics['loss_ratio'] > self.overfitting_loss_ratio:\n",
    "                decision = {'action': 'stop', 'reason': f'High loss ratio ({metrics[\"loss_ratio\"]:.2f})'}\n",
    "        \n",
    "        return decision\n",
    "    \n",
    "    def _is_good_checkpoint(self, metrics, phase):\n",
    "        \"\"\"Determine if current epoch is a good checkpoint candidate.\"\"\"\n",
    "        # Always save in exploration\n",
    "        if phase == 'exploration':\n",
    "            return True\n",
    "            \n",
    "        # Save if good performance and not overfitting\n",
    "        return (metrics['smoothed_val_concord'] > 0.6 and \n",
    "                metrics['gap'] < self.overfitting_gap_warning and\n",
    "                metrics['loss_ratio'] < 1.5)\n",
    "    \n",
    "    def _save_checkpoint_candidate(self, epoch, metrics):\n",
    "        \"\"\"Save checkpoint candidate information.\"\"\"\n",
    "        # Get actual model checkpoints\n",
    "        if hasattr(self.model, 'best_model_weights'):\n",
    "            available_epochs = list(self.model.best_model_weights.keys())\n",
    "            \n",
    "            # Find closest available epoch\n",
    "            closest_epoch = None\n",
    "            min_diff = float('inf')\n",
    "            for avail_epoch in available_epochs:\n",
    "                epoch_num = int(avail_epoch.replace('epoch', ''))\n",
    "                diff = abs(epoch_num - epoch)\n",
    "                if diff < min_diff:\n",
    "                    min_diff = diff\n",
    "                    closest_epoch = avail_epoch\n",
    "            \n",
    "            if closest_epoch and min_diff <= 2:  # Within 2 epochs\n",
    "                self.checkpoint_candidates[closest_epoch] = metrics.copy()\n",
    "    \n",
    "    def _select_best_checkpoint(self, dataloaders):\n",
    "        \"\"\"Select best checkpoint by evaluating candidates on full validation set.\"\"\"\n",
    "        print(\"\\nðŸ” Evaluating checkpoint candidates...\")\n",
    "        \n",
    "        best_checkpoint = None\n",
    "        best_score = 0\n",
    "        \n",
    "        # Get available checkpoints\n",
    "        available_checkpoints = []\n",
    "        if hasattr(self.model, 'best_model_weights'):\n",
    "            available_checkpoints.extend(list(self.model.best_model_weights.keys()))\n",
    "        if hasattr(self.model, 'current_concord'):\n",
    "            available_checkpoints.append('current')\n",
    "        \n",
    "        for checkpoint_epoch in available_checkpoints:\n",
    "            # Load checkpoint\n",
    "            try:\n",
    "                # Save current state\n",
    "                current_state = self.model.model.state_dict().copy()\n",
    "                \n",
    "                # Load checkpoint weights\n",
    "                if checkpoint_epoch == 'current':\n",
    "                    # Current weights are already loaded\n",
    "                    pass\n",
    "                else:\n",
    "                    # Load from best_model_weights\n",
    "                    checkpoint_state = self.model.best_model_weights[checkpoint_epoch]\n",
    "                    self.model.model.load_state_dict(checkpoint_state)\n",
    "                \n",
    "                # Evaluate on full validation set\n",
    "                performance = utils.Evaluation(\n",
    "                    model=self.model,\n",
    "                    dataset=dataloaders['val'].dataset,\n",
    "                    device=self.device\n",
    "                )\n",
    "                performance.compute_metrics()\n",
    "                \n",
    "                # Get metrics\n",
    "                val_ctd = performance.c_index_td if performance.c_index_td else 0\n",
    "                val_ibs = performance.ibs if performance.ibs else 1.0\n",
    "                \n",
    "                # Composite score (prioritize Ctd, penalize high IBS)\n",
    "                score = val_ctd - 0.1 * val_ibs\n",
    "                \n",
    "                print(f\"  {checkpoint_epoch}: Ctd={val_ctd:.4f}, IBS={val_ibs:.4f}, Score={score:.4f}\")\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_checkpoint = {\n",
    "                        'epoch': checkpoint_epoch,\n",
    "                        'val_ctd': val_ctd,\n",
    "                        'val_ibs': val_ibs,\n",
    "                        'score': score\n",
    "                    }\n",
    "                \n",
    "                # Restore original state\n",
    "                self.model.model.load_state_dict(current_state)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error evaluating {checkpoint_epoch}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Load best checkpoint\n",
    "        if best_checkpoint:\n",
    "            if best_checkpoint['epoch'] == 'current':\n",
    "                # Already using current weights\n",
    "                pass\n",
    "            else:\n",
    "                checkpoint_state = self.model.best_model_weights[best_checkpoint['epoch']]\n",
    "                self.model.model.load_state_dict(checkpoint_state)\n",
    "            print(f\"\\nâœ… Selected {best_checkpoint['epoch']} as best checkpoint\")\n",
    "        \n",
    "        return best_checkpoint\n",
    "    \n",
    "    def _final_evaluation(self, checkpoint, dataloaders):\n",
    "        \"\"\"Perform final evaluation on val and test sets.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Validation set evaluation\n",
    "        print(\"\\nðŸ“Š Validation Set Performance:\")\n",
    "        val_performance = utils.Evaluation(\n",
    "            model=self.model,\n",
    "            dataset=dataloaders['val'].dataset,\n",
    "            device=self.device\n",
    "        )\n",
    "        val_performance.compute_metrics()\n",
    "        val_performance.show_results()\n",
    "        \n",
    "        results['val_ctd'] = val_performance.c_index_td if val_performance.c_index_td else 0\n",
    "        results['val_ibs'] = val_performance.ibs if val_performance.ibs else 1.0\n",
    "        \n",
    "        # Test set evaluation\n",
    "        print(\"\\nðŸ“Š Test Set Performance:\")\n",
    "        test_performance = utils.Evaluation(\n",
    "            model=self.model,\n",
    "            dataset=dataloaders['test'].dataset,\n",
    "            device=self.device\n",
    "        )\n",
    "        test_performance.compute_metrics()\n",
    "        test_performance.show_results()\n",
    "        \n",
    "        results['test_ctd'] = test_performance.c_index_td if test_performance.c_index_td else 0\n",
    "        results['test_ibs'] = test_performance.ibs if test_performance.ibs else 1.0\n",
    "        \n",
    "        # Save best weights\n",
    "        if checkpoint:\n",
    "            save_name = f\"best_{checkpoint['epoch']}\"\n",
    "            self.model.save_weights(\n",
    "                saved_epoch=checkpoint['epoch'], \n",
    "                prefix=f\"auto_trained_{save_name}\", \n",
    "                weight_dir=self.models_dir\n",
    "            )\n",
    "            print(f\"\\nðŸ’¾ Saved best weights as: auto_trained_{save_name}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "def run_automated_training(gpu_multisurv, dataloaders, device, models_dir='./models/'):\n",
    "    \"\"\"Run automated training with intelligent decision making.\"\"\"\n",
    "    \n",
    "    # Initialize auto trainer\n",
    "    trainer = AutoTrainer(\n",
    "        gpu_model=gpu_multisurv,\n",
    "        device=device,\n",
    "        models_dir=MODELS\n",
    "    )\n",
    "    \n",
    "    # You can customize parameters if needed\n",
    "    # trainer.base_lr = 5e-4\n",
    "    # trainer.num_epochs = 80\n",
    "    # trainer.min_acceptable_performance = 0.60\n",
    "    \n",
    "    # Run training\n",
    "    results = trainer.train(dataloaders)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# To use:\n",
    "# results = run_automated_training(gpu_multisurv, dataloaders, device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "results = run_automated_training(gpu_multisurv, dataloaders, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model weights\n",
    "\n",
    "If desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using event file: /app/training_logs/mRNA_lr1e-06_breast_cancer_gpu_20250630_182536/events.out.tfevents.1751307937.4775115cad33.84349.0\n",
      "Available scalar tags: ['train_loss', 'train_concord', 'val_loss', 'val_concord']\n",
      "Best smoothed val_concord = 0.6786 at epoch 67\n",
      "\n",
      " epoch â”‚ raw val_concord â”‚ smoothed\n",
      "â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   3   â”‚      0.6775     â”‚  0.6559\n",
      "   4   â”‚      0.6869     â”‚  0.6605\n",
      "   5   â”‚      0.6708     â”‚  0.6784\n",
      "   6   â”‚      0.6669     â”‚  0.6749\n",
      "   7   â”‚      0.6507     â”‚  0.6628\n",
      "   8   â”‚      0.6797     â”‚  0.6657\n",
      "   9   â”‚      0.6719     â”‚  0.6674\n",
      "  10   â”‚      0.6775     â”‚  0.6763\n",
      "  11   â”‚      0.6172     â”‚  0.6555\n",
      "  12   â”‚      0.6473     â”‚  0.6473\n",
      "  13   â”‚      0.6596     â”‚  0.6414\n",
      "  14   â”‚      0.6635     â”‚  0.6568\n",
      "  15   â”‚      0.6434     â”‚  0.6555\n",
      "  16   â”‚      0.6652     â”‚  0.6574\n",
      "  17   â”‚      0.6529     â”‚  0.6538\n",
      "  18   â”‚      0.6719     â”‚  0.6633\n",
      "  19   â”‚      0.6629     â”‚  0.6626\n",
      "  20   â”‚      0.6713     â”‚  0.6687\n",
      "  21   â”‚      0.6557     â”‚  0.6633\n",
      "  22   â”‚      0.6747     â”‚  0.6672\n",
      "  23   â”‚      0.6735     â”‚  0.6680\n",
      "  24   â”‚      0.6680     â”‚  0.6721\n",
      "  25   â”‚      0.6674     â”‚  0.6696\n",
      "  26   â”‚      0.6702     â”‚  0.6685\n",
      "  27   â”‚      0.6819     â”‚  0.6732\n",
      "  28   â”‚      0.6613     â”‚  0.6711\n",
      "  29   â”‚      0.6747     â”‚  0.6726\n",
      "  30   â”‚      0.6562     â”‚  0.6641\n",
      "  31   â”‚      0.6669     â”‚  0.6659\n",
      "  32   â”‚      0.6680     â”‚  0.6637\n",
      "  33   â”‚      0.6769     â”‚  0.6706\n",
      "  34   â”‚      0.6814     â”‚  0.6754\n",
      "  35   â”‚      0.6769     â”‚  0.6784\n",
      "  36   â”‚      0.6730     â”‚  0.6771\n",
      "  37   â”‚      0.6741     â”‚  0.6747\n",
      "  38   â”‚      0.6624     â”‚  0.6698\n",
      "  39   â”‚      0.6769     â”‚  0.6711\n",
      "  40   â”‚      0.6713     â”‚  0.6702\n",
      "  41   â”‚      0.6166     â”‚  0.6549\n",
      "  42   â”‚      0.6713     â”‚  0.6531\n",
      "  43   â”‚      0.6724     â”‚  0.6535\n",
      "  44   â”‚      0.6758     â”‚  0.6732\n",
      "  45   â”‚      0.6456     â”‚  0.6646\n",
      "  46   â”‚      0.6741     â”‚  0.6652\n",
      "  47   â”‚      0.6825     â”‚  0.6674\n",
      "  48   â”‚      0.6702     â”‚  0.6756\n",
      "  49   â”‚      0.6814     â”‚  0.6780\n",
      "  50   â”‚      0.6691     â”‚  0.6735\n",
      "  51   â”‚      0.6579     â”‚  0.6695\n",
      "  52   â”‚      0.6384     â”‚  0.6551\n",
      "  53   â”‚      0.6747     â”‚  0.6570\n",
      "  54   â”‚      0.6652     â”‚  0.6594\n",
      "  55   â”‚      0.6758     â”‚  0.6719\n",
      "  56   â”‚      0.6713     â”‚  0.6708\n",
      "  57   â”‚      0.6590     â”‚  0.6687\n",
      "  58   â”‚      0.6691     â”‚  0.6665\n",
      "  59   â”‚      0.6730     â”‚  0.6670\n",
      "  60   â”‚      0.6724     â”‚  0.6715\n",
      "  61   â”‚      0.6747     â”‚  0.6734\n",
      "  62   â”‚      0.6323     â”‚  0.6598\n",
      "  63   â”‚      0.6607     â”‚  0.6559\n",
      "  64   â”‚      0.6652     â”‚  0.6527\n",
      "  65   â”‚      0.6769     â”‚  0.6676\n",
      "  66   â”‚      0.6825     â”‚  0.6749\n",
      "  67   â”‚      0.6763     â”‚  0.6786\n",
      "  68   â”‚      0.6574     â”‚  0.6721\n",
      "  69   â”‚      0.6370     â”‚  0.6569\n",
      "  70   â”‚      0.6797     â”‚  0.6580\n",
      "  71   â”‚      0.6702     â”‚  0.6623\n",
      "  72   â”‚      0.5748     â”‚  0.6416\n",
      "  73   â”‚      0.6657     â”‚  0.6369\n",
      "  74   â”‚      0.5781     â”‚  0.6062\n",
      "  75   â”‚      0.6685     â”‚  0.6375\n",
      "  76   â”‚      0.6568     â”‚  0.6345\n",
      "  77   â”‚      0.6730     â”‚  0.6661\n",
      "  78   â”‚      0.6607     â”‚  0.6635\n",
      "  79   â”‚      0.6607     â”‚  0.6648\n",
      "  80   â”‚      0.6696     â”‚  0.6637\n",
      "  81   â”‚      0.6496     â”‚  0.6600\n",
      "  82   â”‚      0.6680     â”‚  0.6624\n",
      "  83   â”‚      0.6680     â”‚  0.6618\n",
      "  84   â”‚      0.6674     â”‚  0.6678\n",
      "  85   â”‚      0.6535     â”‚  0.6629\n",
      "  86   â”‚      0.6719     â”‚  0.6642\n",
      "  87   â”‚      0.6691     â”‚  0.6648\n",
      "  88   â”‚      0.6691     â”‚  0.6700\n",
      "  89   â”‚      0.6295     â”‚  0.6559\n",
      "  90   â”‚      0.6702     â”‚  0.6562\n",
      "  91   â”‚      0.6747     â”‚  0.6581\n",
      "  92   â”‚      0.6735     â”‚  0.6728\n",
      "  93   â”‚      0.5675     â”‚  0.6386\n",
      "  94   â”‚      0.6735     â”‚  0.6382\n",
      "  95   â”‚      0.6702     â”‚  0.6371\n",
      "  96   â”‚      0.6462     â”‚  0.6633\n",
      "  97   â”‚      0.6669     â”‚  0.6611\n",
      "  98   â”‚      0.6434     â”‚  0.6522\n",
      "  99   â”‚      0.6702     â”‚  0.6602\n",
      "  100   â”‚      0.6713     â”‚  0.6616\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# 1) Path to your events file:\n",
    "events_path = os.path.join('/app', 'training_logs/' ,run_tag)\n",
    "event_files = glob.glob(os.path.join(events_path, \"events.out.tfevents.*\"))\n",
    "if not event_files:\n",
    "    raise FileNotFoundError(f\"No TensorBoard event files found in {events_path}\")\n",
    "event_file = sorted(event_files, key=os.path.getmtime)[-1]  # use the newest\n",
    "print(f\"Using event file: {event_file}\")\n",
    "\n",
    "# 2) Load all scalar data\n",
    "ea = EventAccumulator(event_file, size_guidance={\"scalars\": 0})\n",
    "ea.Reload()\n",
    "\n",
    "# 3) Check available scalar tags\n",
    "print(\"Available scalar tags:\", ea.Tags()[\"scalars\"])\n",
    "# => ['train_loss', 'train_concord', 'val_loss', 'val_concord']\n",
    "\n",
    "# 4) Extract (epoch, value) pairs for val_concord\n",
    "val_concord = [(e.step, e.value) for e in ea.Scalars(\"val_concord\")]\n",
    "\n",
    "# 5) Separate epochs and values\n",
    "epochs = [step for step, _ in val_concord]\n",
    "vals   = np.array([v for _, v in val_concord])\n",
    "\n",
    "# 6) Compute 3-epoch moving average\n",
    "window = 3\n",
    "# 'valid' mode produces len(vals) - window + 1 points\n",
    "smoothed = np.convolve(vals, np.ones(window)/window, mode=\"valid\")\n",
    "smoothed_epochs = epochs[window - 1 :]  # first smoothed point corresponds to epoch=window\n",
    "\n",
    "# 7) Find epoch with highest smoothed val_concord\n",
    "best_idx = np.argmax(smoothed)\n",
    "best_epoch = smoothed_epochs[best_idx]\n",
    "best_smoothed_concord = smoothed[best_idx]\n",
    "print(f\"Best smoothed val_concord = {best_smoothed_concord:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "# 8) (Optional) print raw vs smoothed for inspection\n",
    "print(\"\\n epoch â”‚ raw val_concord â”‚ smoothed\")\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "for ep, raw, sm in zip(smoothed_epochs, vals[window - 1 :], smoothed):\n",
    "    print(f\"  {ep:>2d}   â”‚      {raw:.4f}     â”‚  {sm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch4', 'epoch49', 'epoch66'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_multisurv.best_model_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch4': 0.6869419642857143,\n",
       " 'epoch49': 0.6813616071428571,\n",
       " 'epoch66': 0.6824776785714286}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_multisurv.best_concord_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch100': 0.6713169642857143}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_multisurv.current_concord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model weights to file:\n",
      "    /mnt/data/multisurv_models/mRNA_lr1e-06_breast_cancer_gpu_20250630_182536_epoch4_concord0.69.pth\n"
     ]
    }
   ],
   "source": [
    "gpu_multisurv.save_weights(saved_epoch='epoch4', prefix=run_tag, weight_dir=MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Applied fix to _BaseEvaluation._predictions_to_pycox\n",
      "âœ… Applied fix to Evaluation._predictions_to_pycox\n",
      "ðŸ’¡ Note: There's a syntax warning in evaluation.py line 187\n",
      "   Change 'self.type is not 'MultiSurv'' to 'self.type != 'MultiSurv''\n",
      "   This doesn't affect functionality but should be fixed in the source code\n",
      "\n",
      "ðŸ”§ Evaluation fixes applied! Now try running your evaluation again.\n"
     ]
    }
   ],
   "source": [
    "# Fix for evaluation.py _predictions_to_pycox method\n",
    "\n",
    "def fixed_evaluation_predictions_to_pycox(self, data, time_points=None):\n",
    "    \"\"\"Fixed evaluation version that handles the correct dimensions.\"\"\"\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    \n",
    "    # Extract predictions from data\n",
    "    # data should be a list of tuples: (predictions, times, events, patient_ids)\n",
    "    predictions_list = []\n",
    "    for item in data:\n",
    "        pred = item[0]  # The prediction tensor\n",
    "        if torch.is_tensor(pred):\n",
    "            pred = pred.detach().cpu().numpy()\n",
    "        predictions_list.append(pred)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    predictions = np.array(predictions_list)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(predictions)\n",
    "    \n",
    "    if time_points is None:\n",
    "        # Get the number of intervals from the model output\n",
    "        n_intervals = predictions.shape[1]  # Should be 18 for your model\n",
    "        \n",
    "        # Use the model's actual output intervals if available\n",
    "        if hasattr(self, 'model') and hasattr(self.model, 'output_intervals') and self.model.output_intervals is not None:\n",
    "            # Use the midpoints of the output intervals\n",
    "            intervals = self.model.output_intervals\n",
    "            if torch.is_tensor(intervals):\n",
    "                intervals = intervals.detach().cpu().numpy()\n",
    "            \n",
    "            if len(intervals) > n_intervals:\n",
    "                # Take the first n_intervals midpoints\n",
    "                midpoints = (intervals[:-1] + intervals[1:]) / 2\n",
    "                time_points = midpoints[:n_intervals]\n",
    "            else:\n",
    "                # Fallback to evenly spaced points\n",
    "                last_interval = intervals[-1] if not torch.is_tensor(intervals) else intervals[-1].item()\n",
    "                time_points = np.linspace(0.5, last_interval / 365, n_intervals)\n",
    "        else:\n",
    "            # Fallback: create time points that match the output size\n",
    "            time_points = np.arange(0.5, 0.5 + n_intervals, 1.0)\n",
    "    \n",
    "    # Ensure time_points matches the prediction dimensions\n",
    "    if len(time_points) != predictions.shape[1]:\n",
    "        print(f\"Warning: Adjusting time_points from {len(time_points)} to {predictions.shape[1]}\")\n",
    "        first_point = time_points[0] if len(time_points) > 0 else 0.5\n",
    "        last_point = time_points[-1] if len(time_points) > 0 else n_intervals + 0.5\n",
    "        time_points = np.linspace(first_point, last_point, predictions.shape[1])\n",
    "    \n",
    "    # FIXED: The DataFrame structure should be transposed\n",
    "    # We want columns to be time points, rows to be patients\n",
    "    df = df.T  # Transpose so shape becomes [num_intervals, num_patients]\n",
    "    \n",
    "    # Convert time_points to numpy if it's a tensor\n",
    "    if torch.is_tensor(time_points):\n",
    "        time_points = time_points.detach().cpu().numpy()\n",
    "    \n",
    "    # Set the index to time points\n",
    "    df.index = time_points\n",
    "    df.index.name = 'time'\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the fix to the evaluation module\n",
    "import evaluation\n",
    "\n",
    "# Check if the _BaseEvaluation class exists and patch it\n",
    "if hasattr(evaluation, '_BaseEvaluation'):\n",
    "    evaluation._BaseEvaluation._predictions_to_pycox = fixed_evaluation_predictions_to_pycox\n",
    "    print(\"âœ… Applied fix to _BaseEvaluation._predictions_to_pycox\")\n",
    "else:\n",
    "    print(\"âŒ _BaseEvaluation class not found\")\n",
    "\n",
    "# Also check for Evaluation class\n",
    "if hasattr(evaluation, 'Evaluation'):\n",
    "    evaluation.Evaluation._predictions_to_pycox = fixed_evaluation_predictions_to_pycox\n",
    "    print(\"âœ… Applied fix to Evaluation._predictions_to_pycox\")\n",
    "\n",
    "# Fix the syntax warning too\n",
    "def patch_syntax_warning():\n",
    "    \"\"\"Fix the syntax warning in evaluation.py if possible.\"\"\"\n",
    "    print(\"ðŸ’¡ Note: There's a syntax warning in evaluation.py line 187\")\n",
    "    print(\"   Change 'self.type is not 'MultiSurv'' to 'self.type != 'MultiSurv''\")\n",
    "    print(\"   This doesn't affect functionality but should be fixed in the source code\")\n",
    "\n",
    "patch_syntax_warning()\n",
    "\n",
    "print(\"\\nðŸ”§ Evaluation fixes applied! Now try running your evaluation again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Applied clinical data shape fix to Evaluation._get_patient_predictions\n"
     ]
    }
   ],
   "source": [
    "# Fix the data shape issue in get_patient_predictions\n",
    "def fixed_get_patient_predictions(self, idx):\n",
    "    \"\"\"Fixed version that handles 1D continuous tensor correctly.\"\"\"\n",
    "    patient_data = self.dataset[idx]\n",
    "    data_dict, time, event = patient_data\n",
    "    \n",
    "    # Create batch data\n",
    "    batch_data = {}\n",
    "    for key, value in data_dict.items():\n",
    "        if key == 'clinical' and isinstance(value, tuple):\n",
    "            cat, cont = value\n",
    "            # Fix: ensure continuous is 2D [1, n_features]\n",
    "            if cont.dim() == 1:\n",
    "                cont = cont.unsqueeze(0)\n",
    "            batch_data[key] = (cat.unsqueeze(0).to(self.device), \n",
    "                              cont.unsqueeze(0).to(self.device))\n",
    "        else:\n",
    "            batch_data[key] = value.unsqueeze(0).to(self.device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        _, risk = self.model.model(batch_data)\n",
    "    \n",
    "    return risk.squeeze(0).cpu()\n",
    "\n",
    "# Apply the fix\n",
    "import evaluation\n",
    "evaluation.Evaluation._get_patient_predictions = fixed_get_patient_predictions\n",
    "print(\"âœ… Applied clinical data shape fix to Evaluation._get_patient_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Applied final fix for nested dictionary structure\n"
     ]
    }
   ],
   "source": [
    "# Final fix for _predictions_to_pycox\n",
    "def fixed_evaluation_predictions_to_pycox_final(self, data, time_points=None):\n",
    "    \"\"\"Fixed version that handles the nested dictionary structure.\"\"\"\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    \n",
    "    # Extract predictions from the nested dictionary structure\n",
    "    predictions_list = []\n",
    "    times_list = []\n",
    "    events_list = []\n",
    "    patient_ids = []\n",
    "    \n",
    "    for patient_id, patient_data in data.items():\n",
    "        if isinstance(patient_data, dict):\n",
    "            # Extract probabilities from the dictionary\n",
    "            prob = patient_data['probabilities']\n",
    "            if torch.is_tensor(prob):\n",
    "                prob = prob.detach().cpu().numpy()\n",
    "            predictions_list.append(prob)\n",
    "            \n",
    "            # Also collect times and events for verification\n",
    "            times_list.append(patient_data['time'])\n",
    "            events_list.append(patient_data['event'])\n",
    "            patient_ids.append(patient_id)\n",
    "        else:\n",
    "            # Fallback for other formats\n",
    "            if torch.is_tensor(patient_data):\n",
    "                predictions_list.append(patient_data.detach().cpu().numpy())\n",
    "            else:\n",
    "                predictions_list.append(patient_data[0])\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    predictions = np.array(predictions_list)\n",
    "    print(f\"Extracted predictions shape: {predictions.shape}\")\n",
    "    \n",
    "    # Create DataFrame with patients as rows, time intervals as columns\n",
    "    df = pd.DataFrame(predictions)\n",
    "    \n",
    "    if time_points is None:\n",
    "        n_intervals = predictions.shape[1]\n",
    "        \n",
    "        # Use the model's actual output intervals if available\n",
    "        if hasattr(self, 'model') and hasattr(self.model, 'output_intervals') and self.model.output_intervals is not None:\n",
    "            intervals = self.model.output_intervals\n",
    "            if torch.is_tensor(intervals):\n",
    "                intervals = intervals.detach().cpu().numpy()\n",
    "            \n",
    "            # Create midpoints for time intervals\n",
    "            if len(intervals) == n_intervals + 1:\n",
    "                # intervals are boundaries, we need midpoints\n",
    "                time_points = (intervals[:-1] + intervals[1:]) / 2\n",
    "            else:\n",
    "                # Fallback\n",
    "                time_points = np.linspace(0.5, 20, n_intervals)\n",
    "        else:\n",
    "            # Fallback: create evenly spaced time points\n",
    "            time_points = np.arange(0.5, 0.5 + n_intervals, 1.0)\n",
    "    \n",
    "    # Ensure time_points matches prediction dimensions\n",
    "    if len(time_points) != predictions.shape[1]:\n",
    "        time_points = np.linspace(0.5, 20, predictions.shape[1])\n",
    "    \n",
    "    # Transpose so time is the index (required by pycox)\n",
    "    df = df.T\n",
    "    df.index = time_points\n",
    "    df.index.name = 'time'\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the final fix\n",
    "import evaluation\n",
    "evaluation.Evaluation._predictions_to_pycox = fixed_evaluation_predictions_to_pycox_final\n",
    "print(\"âœ… Applied final fix for nested dictionary structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect patient predictions: 165/165\n",
      "\n",
      "Extracted predictions shape: (165, 17)\n",
      "C-index   0.684\n",
      "Ctd       0.685\n",
      "IBS       0.218\n",
      "INBLL     0.759\n"
     ]
    }
   ],
   "source": [
    "performance = utils.Evaluation(\n",
    "    model=gpu_multisurv, dataset=dataloaders['val'].dataset,\n",
    "    device=device)\n",
    "performance.compute_metrics()\n",
    "performance.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect patient predictions: 219/219\n",
      "\n",
      "Extracted predictions shape: (219, 17)\n",
      "C-index   0.549\n",
      "Ctd       0.549\n",
      "IBS       0.299\n",
      "INBLL     1.037\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on TEST set (not validation)\n",
    "performance = utils.Evaluation(\n",
    "    model=gpu_multisurv, \n",
    "    dataset=dataloaders['test'].dataset,  # Note: 'test' not 'val'\n",
    "    device=device\n",
    ")\n",
    "performance.compute_metrics()\n",
    "performance.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watermark <a class='tocSkip'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark --iversions\n",
    "%watermark -v\n",
    "print()\n",
    "%watermark -u -n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of the page](#Top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "197px",
    "width": "372px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "236px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
