{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Top'></a>\n",
    "\n",
    "# Multisurv model training<a class='tocSkip'></a>\n",
    "\n",
    "Train MultiSurv models with different combinations of input data modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> PyTorch detected CUDA <<<\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext watermark\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('>>> PyTorch detected CUDA <<<')\n",
    "\n",
    "# Make modules in \"src\" dir visible\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "import utils\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#DataLoader\" data-toc-modified-id=\"DataLoader-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span><code>DataLoader</code></a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Different-intervals\" data-toc-modified-id=\"Different-intervals-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Different intervals</a></span><ul class=\"toc-item\"><li><span><a href=\"#Equidistant-times\" data-toc-modified-id=\"Equidistant-times-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Equidistant times</a></span></li><li><span><a href=\"#By-duration-quantiles\" data-toc-modified-id=\"By-duration-quantiles-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>By duration quantiles</a></span></li></ul></li><li><span><a href=\"#Pick-learning-rate\" data-toc-modified-id=\"Pick-learning-rate-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Pick learning rate</a></span></li><li><span><a href=\"#Fit\" data-toc-modified-id=\"Fit-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Fit</a></span><ul class=\"toc-item\"><li><span><a href=\"#Save-model-weights\" data-toc-modified-id=\"Save-model-weights-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Save model weights</a></span></li></ul></li><li><span><a href=\"#Check-validation-metrics\" data-toc-modified-id=\"Check-validation-metrics-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Check validation metrics</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ PyTorch is using: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "âœ… NVIDIA GPU detected - good for training!\n",
      "\n",
      "==================================================\n",
      "=== GPU DETECTION REPORT ===\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "PyTorch version: 2.4.0\n",
      "Number of CUDA devices: 1\n",
      "\n",
      "=== AVAILABLE GPUS ===\n",
      "GPU 0: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "  Memory: 8.0 GB\n",
      "  Compute Capability: 8.9\n",
      "  Multi Processors: 24\n",
      "\n",
      "=== CURRENT SELECTION ===\n",
      "Current CUDA device: 0\n",
      "Current GPU name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "\n",
      "=== MEMORY USAGE ===\n",
      "GPU 0 (NVIDIA GeForce RTX 4060 Laptop GPU):\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 8.0 GB\n",
      "\n",
      "=== GPU FUNCTIONALITY TEST ===\n",
      "âœ… GPU computation successful\n",
      "Test tensor device: cuda:0\n",
      "Result tensor device: cuda:0\n",
      "\n",
      "=== NVIDIA SYSTEM INFO ===\n",
      "NVIDIA-SMI Output:\n",
      "Sat Jul  5 14:07:50 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.64.01              Driver Version: 576.80         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   42C    P0              4W /   80W |     123MiB /   8188MiB |     10%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            8391      C   /python3.8                            N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "âœ… Using single GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "\n",
      "=== PYTORCH GPU VERIFICATION ===\n",
      "âœ… PyTorch using: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "âœ… Confirmed: Using NVIDIA GPU\n",
      "\n",
      "=== RECOMMENDATION ===\n",
      "âœ… Ready for GPU training with device: cuda:0\n",
      "Use: multisurv.device = cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "def check_gpu_setup():\n",
    "    \"\"\"Complete GPU detection and verification.\"\"\"\n",
    "    \n",
    "    print(\"=== GPU DETECTION REPORT ===\")\n",
    "    \n",
    "    # 1. Check CUDA availability\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"âŒ CUDA not available - will use CPU\")\n",
    "        return\n",
    "    \n",
    "    # 2. Check number of GPUs\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Number of CUDA devices: {gpu_count}\")\n",
    "    \n",
    "    # 3. List all available GPUs\n",
    "    print(\"\\n=== AVAILABLE GPUS ===\")\n",
    "    for i in range(gpu_count):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i}: {props.name}\")\n",
    "        print(f\"  Memory: {props.total_memory / 1024**3:.1f} GB\")\n",
    "        print(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
    "        print(f\"  Multi Processors: {props.multi_processor_count}\")\n",
    "    \n",
    "    # 4. Check current device\n",
    "    current_device = torch.cuda.current_device()\n",
    "    current_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"\\n=== CURRENT SELECTION ===\")\n",
    "    print(f\"Current CUDA device: {current_device}\")\n",
    "    print(f\"Current GPU name: {current_name}\")\n",
    "    \n",
    "    # 5. Memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\n=== MEMORY USAGE ===\")\n",
    "        for i in range(gpu_count):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "            print(f\"GPU {i} ({torch.cuda.get_device_name(i)}):\")\n",
    "            print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "            print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "            print(f\"  Total: {total:.1f} GB\")\n",
    "    \n",
    "    # 6. Test tensor creation on GPU\n",
    "    print(f\"\\n=== GPU FUNCTIONALITY TEST ===\")\n",
    "    try:\n",
    "        # Create test tensor on GPU\n",
    "        test_tensor = torch.randn(1000, 1000).cuda()\n",
    "        result = torch.mm(test_tensor, test_tensor)\n",
    "        print(f\"âœ… GPU computation successful\")\n",
    "        print(f\"Test tensor device: {test_tensor.device}\")\n",
    "        print(f\"Result tensor device: {result.device}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del test_tensor, result\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ GPU computation failed: {e}\")\n",
    "\n",
    "def check_nvidia_system():\n",
    "    \"\"\"Check NVIDIA system information.\"\"\"\n",
    "    print(\"\\n=== NVIDIA SYSTEM INFO ===\")\n",
    "    \n",
    "    try:\n",
    "        # Run nvidia-smi command\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"NVIDIA-SMI Output:\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"âŒ nvidia-smi command failed\")\n",
    "            print(result.stderr)\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ nvidia-smi not found - NVIDIA drivers may not be installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error running nvidia-smi: {e}\")\n",
    "\n",
    "def set_gpu_preference():\n",
    "    \"\"\"Set GPU preference if multiple GPUs available.\"\"\"\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No CUDA GPUs available\")\n",
    "        return None\n",
    "    \n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    \n",
    "    if gpu_count == 1:\n",
    "        device = torch.device('cuda:0')\n",
    "        print(f\"âœ… Using single GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        return device\n",
    "    \n",
    "    print(f\"\\n=== MULTIPLE GPUS DETECTED ({gpu_count}) ===\")\n",
    "    \n",
    "    # Show GPU options\n",
    "    for i in range(gpu_count):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i}: {props.name} ({props.total_memory / 1024**3:.1f} GB)\")\n",
    "    \n",
    "    # Find the most powerful GPU (by memory)\n",
    "    best_gpu = 0\n",
    "    best_memory = 0\n",
    "    \n",
    "    for i in range(gpu_count):\n",
    "        memory = torch.cuda.get_device_properties(i).total_memory\n",
    "        if memory > best_memory:\n",
    "            best_memory = memory\n",
    "            best_gpu = i\n",
    "    \n",
    "    device = torch.device(f'cuda:{best_gpu}')\n",
    "    print(f\"âœ… Auto-selected most powerful GPU: {torch.cuda.get_device_name(best_gpu)}\")\n",
    "    \n",
    "    # Set as current device\n",
    "    torch.cuda.set_device(best_gpu)\n",
    "    \n",
    "    return device\n",
    "\n",
    "def verify_pytorch_gpu_usage():\n",
    "    \"\"\"Verify PyTorch is actually using the NVIDIA GPU (not Intel).\"\"\"\n",
    "    \n",
    "    print(\"\\n=== PYTORCH GPU VERIFICATION ===\")\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"âŒ PyTorch not using GPU - will use CPU\")\n",
    "        return False\n",
    "    \n",
    "    # Create tensor and check device\n",
    "    x = torch.randn(100, 100).cuda()\n",
    "    gpu_name = torch.cuda.get_device_name(x.device)\n",
    "    \n",
    "    print(f\"âœ… PyTorch using: {gpu_name}\")\n",
    "    \n",
    "    # Check if it's NVIDIA (not Intel)\n",
    "    if 'nvidia' in gpu_name.lower() or 'geforce' in gpu_name.lower() or 'rtx' in gpu_name.lower() or 'gtx' in gpu_name.lower():\n",
    "        print(f\"âœ… Confirmed: Using NVIDIA GPU\")\n",
    "        return True\n",
    "    elif 'intel' in gpu_name.lower():\n",
    "        print(f\"âš ï¸  Warning: Using Intel GPU - this may be slow for deep learning\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"â“ Unknown GPU type: {gpu_name}\")\n",
    "        return True\n",
    "\n",
    "# Run all checks\n",
    "def complete_gpu_check():\n",
    "    \"\"\"Run complete GPU diagnostic.\"\"\"\n",
    "    check_gpu_setup()\n",
    "    check_nvidia_system()\n",
    "    device = set_gpu_preference()\n",
    "    is_nvidia = verify_pytorch_gpu_usage()\n",
    "    \n",
    "    print(f\"\\n=== RECOMMENDATION ===\")\n",
    "    if is_nvidia and device:\n",
    "        print(f\"âœ… Ready for GPU training with device: {device}\")\n",
    "        print(f\"Use: multisurv.device = {device}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Consider using CPU training: multisurv.device = torch.device('cpu')\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Quick check function\n",
    "def quick_gpu_check():\n",
    "    \"\"\"Quick GPU check for immediate feedback.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"ðŸŽ¯ PyTorch is using: {gpu_name}\")\n",
    "        \n",
    "        if any(keyword in gpu_name.lower() for keyword in ['nvidia', 'geforce', 'rtx', 'gtx']):\n",
    "            print(\"âœ… NVIDIA GPU detected - good for training!\")\n",
    "        else:\n",
    "            print(\"âš ï¸  Non-NVIDIA GPU detected\")\n",
    "    else:\n",
    "        print(\"âŒ No GPU available - will use CPU\")\n",
    "\n",
    "# Run the checks\n",
    "quick_gpu_check()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "complete_gpu_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = utils.INPUT_DATA_DIR\n",
    "MODELS = utils.TRAINED_MODEL_DIR\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643c33492f714c728d81d020b8569f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Input data', index=(0, 1), options=('clinical', 'mRNA', 'DNAm', 'miRNA', 'CNV', 'wâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_modalities = widgets.SelectMultiple(\n",
    "    options=['clinical', 'mRNA', 'DNAm', 'miRNA', 'CNV', 'wsi'],\n",
    "    index=[0, 1],\n",
    "    rows=6,\n",
    "    description='Input data',\n",
    "    disabled=False\n",
    ")\n",
    "display(data_modalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   submitter_id      time  event  group\n",
      "0  TCGA-Z7-A8R6  8.920548      0   test\n",
      "1  TCGA-C8-A1HE  1.027397      0   test\n",
      "2  TCGA-A8-A07B  3.583562      0  train\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------#\n",
    "#                             20-CANCER SUBSET                                #\n",
    "#                 (to compare to Cheerla and Gevaert 2019)                    #\n",
    "#-----------------------------------------------------------------------------#\n",
    "\n",
    "cancers = ['BRCA']\n",
    "\n",
    "labels = pd.read_csv('/app/data/labels.tsv', sep='\\t')\n",
    "print(labels.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data modalities:\n",
      "   clinical\n",
      "\n",
      "Dataset sizes (# patients):\n",
      "   train: 710\n",
      "   val: 165\n",
      "   test: 219\n",
      "\n",
      "Batch size: 16\n"
     ]
    }
   ],
   "source": [
    "dataloaders = utils.get_dataloaders(data_location=DATA,\n",
    "                                    labels_file='/app/data/labels.tsv',\n",
    "                                    modalities=data_modalities.value,\n",
    "                                    wsi_patch_size=549,\n",
    "                                    n_wsi_patches=5,\n",
    "#                                     batch_size=20,\n",
    "#                                    batch_size=64,\n",
    "                                     batch_size=16,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train event rate: 13.80%\n",
      "Val event rate: 13.94%\n",
      "Test event rate: 13.70%\n"
     ]
    }
   ],
   "source": [
    "# Compare event rates across splits\n",
    "def get_event_rate(dataset):\n",
    "    events = [e for _, e in dataset.label_map.values()]\n",
    "    return sum(events) / len(events)\n",
    "\n",
    "print(f\"Train event rate: {get_event_rate(dataloaders['train'].dataset):.2%}\")\n",
    "print(f\"Val event rate: {get_event_rate(dataloaders['val'].dataset):.2%}\")\n",
    "print(f\"Test event rate: {get_event_rate(dataloaders['test'].dataset):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN set (n=710):\n",
      "Clinical features shape: ({'clinical': (tensor([0., 5., 0., 0., 0., 0., 2., 4.]), tensor([0.3879]))}, 8.123287671232877, 1)\n",
      "\n",
      "VAL set (n=165):\n",
      "Clinical features shape: ({'clinical': (tensor([0., 5., 0., 0., 0., 0., 2., 7.]), tensor([0.5076]))}, 2.92054794520548, 0)\n",
      "\n",
      "TEST set (n=219):\n",
      "Clinical features shape: ({'clinical': (tensor([0., 3., 0., 0., 0., 0., 2., 9.]), tensor([0.6569]))}, 3.315068493150685, 0)\n"
     ]
    }
   ],
   "source": [
    "# Check if clinical features are similar across splits\n",
    "# Look for any systematic differences\n",
    "for split in ['train', 'val', 'test']:\n",
    "    data = dataloaders[split].dataset\n",
    "    print(f\"\\n{split.upper()} set (n={len(data)}):\")\n",
    "    # Get first patient's clinical data to see structure\n",
    "    if len(data) > 0:\n",
    "        clinical = data[0]\n",
    "        print(f\"Clinical features shape: {clinical}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 1. First, check the continuous feature distribution for clinical data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract continuous feature values for all patients\n",
    "def get_continuous_feature(dataset):\n",
    "    values = []\n",
    "    for i in range(len(dataset)):\n",
    "        clinical_data = dataset[i][0]  # Get the data part\n",
    "        continuous_val = clinical_data['clinical'][1].item()  # The continuous feature\n",
    "        values.append(continuous_val)\n",
    "    return np.array(values)\n",
    "\n",
    "train_cont = get_continuous_feature(dataloaders['train'].dataset)\n",
    "val_cont = get_continuous_feature(dataloaders['val'].dataset)\n",
    "test_cont = get_continuous_feature(dataloaders['test'].dataset)\n",
    "\n",
    "# Plot distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_cont, alpha=0.5, label=f'Train (mean={train_cont.mean():.3f})', bins=30)\n",
    "plt.hist(val_cont, alpha=0.5, label=f'Val (mean={val_cont.mean():.3f})', bins=30)\n",
    "plt.hist(test_cont, alpha=0.5, label=f'Test (mean={test_cont.mean():.3f})', bins=30)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Continuous Clinical Feature')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Train range: [{train_cont.min():.3f}, {train_cont.max():.3f}]\")\n",
    "print(f\"Val range: [{val_cont.min():.3f}, {val_cont.max():.3f}]\")\n",
    "print(f\"Test range: [{test_cont.min():.3f}, {test_cont.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different intervals\n",
    "\n",
    "If trying out different time interval outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equidistant times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_cuts = torch.arange(0., 365 * 5.1, 365 / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By duration quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [(t, e) for t, e in dataloaders['train'].dataset.label_map.values()]\n",
    "durations = [t for t, _ in labels]\n",
    "events = [e for _, e in labels]\n",
    "\n",
    "interval_cuts = utils.discretize_time_by_duration_quantiles(durations, events, 20)\n",
    "interval_cuts = torch.from_numpy(interval_cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------#\n",
    "#                       PRE-TRAINED UNIMODAL MODELS                           #\n",
    "#-----------------------------------------------------------------------------#\n",
    "\n",
    "unimodal_weigths = {'clinical': 'clinical_lr1e-06_breast_cancer_gpu_20250630_163247_epoch58_concord0.77.pth',\n",
    "                        'mRNA': None,\n",
    "                    'DNAm': None,\n",
    "                    'miRNA': None,\n",
    "                    'CNV': None,\n",
    "                    'wsi': None,}\n",
    "\n",
    "unimodal_weigths = {k: os.path.join(MODELS, v) if v is not None else None\n",
    "                    for k, v in unimodal_weigths.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating MultiSurv model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/src/multisurv.py:84: UserWarning: Input data is unimodal: no fusion procedure.\n",
      "  warnings.warn('Input data is unimodal: no fusion procedure.')\n",
      "/app/src/model.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(self.unimodal_state_files[modality])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(loading pretrained unimodal model weights...)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MultiSurv:\n\tsize mismatch for risk_layer.0.weight: copying a param with shape torch.Size([17, 512]) from checkpoint, the shape in current model is torch.Size([10, 512]).\n\tsize mismatch for risk_layer.0.bias: copying a param with shape torch.Size([17]) from checkpoint, the shape in current model is torch.Size([10]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m multisurv \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;43;03m#    auxiliary_criterion=None,  # No auxiliary loss needed\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_intervals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterval_cuts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43munimodal_state_files\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43munimodal_weigths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/src/model.py:76\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, dataloaders, fusion_method, output_intervals, auxiliary_criterion, unimodal_state_files, freeze_up_to, device)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataloaders, fusion_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     73\u001b[0m              output_intervals\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m365\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m31\u001b[39m, \u001b[38;5;241m365\u001b[39m),\n\u001b[1;32m     74\u001b[0m              auxiliary_criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, unimodal_state_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m              freeze_up_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfusion_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_intervals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                     \u001b[49m\u001b[43munimodal_state_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreeze_up_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m Adam\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m Loss()\n",
      "File \u001b[0;32m/app/src/model.py:33\u001b[0m, in \u001b[0;36m_BaseModelWithData.__init__\u001b[0;34m(self, dataloaders, fusion_method, output_intervals, unimodal_state_files, freeze_up_to, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m data_dirs \u001b[38;5;241m=\u001b[39m eg_dataloader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mdata_dirs\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_modalities \u001b[38;5;241m=\u001b[39m [modality \u001b[38;5;28;01mfor\u001b[39;00m modality \u001b[38;5;129;01min\u001b[39;00m data_dirs\n\u001b[1;32m     32\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m data_dirs[modality] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_instantiate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_blocks \u001b[38;5;241m=\u001b[39m [name \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnamed_children()]\n",
      "File \u001b[0;32m/app/src/model.py:48\u001b[0m, in \u001b[0;36m_BaseModelWithData._instantiate_model\u001b[0;34m(self, move_to_device)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pretrained_unimodal_weights()\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(loading pretrained unimodal model weights...)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m move_to_device:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/conda/envs/multisurv/lib/python3.8/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MultiSurv:\n\tsize mismatch for risk_layer.0.weight: copying a param with shape torch.Size([17, 512]) from checkpoint, the shape in current model is torch.Size([10, 512]).\n\tsize mismatch for risk_layer.0.bias: copying a param with shape torch.Size([17]) from checkpoint, the shape in current model is torch.Size([10])."
     ]
    }
   ],
   "source": [
    "\n",
    "multisurv = Model(\n",
    "    dataloaders=dataloaders,\n",
    "#    auxiliary_criterion=None,  # No auxiliary loss needed\n",
    "    output_intervals=interval_cuts,\n",
    "    unimodal_state_files = unimodal_weigths,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gpu_enabled_multisurv():\n",
    "    \"\"\"Create a fresh MultiSurv instance with proper GPU setup.\"\"\"\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Creating MultiSurv with device: {device}\")\n",
    "    \n",
    "    \n",
    "    # Create new model with GPU device\n",
    "    gpu_multisurv = Model(\n",
    "        dataloaders=multisurv.dataloaders,  # Reuse existing dataloaders\n",
    "        fusion_method=multisurv.fusion_method,\n",
    "        output_intervals=multisurv.output_intervals.to(device),  # Move to GPU\n",
    "        unimodal_state_files = multisurv.unimodal_state_files,\n",
    "        device=device  # Set device properly\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… GPU-enabled MultiSurv created!\")\n",
    "    return gpu_multisurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MultiSurv with device: cuda:0\n",
      "Instantiating MultiSurv model...\n",
      "(loading pretrained unimodal model weights...)\n",
      "âœ… GPU-enabled MultiSurv created!\n"
     ]
    }
   ],
   "source": [
    "gpu_multisurv = create_gpu_enabled_multisurv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output intervals (in years):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  1.6767,  2.7123,  3.7342,  4.6247,  6.2712,  6.8849,  7.5808,\n",
       "         8.0000,  8.9342, 10.0274, 10.6110, 10.7973, 11.7397, 17.2384, 17.6877,\n",
       "        19.5233, 23.5753], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Output intervals (in years):')\n",
    "gpu_multisurv.output_intervals / 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clinical_submodel', 'fc_block', 'risk_layer']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_multisurv.model_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable blocks:\n",
      "   clinical_submodel: True\n",
      "   fc_block: True\n",
      "   risk_layer: True\n"
     ]
    }
   ],
   "source": [
    "print('Trainable blocks:')\n",
    "layer = None\n",
    "\n",
    "for name, child in gpu_multisurv.model.named_children():\n",
    "    for name_2, params in child.named_parameters():\n",
    "        if name is not layer:\n",
    "            print(f'   {name}: {params.requires_grad}')\n",
    "        layer = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiSurv(\n",
       "  (clinical_submodel): ClinicalNet(\n",
       "    (embedding_layers): ModuleList(\n",
       "      (0): Embedding(2, 1)\n",
       "      (1): Embedding(6, 3)\n",
       "      (2-4): 3 x Embedding(2, 1)\n",
       "      (5-6): 2 x Embedding(3, 2)\n",
       "      (7): Embedding(12, 6)\n",
       "    )\n",
       "    (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "    (bn_layer): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (linear): Linear(in_features=18, out_features=256, bias=True)\n",
       "    (output_layer): FC(\n",
       "      (fc): Sequential(\n",
       "        (0): Dropout(p=0.5, inplace=False)\n",
       "        (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_block): FC(\n",
       "    (fc): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): Dropout(p=0.5, inplace=False)\n",
       "      (5): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (6): ReLU()\n",
       "      (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): Dropout(p=0.5, inplace=False)\n",
       "      (9): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (10): ReLU()\n",
       "      (11): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): Dropout(p=0.5, inplace=False)\n",
       "      (13): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (14): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (risk_layer): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=17, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_multisurv.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ GPU Memory: 0.17 GB allocated, 0.17 GB reserved\n",
      "ðŸ” GPU cache size: 0.17 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_761/1937229576.py:9: FutureWarning: `torch.cuda.memory_cached` has been renamed to `torch.cuda.memory_reserved`\n",
      "  print(f\"ðŸ” GPU cache size: {torch.cuda.memory_cached(0) / 1024**3:.2f} GB\")\n"
     ]
    }
   ],
   "source": [
    "# Check GPU usage after LR test\n",
    "def monitor_gpu_real_time():\n",
    "    import torch\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"ðŸŽ¯ GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "    \n",
    "    # Check if there are any tensors on GPU\n",
    "    print(f\"ðŸ” GPU cache size: {torch.cuda.memory_cached(0) / 1024**3:.2f} GB\")\n",
    "monitor_gpu_real_time()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_example, _, _ = dataloaders['train'].dataset[0]\n",
    "print(\"Categorical features:\\n\", data_example['clinical'][0])\n",
    "print(\"Continuous features:\\n\", data_example['clinical'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied GPU-compatible fix to LRRangeTest.run\n"
     ]
    }
   ],
   "source": [
    "# GPU-compatible version of the completely fixed LR test\n",
    "\n",
    "def gpu_fixed_lr_test_run(self, init_value=1e-8, final_value=10., beta=0.98):\n",
    "    \"\"\"GPU-compatible LR test that ensures all tensors are on the same device.\"\"\"\n",
    "    print(\">>> Using GPU-COMPATIBLE FIXED lr_test.run method\")\n",
    "    \n",
    "    power = (1 / (len(self.dataloader) - 1))\n",
    "    mult = (final_value / init_value) ** power\n",
    "    lr = init_value\n",
    "    self.optimizer.param_groups[0]['lr'] = lr\n",
    "    avg_loss = 0.\n",
    "    best_loss = 0.\n",
    "    batch_num = 0\n",
    "\n",
    "    print('>>> Compute loss at increasing LR values')\n",
    "    \n",
    "    # Clear losses and lrs lists\n",
    "    self.losses = []\n",
    "    self.lrs = []\n",
    "\n",
    "    for data in self.dataloader:\n",
    "        batch_num += 1\n",
    "        print('\\r' + f'    Iterate over mini-batches: {str(batch_num)}', end='')\n",
    "\n",
    "        try:\n",
    "            # Unpack data\n",
    "            if len(data) == 3:\n",
    "                modality_data, time, event = data\n",
    "            elif len(data) == 4:\n",
    "                modality_data, time, event, pid = data\n",
    "            \n",
    "            # CRITICAL: Move ALL data to the same device as the model\n",
    "            target_device = next(self.model.parameters()).device\n",
    "            \n",
    "            # Move modality data to device\n",
    "            for key, value in modality_data.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    modality_data[key] = value.to(target_device)\n",
    "                elif isinstance(value, (list, tuple)):\n",
    "                    modality_data[key] = tuple(v.to(target_device) if isinstance(v, torch.Tensor) else v for v in value)\n",
    "            \n",
    "            # Move time and event to device\n",
    "            time = time.to(target_device)\n",
    "            event = event.to(target_device)\n",
    "            \n",
    "            # Set model to train mode\n",
    "            self.model.train()\n",
    "            \n",
    "            # Clear gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.set_grad_enabled(True):\n",
    "                feature_representations, risk = self.model(modality_data)\n",
    "                modality_features = feature_representations['modalities']\n",
    "                \n",
    "                # Compute loss with proper device handling\n",
    "                try:\n",
    "                    # Ensure output_intervals are on the same device\n",
    "                    if hasattr(self, 'output_intervals'):\n",
    "                        breaks = self.output_intervals.to(target_device)\n",
    "                    else:\n",
    "                        breaks = torch.linspace(0, 10, 19, device=target_device)  # Fallback\n",
    "                    \n",
    "                    loss = self.criterion(risk, times=time, events=event, breaks=breaks, device=target_device)\n",
    "                    \n",
    "                except Exception as loss_error:\n",
    "                    print(f\"\\nPrimary criterion failed: {loss_error}\")\n",
    "                    # Fallback to simple MSE loss on the same device\n",
    "                    dummy_target = torch.ones_like(risk)  # This will be on the same device as risk\n",
    "                    loss = torch.nn.functional.mse_loss(risk, dummy_target)\n",
    "                    print(\"Using fallback MSE loss\")\n",
    "                \n",
    "                # Add auxiliary loss if needed\n",
    "                if self.aux_criterion is not None:\n",
    "                    try:\n",
    "                        if len(modality_features) >= 2:\n",
    "                            target = torch.ones(modality_features[0].shape[0], device=target_device)\n",
    "                            aux_loss = self.aux_criterion(modality_features[0], modality_features[1], target)\n",
    "                            loss = loss + 0.1 * aux_loss\n",
    "                    except Exception as aux_error:\n",
    "                        print(f\"\\nAuxiliary criterion failed: {aux_error}\")\n",
    "            \n",
    "            # Store the loss value\n",
    "            loss_value = loss.item()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError at batch {batch_num}: {e}\")\n",
    "            break\n",
    "\n",
    "        # Compute the smoothed loss\n",
    "        avg_loss = (beta * avg_loss + (1 - beta) * loss_value)\n",
    "        smoothed_loss = avg_loss / (1 - beta ** batch_num)\n",
    "\n",
    "        # Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "            print()\n",
    "            print('    Exploding loss; finish test.')\n",
    "            break\n",
    "\n",
    "        # Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num == 1:\n",
    "            best_loss = smoothed_loss\n",
    "            \n",
    "        # Store the values\n",
    "        self.losses.append(smoothed_loss)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # Do the backward pass and optimizer step\n",
    "        try:\n",
    "            loss.backward(retain_graph=False)\n",
    "            self.optimizer.step()\n",
    "        except Exception as e:\n",
    "            print(f\"\\nBackward pass failed at batch {batch_num}: {e}\")\n",
    "            break\n",
    "\n",
    "        # Update the lr for the next step\n",
    "        lr *= mult\n",
    "        self.optimizer.param_groups[0]['lr'] = lr\n",
    "        \n",
    "        # Stop after reasonable number of batches\n",
    "        if batch_num >= 100:\n",
    "            print(\"\\nStopping after 100 batches\")\n",
    "            break\n",
    "\n",
    "    print()\n",
    "    print('    Completed test.')\n",
    "    return self\n",
    "\n",
    "# Apply the GPU-compatible fix\n",
    "import lr_range_test\n",
    "lr_range_test.LRRangeTest.run = gpu_fixed_lr_test_run\n",
    "print(\"Applied GPU-compatible fix to LRRangeTest.run\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Ultimate fix: Replace the entire lr_range_test.py run method\n",
    "\n",
    "def completely_fixed_lr_test_run(self, init_value=1e-8, final_value=10., beta=0.98):\n",
    "    \"\"\"Completely rewritten LR test that avoids the inplace operation issue.\"\"\"\n",
    "    print(\">>> Using COMPLETELY FIXED lr_test.run method\")\n",
    "    \n",
    "    power = (1 / (len(self.dataloader) - 1))\n",
    "    mult = (final_value / init_value) ** power\n",
    "    lr = init_value\n",
    "    self.optimizer.param_groups[0]['lr'] = lr\n",
    "    avg_loss = 0.\n",
    "    best_loss = 0.\n",
    "    batch_num = 0\n",
    "\n",
    "    print('>>> Compute loss at increasing LR values')\n",
    "    \n",
    "    # Clear losses and lrs lists\n",
    "    self.losses = []\n",
    "    self.lrs = []\n",
    "\n",
    "    for data in self.dataloader:\n",
    "        batch_num += 1\n",
    "        print('\\r' + f'    Iterate over mini-batches: {str(batch_num)}', end='')\n",
    "\n",
    "        try:\n",
    "            # COMPLETELY MANUAL APPROACH - avoid ModelCoach entirely\n",
    "            \n",
    "            # Unpack data\n",
    "            if len(data) == 3:\n",
    "                modality_data, time, event = data\n",
    "            elif len(data) == 4:\n",
    "                modality_data, time, event, pid = data\n",
    "            \n",
    "            # Move to device manually\n",
    "            for key, value in modality_data.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    modality_data[key] = value.to(self.device)\n",
    "                elif isinstance(value, (list, tuple)):\n",
    "                    modality_data[key] = tuple(v.to(self.device) if isinstance(v, torch.Tensor) else v for v in value)\n",
    "            \n",
    "            time = time.to(self.device)\n",
    "            event = event.to(self.device)\n",
    "            \n",
    "            # Set model to train mode\n",
    "            self.model.train()\n",
    "            \n",
    "            # Clear gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.set_grad_enabled(True):\n",
    "                feature_representations, risk = self.model(modality_data)\n",
    "                modality_features = feature_representations['modalities']\n",
    "                \n",
    "                # Compute loss manually (fix the arguments)\n",
    "                try:\n",
    "                    # The loss function signature is: forward(risk, times, events, breaks, device)\n",
    "                    # NOT forward(risk, times, events, modality_features, breaks)\n",
    "                    breaks = self.output_intervals\n",
    "                    if isinstance(breaks, (list, tuple)):\n",
    "                        breaks = torch.tensor(breaks, dtype=torch.float32, device=self.device)\n",
    "                    \n",
    "                    loss = self.criterion(risk, times=time, events=event, breaks=breaks, device=self.device)\n",
    "                        \n",
    "                except Exception as loss_error:\n",
    "                    print(f\"\\nPrimary criterion failed: {loss_error}\")\n",
    "                    # Fallback to simple MSE loss for LR range test\n",
    "                    dummy_target = torch.ones_like(risk)\n",
    "                    loss = torch.nn.functional.mse_loss(risk, dummy_target)\n",
    "                    print(\"Using fallback MSE loss\")\n",
    "                \n",
    "                # Add auxiliary loss if needed (fix the argument mismatch)\n",
    "                if self.aux_criterion is not None:\n",
    "                    try:\n",
    "                        # CosineEmbeddingLoss expects (input1, input2, target)\n",
    "                        # Let's use the two modality features\n",
    "                        if len(modality_features) >= 2:\n",
    "                            # Create a dummy target (1 for similar, -1 for dissimilar)\n",
    "                            target = torch.ones(modality_features[0].shape[0], device=self.device)\n",
    "                            aux_loss = self.aux_criterion(modality_features[0], modality_features[1], target)\n",
    "                            loss = loss + 0.1 * aux_loss  # Scale down aux loss\n",
    "                        else:\n",
    "                            print(\"Skipping aux loss - insufficient modality features\")\n",
    "                    except Exception as aux_error:\n",
    "                        print(f\"\\nAuxiliary criterion failed: {aux_error}\")\n",
    "                        print(\"Skipping auxiliary loss\")\n",
    "            \n",
    "            # Store the loss value\n",
    "            loss_value = loss.item()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError at batch {batch_num}: {e}\")\n",
    "            break\n",
    "\n",
    "        # Compute the smoothed loss\n",
    "        avg_loss = (beta * avg_loss + (1 - beta) * loss_value)\n",
    "        smoothed_loss = avg_loss / (1 - beta ** batch_num)\n",
    "\n",
    "        # Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "            print()\n",
    "            print('    Exploding loss; finish test.')\n",
    "            break\n",
    "\n",
    "        # Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num == 1:\n",
    "            best_loss = smoothed_loss\n",
    "            \n",
    "        # Store the values\n",
    "        self.losses.append(smoothed_loss)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # Do the backward pass and optimizer step\n",
    "        try:\n",
    "            # CRITICAL: Use retain_graph=False and no double backward pass\n",
    "            loss.backward(retain_graph=False)\n",
    "            self.optimizer.step()\n",
    "        except Exception as e:\n",
    "            print(f\"\\nBackward pass failed at batch {batch_num}: {e}\")\n",
    "            break\n",
    "\n",
    "        # Update the lr for the next step\n",
    "        lr *= mult\n",
    "        self.optimizer.param_groups[0]['lr'] = lr\n",
    "        \n",
    "        # Stop after reasonable number of batches to avoid infinite loops\n",
    "        if batch_num >= 100:\n",
    "            print(\"\\nStopping after 100 batches\")\n",
    "            break\n",
    "\n",
    "    print()\n",
    "    print('    Completed test.')\n",
    "    return self\n",
    "\n",
    "# Apply the complete fix\n",
    "import lr_range_test\n",
    "lr_range_test.LRRangeTest.run = completely_fixed_lr_test_run\n",
    "print(\"Applied COMPLETE fix to LRRangeTest.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Using GPU-COMPATIBLE FIXED lr_test.run method\n",
      ">>> Compute loss at increasing LR values\n",
      "    Iterate over mini-batches: 22\n",
      "    Exploding loss; finish test.\n",
      "\n",
      "    Completed test.\n",
      "CPU times: user 1.32 s, sys: 210 ms, total: 1.53 s\n",
      "Wall time: 2.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)  # Disable anomaly detection too\n",
    "gpu_multisurv.test_lr_range()  # Use the new GPU instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loss values collected: 21\n",
      "Number of learning rates tested: 21\n",
      "LR range: 1.00e-06 to 4.64e+00\n",
      "Loss range: 0.010118 to 0.039159\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdwAAAEqCAYAAAC/aOHxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/UklEQVR4nO3deZyNdf/H8deZMYtZDAazYJhIQ0jNRKMmdGtsIUtJ3SgSjX7K1N1CZYncN25JlokUkq3SoqaEkJhuS9wVkoSxzNJwM8is5/r9ceXomBmzGOfM8n4+HudxO9/re13X55yL3vd1ne/1vSyGYRiIiIjINeXi7AJEREQqAwWuiIiIAyhwRUREHECBKyIi4gAKXBEREQdQ4IqIiDiAAldERMQBFLgiIiIOoMAVERFxAAWulDsLFy7EYrGwY8cOZ5dSbO3bt6d9+/bOLqPEli5dyowZM5xdRrnx6quv8vHHH1/Tfezdu5dx48Zx+PDha7ofuXoKXBEHmjNnDnPmzHF2GSWmwC0eRwXu+PHjFbjlQBVnFyBSXhmGQUZGBlWrVi3yOs2aNbuGFRXfhQsXilW/I/3xxx94eXk5uwyRUqMzXKmwDhw4wIMPPkidOnXw8PCgadOmzJ49265PRkYGTz/9NK1atcLPz4+aNWsSGRnJJ598kmd7FouFJ554gri4OJo2bYqHhweLFi2yXeLesGEDjz/+OLVq1cLf35/evXtz4sQJu21cfkn58OHDWCwWpk2bxvTp0wkNDcXHx4fIyEi+++67PDXMnz+fJk2a4OHhQbNmzVi6dCkPP/wwDRs2LPT7aNiwIffccw+rVq3i5ptvxtPTk/HjxwMwe/Zs7rzzTurUqYO3tzctWrRgypQpZGdn29X++eefc+TIESwWi+11UVZWFhMnTiQsLAwPDw9q167NI488wu+//15obQ8//DA+Pj78+OOPREdH4+vry9/+9jcA1q5dS8+ePalXrx6enp40btyYYcOGkZaWZreNcePGYbFY2LNnD/3798fPz4+AgAAGDx7MmTNn7PqePn2aIUOGULNmTXx8fOjWrRu//fYbFouFcePG2fUtyt+j/FgsFs6fP8+iRYts39Vfj31ycjLDhg2jXr16uLu7Exoayvjx48nJybHbzty5c7npppvw8fHB19eXsLAwRo8eDZg/r9x3330AdOjQwbafhQsXFlqfOJ7OcKVC2rt3L23btiUkJIR///vfBAYGsmbNGkaOHElaWhpjx44FIDMzk1OnTvHMM89Qt25dsrKyWLduHb179+add95h4MCBdtv9+OOP2bx5My+//DKBgYHUqVOH7du3A/Doo4/SrVs3li5dytGjR/nHP/7B3//+d77++utC6509ezZhYWG2y7UvvfQSXbt25dChQ/j5+QEwb948hg0bRp8+fXjttdc4c+YM48ePJzMzs8jfy/fff8++fft48cUXCQ0NxdvbG4CDBw/y4IMPEhoairu7O//973+ZNGkSP//8M2+//TZgXg5/7LHHOHjwIB999JHddq1WKz179mTz5s08++yztG3bliNHjjB27Fjat2/Pjh07Cj2TzsrKokePHgwbNoznn3/eFjwHDx4kMjKSRx99FD8/Pw4fPsz06dO54447+PHHH3Fzc7PbTp8+fejXrx9Dhgzhxx9/5IUXXgCwfQ6r1Ur37t3ZsWMH48aN45ZbbiEhIYHOnTvnqamof4/yk5CQwF133UWHDh146aWXAKhWrRpghm3r1q1xcXHh5ZdfplGjRiQkJDBx4kQOHz7MO++8A8Dy5cuJiYnh//7v/5g2bRouLi78+uuv7N27F4Bu3brx6quvMnr0aGbPns0tt9wCQKNGja74XYuTGCLlzDvvvGMAxvbt2wvs06lTJ6NevXrGmTNn7NqfeOIJw9PT0zh16lS+6+Xk5BjZ2dnGkCFDjJtvvtluGWD4+fnlWfdiPTExMXbtU6ZMMQAjKSnJ1tauXTujXbt2tveHDh0yAKNFixZGTk6OrX3btm0GYCxbtswwDMPIzc01AgMDjTZt2tjt48iRI4abm5vRoEGDAr+Lixo0aGC4uroa+/fvv2K/3NxcIzs721i8eLHh6upq93m7deuW776WLVtmAMaHH35o1759+3YDMObMmXPFfQ4aNMgAjLfffvuK/axWq5GdnW0cOXLEAIxPPvnEtmzs2LEGYEyZMsVunZiYGMPT09OwWq2GYRjG559/bgDG3Llz7fpNnjzZAIyxY8fa2kr69+gib29vY9CgQXnahw0bZvj4+BhHjhyxa582bZoBGHv27LHtp3r16lfcx/vvv28AxoYNG67YT5xPl5SlwsnIyGD9+vX06tULLy8vcnJybK+uXbuSkZFhd7n2/fff5/bbb8fHx4cqVarg5ubGggUL2LdvX55t33XXXdSoUSPf/fbo0cPufcuWLQE4cuRIoTV369YNV1fXAtfdv38/ycnJ3H///XbrhYSEcPvttxe6/b9ut0mTJnnad+3aRY8ePfD398fV1RU3NzcGDhxIbm4uv/zyS6Hb/eyzz6hevTrdu3e3+75btWpFYGAgGzduLFJ9ffr0ydOWmprK8OHDqV+/vu34NGjQACDfY5TfccjIyCA1NRWATZs2AeT5Lvv372/3vrh/j4rjs88+o0OHDgQHB9ttt0uXLnY1tm7dmtOnT9O/f38++eSTPJfRpXxR4EqFc/LkSXJycnjjjTdwc3Oze3Xt2hXA9h+uVatWcf/991O3bl2WLFlCQkIC27dvZ/DgwWRkZOTZdlBQUIH79ff3t3vv4eEBmAOTClPYuidPngQgICAgz7r5tRUkv/oTExOJiori+PHjvP7662zevJnt27fbfqcsSv0pKSmcPn0ad3f3PN95cnJykYLCy8vLdsn1IqvVSnR0NKtWreLZZ59l/fr1bNu2zRZ0+dVWlO+ySpUq1KxZ067f5d9jcf4eFVdKSgqrV6/Os90bb7zRbrsDBgzg7bff5siRI/Tp04c6derQpk0b1q5dW6L9inPpN1ypcGrUqIGrqysDBgxgxIgR+fYJDQ0FYMmSJYSGhrJixQq7AUAF/S761z6OdDFEUlJS8ixLTk4u8nbyq//jjz/m/PnzrFq1ynbmCLB79+4ib/fiQLEvv/wy3+W+vr4lqu2nn37iv//9LwsXLmTQoEG29l9//bXItV3O39+fnJwcTp06ZRe6l3+Pxfl7VFy1atWiZcuWTJo0Kd/lwcHBtj8/8sgjPPLII5w/f55vvvmGsWPHcs899/DLL7/YHS8p+xS4UuF4eXnRoUMHdu3aRcuWLXF3dy+wr8Viwd3d3e4/9snJyfmOUnamG264gcDAQFauXElsbKytPTExka1bt9r9B7q4Ln72i2eCYN7yNH/+/Dx9PTw88j2rvOeee1i+fDm5ubm0adOmxLUUpTaAN998s8TbbNeuHVOmTGHFihU8/vjjtvbly5fb9SvO36OCXOn7io+Pp1GjRgX+RHE5b29vunTpQlZWFvfeey979uyhQYMGxbqSIs6lwJVy6+uvv873Zv+uXbvy+uuvc8cddxAVFcXjjz9Ow4YNOXv2LL/++iurV6+2jRy+eJtMTEwMffv25ejRo7zyyisEBQVx4MABB3+igrm4uDB+/HiGDRtG3759GTx4MKdPn2b8+PEEBQXh4lLyX4fuvvtu3N3d6d+/P88++ywZGRnMnTuX//3vf3n6tmjRglWrVjF37lzCw8NxcXEhIiKCBx54gPfee4+uXbvy5JNP0rp1a9zc3Dh27BgbNmygZ8+e9OrVq9i1hYWF0ahRI55//nkMw6BmzZqsXr36qi6pdu7cmdtvv52nn36a9PR0wsPDSUhIYPHixQB232VR/x4VpEWLFmzcuJHVq1cTFBSEr68vN9xwAxMmTGDt2rW0bduWkSNHcsMNN5CRkcHhw4eJj48nLi6OevXqMXToUKpWrcrtt99OUFAQycnJTJ48GT8/P2699VYAmjdvDpij2H19ffH09CQ0NDTPpXUpA5w9akukuC6OCi7odejQIcMwzBHAgwcPNurWrWu4ubkZtWvXNtq2bWtMnDjRbnv//Oc/jYYNGxoeHh5G06ZNjfnz59tGvP4VYIwYMaLAei4fNb1hw4Y8o0cLGqU8derUPNvlshGzhmEY8+bNMxo3bmy4u7sbTZo0Md5++22jZ8+eeUZU56dBgwZGt27d8l22evVq46abbjI8PT2NunXrGv/4xz+ML774Ik/9p06dMvr27WtUr17dsFgsdt9Rdna2MW3aNNt2fHx8jLCwMGPYsGHGgQMHrljboEGDDG9v73yX7d2717j77rsNX19fo0aNGsZ9991nJCYm5vl+Lh6z33//3W79i8fn4t+Li5/jkUceMapXr254eXkZd999t/Hdd98ZgPH666/brV/Uv0f52b17t3H77bcbXl5eBmB37H///Xdj5MiRRmhoqOHm5mbUrFnTCA8PN8aMGWOcO3fOMAzDWLRokdGhQwcjICDAcHd3N4KDg43777/f+OGHH+z2M2PGDCM0NNRwdXU1AOOdd94ptDZxPIthGIZDE15ESs3p06dp0qQJ9957L/PmzXN2OeXa0qVLeeihh9iyZQtt27Z1djlSAemSskg5kZyczKRJk+jQoQP+/v4cOXKE1157jbNnz/Lkk086u7xyZdmyZRw/fpwWLVrg4uLCd999x9SpU7nzzjsVtnLNKHBFygkPDw8OHz5MTEwMp06dwsvLi9tuu424uDjb7SRSNL6+vixfvpyJEydy/vx5goKCePjhh5k4caKzS5MKTJeURUREHEATX4iIiDiAAldERMQBFLgiIiIOoEFTJWS1Wjlx4gS+vr5Om+5PRESczzAMzp49S3Bw8BUnoVHgltCJEyeoX7++s8sQEZEy4ujRo9SrV6/A5QrcEro4GfvRo0fzPOHkSqxWK6mpqdSpU+eqpuMrjW0Wd72i9C+sz5WWF7Qsv/Zr8T2WVFk6psVdV8e0YKVdj6OOaVH7V8bjeq3+rf7222+Eh4cX+pAOBW4JXbyMXK1atWIH7oULF6hWrVqpHvCSbLO46xWlf2F9rrS8oGX5tV+L77GkytIxLe66OqYFK+16HHVMi9q/Mh7Xa/Vv1cfHByj8aWLO/1stIiJSCShwRUREHECBKyIi4gAKXBEREQdQ4IqIiDiAAldERCqnLVtg5kyH7U6BKyIilUtSEgwcCHfcAbGxsGePQ3ar+3BFRKRyyM7GOy4Oy2uvwdmzYLHA4MEQEOCQ3StwRUSk4lu3DsvIkVTbt89836YNzJoFEREOK0GXlEVEpOI6cgT69oW778aybx+5/v5Y33oLtm51aNiCznBFRKQiysiAadPg1VfhwgVwdcUYMYLfH3+cOk2agBOmmVTgiohIxfLZZ/Dkk/Dbb+b7du3gjTcwbrwRIyXFaWXpkrKIiFQMBw5At27QvbsZtsHBsGwZbNgALVo4uzrnB+6cOXMIDQ3F09OT8PBwNm/efMX+mzZtIjw8HE9PT6677jri4uLslq9atYqIiAiqV6+Ot7c3rVq14t1337XrM27cOCwWi90rMDCw1D+biIg4wPnzMGYMNG8O8fHg5gbPPw/798MDD5ijkcsAp15SXrFiBU899RRz5szh9ttv580336RLly7s3buXkJCQPP0PHTpE165dGTp0KEuWLGHLli3ExMRQu3Zt+vTpA0DNmjUZM2YMYWFhuLu789lnn/HII49Qp04dOnXqZNvWjTfeyLp162zvXV1dr/0HFhGR0mMY8P778PTTcOyY2da5M7z+OjRp4tza8uHUwJ0+fTpDhgzh0UcfBWDGjBmsWbOGuXPnMnny5Dz94+LiCAkJYcaMGQA0bdqUHTt2MG3aNFvgtm/f3m6dJ598kkWLFvHtt9/aBW6VKlV0VisiUl7t2QP/93/m5WKA0FCYMcO8nFxGzmgv57RLyllZWezcuZPo6Gi79ujoaLZu3ZrvOgkJCXn6d+rUiR07dpCdnZ2nv2EYrF+/nv3793PnnXfaLTtw4ADBwcGEhobywAMP8NvFH9cLkJmZSXp6ut1LREQc7MwZGDUKbrrJDFtPTxg/3gzgHj3KbNiCEwM3LS2N3NxcAi6b4SMgIIDk5OR810lOTs63f05ODmlpaba2M2fO4OPjg7u7O926deONN97g7rvvti1v06YNixcvZs2aNcyfP5/k5GTatm3LyZMnC6x38uTJ+Pn52V7169cvyccWEZGSsFph0SLzUvGMGZCbC717w7598PLLULWqsysslNNvC7Jc9v9GDMPI01ZY/8vbfX192b17N+fOnWP9+vXExsZy3XXX2S43d+nSxda3RYsWREZG0qhRIxYtWkRsbGy++33hhRfslqWnpyt0RUQcYedOeOIJ+O478/0NN5gPHbjsimdZ57TArVWrFq6urnnOZlNTU/OcxV4UGBiYb/8qVarg7+9va3NxcaFx48YAtGrVin379jF58uQ8v+9e5O3tTYsWLThw4ECB9Xp4eODh4VGUjyYiIqXh5Elz9PG8eeYAKR8fGDsWRo4Ed3dnV1dsTruk7O7uTnh4OGvXrrVrX7t2LW3bts13ncjIyDz9v/rqKyIiInBzcytwX4ZhkJmZWeDyzMxM9u3bR1BQUDE+gYiIXBO5uTB3Llx/Pbz5phm2Dz1k3ubzzDPlMmzByZeUY2NjGTBgABEREURGRjJv3jwSExMZPnw4YF7GPX78OIsXLwZg+PDhzJo1i9jYWIYOHUpCQgILFixg2bJltm1OnjyZiIgIGjVqRFZWFvHx8SxevJi5c+fa+jzzzDN0796dkJAQUlNTmThxIunp6QwaNMixX4CIiNjbssW8fLx7t/m+ZUt44w24bOBreeTUwO3Xrx8nT55kwoQJJCUl0bx5c+Lj42nQoAEASUlJJCYm2vqHhoYSHx/PqFGjmD17NsHBwcycOdN2SxDA+fPniYmJ4dixY1StWpWwsDCWLFlCv379bH2OHTtG//79SUtLo3bt2tx222189913tv2KiIiDJSXBc8/BxYmKqleHV16B4cOhitOHG5UKp3+KmJgYYmJi8l22cOHCPG3t2rXj+++/L3B7EydOZOLEiVfc5/Lly4tVo4iIXCPZ2eYZ7Lhxl55RO2SI+dCB2rWdXV2pcnrgiohIJbV+vTl5xcVn1LZubT6j9tZbnVvXNeL0uZRFRKSSSUyE++6Djh3NsK1dGxYsgISEChu2oMAVERFHyciASZMgLAw++MB8Ju3IkfDLLzB4sFOeUetIuqQsIiLX3mefmVMyXpxG9847zd9uW7Z0bl0OpMAVEZFr59dfqTFiBC4Xn84WHAzTppWpx+Y5igJXRERKl2HAunUwezaWTz/F02rFcHPDEhsLL75ozhhVCSlwRUSkdJw5g9dbb2F57z1zVijAAmTcdRfus2ZhadrUufU5mQJXRESuzo8/mmezS5bgd/682ebjA4MGYR0+nP/5+xc4R35losAVEZHiy8qCFSvMOY83bwbMs9nsJk1wHTkSl4EDwdfXfKxeSopzay0jFLgiIlJ0x49jiYujzrx5uKSmmm2urtC7N9bhw0kLCyMgMLDC3+JTEgpcERG5MsOAjRth9mz4+GMsubm4AkZQEJbHHoPHHjNHH+ts9ooUuCIiki/L2bPw4YfmZeO9e23txp13cvrBB/F7+GEsek54kSlwRUTE3p49WGbNos677+JycRCUtzcMGAAxMRg33khGSgp+V3gOueSlwBUREfOpPR9/bF423rQJC+YgKCMsDEtMDAwcCH5+Zl+r1YmFll8KXBGRyiwpCd56C+bNgxMnzDZXV4wePTjVvz81evfG4urq3BorCAWuiEhlYxjwzTdUnz4dyxdfQE6O2R4QAEOHwrBhGMHBZKWkVLrpF68lBa6ISGVx7hy8+y7MmYPLTz9R9WL7HXfAiBHQuze4u5ttumxc6hS4IiIV3b59MGcOLFoEZ88CYHh58Ufv3lSNjcXl5pudXGDloMAVEamIcnJg1Srzlp6vv77U3qSJOdJ4wADSMzOpqikXHUaBKyJSkaSkwLx51Jk7F5ekJLPNxQV69DAvG991l/lek1Q4nAJXRKS8MwzYutW8peeDD3DJzjaba9fG8ucgKEJCnFykKHBFRMqzffvg73+H77+3NRmRkZx+6CH8Bg/GUrXqFVYWR1LgioiUV198AQ88AOnpULUqPPggjBiBcdNN5kxQmnaxTFHgioiUN4YBr78OTz9t/hZ7553w/vtQp465XLf0lEl6fpKISHmSlWX+JjtqlBmsQ4bA2rWXwlbKLJ3hioiUFydPwn33waZN5kjjadPgqac0G1Q5ocAVESkHqvzyC5bBg+G338DXF5Yvh65dnV2WFIMCV0SkrPviC/z79zefTxsaCqtXw403OrsqKSb9hisiUlYZBsyYgaVHD1zOnsW4807Ytk1hW04pcEVEyqK/DI6yWK380b8/xpo1UKuWsyuTEtIlZRGRsubkSejTxzY4yjp1KmceeADPi0/ykXJJgSsiUpbs2wfdu8PBg5cGR3XurHmPKwBdUhYRKSu+/BJuu80M29BQSEjQSOQKRIErIuJsfw6Ools3c5pGDY6qkBS4IiLOVNDMURocVeHoN1wREWe5bHCUZo6q2BS4IiLOkN/gKP1eW6E5/ZLynDlzCA0NxdPTk/DwcDZv3nzF/ps2bSI8PBxPT0+uu+464uLi7JavWrWKiIgIqlevjre3N61ateLdd9+96v2KiJQaDY6qlJwauCtWrOCpp55izJgx7Nq1i6ioKLp06UJiYmK+/Q8dOkTXrl2Jiopi165djB49mpEjR/Lhhx/a+tSsWZMxY8aQkJDADz/8wCOPPMIjjzzCmjVrSrxfEZFSYRgwc6YGR1VSTg3c6dOnM2TIEB599FGaNm3KjBkzqF+/PnPnzs23f1xcHCEhIcyYMYOmTZvy6KOPMnjwYKZNm2br0759e3r16kXTpk1p1KgRTz75JC1btuTbb78t8X5FRK5aVhZ+zz6LiwZHVVpOC9ysrCx27txJdHS0XXt0dDRbt27Nd52EhIQ8/Tt16sSOHTvIzs7O098wDNavX8/+/fu58847S7xfgMzMTNLT0+1eIiJFcvIkls6d8XrvPQwXF5g+HebPB80cVak4bdBUWloaubm5BAQE2LUHBASQnJyc7zrJycn59s/JySEtLY2goCAAzpw5Q926dcnMzMTV1ZU5c+Zw9913l3i/AJMnT2b8+PHF/pwiUsn9OTjKcvAgVh8fWLYMyz33OLsqcQKnD5qyXDb83TCMPG2F9b+83dfXl927d7N9+3YmTZpEbGwsGzduvKr9vvDCC5w5c8b2Onr06BU/l4jIXwdHGaGhnFy9WoOjKjGnneHWqlULV1fXPGeVqampec4+LwoMDMy3f5UqVfD397e1ubi40LhxYwBatWrFvn37mDx5Mu3bty/RfgE8PDzw8PAo1mcUkUrq4uCo2Fjz99qoKIwPPiAnN9fZlYkTOe0M193dnfDwcNauXWvXvnbtWtq2bZvvOpGRkXn6f/XVV0RERODm5lbgvgzDIDMzs8T7FREpsoszRz311KXBUevWaXCUOHfii9jYWAYMGEBERASRkZHMmzePxMREhg8fDpiXcY8fP87ixYsBGD58OLNmzSI2NpahQ4eSkJDAggULWLZsmW2bkydPJiIigkaNGpGVlUV8fDyLFy+2G4Fc2H5FRErkSjNHWa3Ork6czKmB269fP06ePMmECRNISkqiefPmxMfH06BBAwCSkpLs7o0NDQ0lPj6eUaNGMXv2bIKDg5k5cyZ9+vSx9Tl//jwxMTEcO3aMqlWrEhYWxpIlS+jXr1+R9ysiUmyaOUoK4fSpHWNiYoiJicl32cKFC/O0tWvXju+//77A7U2cOJGJEyde1X5FRIrlyy+hXz9zMovQUFi9WpNZSB5OH6UsIlJuGQa8/vqlmaOiojRzlBRIgSsiUhLZ2TB8+KXBUYMHa3CUXJHTLymLiJQ7J09C376wcaM5OGrqVPN5tnqsnlyBAldEpDg0OEpKSIErIlJUGhwlV0G/4YqIFEaDo6QUKHBFRApiGOZZ7G23aXCUXDVdUhYRuZzVCh98QK3x43HZu9dsq1oVJk26NHOUSDEpcEVELsrNhRUrYNIkXPbuxQUwfHywPPGEOQq5Th1nVyjlmAJXRCQ7G5YsgcmT4cABAAw/P84NHoz36NFYdPlYSoECV0Qqr8xMWLgQ/vlPOHzYbPP3h1GjMGJiOJeRgXfNms6sUCoQBa6IVD4XLsD8+TBlChw/brYFBMAzz5izR/n4mL/jZmQ4t06pUBS4IlJ5nDsHcXHmY/NSUsy2unXh2Wdh6FBzYJTINaLAFZGK78wZmDULXnvNnJYRoEEDeOEFePhh8PBwanlSOShwRaTiOnXKnLBi5kw4fdpsa9wYRo+Gv/8d3NycWp5ULgpcEal4UlNh+nSYPdu8jAzQrBmMGQP33w9V9J8+cTz9rRORiiMpyXxyT1ycOTAK4Kab4MUXoXdv88k+Ik6iwBWR8i8xEf71L1iwwLzVB+DWW+Gll+CeezQzlJQJJQrco0ePYrFYqFevHgDbtm1j6dKlNGvWjMcee6xUCxQRKdBvv5mTVSxaZE5eAXD77WbQRkcraKVMKdH1lQcffJANGzYAkJyczN133822bdsYPXo0EyZMKNUCRUTy+PlnGDgQmjSBt94yw/auu+Drr2HzZujUSWErZU6JAvenn36idevWAKxcuZLmzZuzdetWli5dysKFC0uzPhGRS378ER54wBwA9e675tzHnTvDli2wfj106KCglTKrRJeUs7Oz8fjzvrV169bRo0cPAMLCwkhKSiq96kREAHbuhIkT4eOPL7X17GmOOr71VqeVJVIcJTrDvfHGG4mLi2Pz5s2sXbuWzp07A3DixAn8/f1LtUARqcQSEsyHvkdEmGFrscB998Hu3eZ7ha2UIyUK3H/961+8+eabtG/fnv79+3PTTTcB8Omnn9ouNYuIlNimTdCxI7RtC/Hx5u08f/87/PQTrFxp3uojUs6U6JJy+/btSUtLIz09nRo1atjaH3vsMby8vEqtOBGpRAwD940bscydaw58AnOCioEDzSkYGzd2bn0iV6lEgXvhwgUMw7CF7ZEjR/joo49o2rQpnTp1KtUCRaSSmDIF/9GjzT+7u8PgwfDcc9CwoVPLEiktJbqk3LNnTxYvXgzA6dOnadOmDf/+97+59957mTt3bqkWKCKVwMGDWMaPB8AYNsy8v3buXIWtVCglCtzvv/+eqKgoAD744AMCAgI4cuQIixcvZubMmaVaoIhUcIYBI0diycwkMyoKY/Zs85F5IhVMiQL3jz/+wNfXF4CvvvqK3r174+Liwm233caRI0dKtUARqeBWr4b4eAw3N85MnKj7aKXCKlHgNm7cmI8//pijR4+yZs0aoqOjAUhNTaVatWqlWqCIVGAXLsCTT5p/jo0l9/rrnVuPyDVUosB9+eWXeeaZZ2jYsCGtW7cmMjISMM92b7755lItUEQqLsu//gWHD0P9+hhjxji7HJFrqkSjlPv27csdd9xBUlKS7R5cgL/97W/06tWr1IoTkYrL9dAhmDLFfPPaa+DtfenZtSIVUIkfzxcYGEhgYCDHjh3DYrFQt25dTXohIkVjGFR76SUsmZnmU3169zYHT4lUYCW6pGy1WpkwYQJ+fn40aNCAkJAQqlevziuvvILVai3tGkWkovn0Uzy//hrDzQ3eeEMDpaRSKNEZ7pgxY1iwYAH//Oc/uf322zEMgy1btjBu3DgyMjKYNGlSadcpIhXFH39gGTXK/PPTT5uP2BOpBEoUuIsWLeKtt96yPSUI4KabbqJu3brExMQocEWkYP/8J5YjR8gNDsYyejQ6t5XKokSXlE+dOkVYWFie9rCwME6dOlWsbc2ZM4fQ0FA8PT0JDw9n88U5VAuwadMmwsPD8fT05LrrriMuLs5u+fz584mKiqJGjRrUqFGDjh07sm3bNrs+48aNw2Kx2L0CAwOLVbeIlMCvv8K//gVA+oQJ5kApkUqiRIF70003MWvWrDzts2bNomXLlkXezooVK3jqqacYM2YMu3btIioqii5dupCYmJhv/0OHDtG1a1eioqLYtWsXo0ePZuTIkXz44Ye2Phs3bqR///5s2LCBhIQEQkJCiI6O5vjx43bbuvHGG0lKSrK9fvzxxyLXLSIl8OeMUmRlYURHk9Gli7MrEnGoEl1SnjJlCt26dWPdunVERkZisVjYunUrR48eJT4+vsjbmT59OkOGDOHRRx8FYMaMGaxZs4a5c+cyefLkPP3j4uIICQlhxowZADRt2pQdO3Ywbdo0+vTpA8B7771nt878+fP54IMPWL9+PQMHDrS1V6lSRWe1Io70ySfwxRfg7o4xc6YGSkmlU6Iz3Hbt2vHLL7/Qq1cvTp8+zalTp+jduzd79uzhnXfeKdI2srKy2Llzp22Wqouio6PZunVrvuskJCTk6d+pUyd27NhBdnZ2vuv88ccfZGdnU7NmTbv2AwcOEBwcTGhoKA888AC//fZbkeoWkRL4449LM0o98wxoRimphEp8H25wcHCewVH//e9/WbRoEW+//Xah66elpZGbm0tAQIBde0BAAMnJyfmuk5ycnG//nJwc0tLSCAoKyrPO888/T926denYsaOtrU2bNixevJgmTZqQkpLCxIkTadu2LXv27MHf3z/ffWdmZpKZmWl7n56eXuhnFBGTZfJkSEyEkBC4+Ag+kUqmRGe4pcly2WUlwzDytBXWP792MC99L1u2jFWrVuHp6Wlr79KlC3369KFFixZ07NiRzz//HDBHXxdk8uTJ+Pn52V7169cv/MOJCK6//QbTpplvZszQQCmptJwWuLVq1cLV1TXP2Wxqamqes9iLAgMD8+1fpUqVPGem06ZN49VXX+Wrr74qdCCXt7c3LVq04MCBAwX2eeGFFzhz5oztdfTo0StuU0QwZ5R68UUsWVnQuTPce6+zKxJxGqcFrru7O+Hh4axdu9aufe3atbRt2zbfdSIjI/P0/+qrr4iIiMDNzc3WNnXqVF555RW+/PJLIiIiCq0lMzOTffv25XtJ+iIPDw+qVatm9xKRQnz8MZ4bN2K4u4MGSkklV6zfcHv37n3F5adPny7WzmNjYxkwYAARERFERkYyb948EhMTGT58OGCeVR4/fpzFixcDMHz4cGbNmkVsbCxDhw4lISGBBQsWsGzZMts2p0yZwksvvcTSpUtp2LCh7YzYx8cHHx8fAJ555hm6d+9OSEgIqampTJw4kfT0dAYNGlSs+kXkCs6fxxIba/5ZA6VEihe4fn5+hS7/6603henXrx8nT55kwoQJJCUl0bx5c+Lj42nQoAEASUlJdvfkhoaGEh8fz6hRo5g9ezbBwcHMnDnTdksQmBNpZGVl0bdvX7t9jR07lnHjxgFw7Ngx+vfvT1paGrVr1+a2227ju+++s+1XRErBq69iSUwkp25dXF54QTNKSaVXrMAt6i0/xRETE0NMTEy+yxYuXJinrV27dnz//fcFbu/w4cOF7nP58uVFLU9ESuKXX2DqVMCcUaq6l5eTCxJxPqePUhaRCsYw4P/+D7KzMTp3JrNzZ2dXJFImKHBFpHR99BF89ZU5o9Trr2uglMifFLgiUnrOn4ennjL//Nxz0LixU8sRKUsUuCJSeiZNgqNHoUEDeP55Z1cjUqYocEWkdOzff2lGqddfBw2UErGjwBWRq2cYWEaOhOxs6NoVevRwdkUiZY4CV0Sumufnn2NZtw48PDSjlEgBFLgicnXOnaPa2LHmn597Dho1cm49ImWUAldErorl1VdxTUrCaNhQA6VErkCBKyIl9/PPMH06AMZrr0HVqk4uSKTsUuCKSMn8OaOUJTubjI4doXt3Z1ckUqYpcEWkZD74ANatw/DwIH3CBA2UEimEAldEiu/cORg1yvzzc8+R27ChU8sRKQ8UuCJSfK+8AsePQ2goxrPPOrsakXJBgSsixbNvn22gFDNnaqCUSBEpcEWk6C4+ei8nB+65x3yJSJEocEWk6N5/H9avN2eUev11Z1cjUq4ocEWkaM6evTRQ6oUX4LrrnFuPSDmjwBWRIrFMnAgnTphBq4FSIsWmwBWRQlX55ReYMcN8o4FSIiWiwBWRKzMMqo0ZgyUnx3zsXrduzq5IpFxS4IrIla1ciceWLRienpfOckWk2BS4IlKws2exPPMMAMbzz0NoqJMLEim/FLgiUrAJE7CcOEFOgwbwj384uxqRck2BKyL527PHdgk5/ZVXwNPTufWIlHMKXBHJyzDgiScgJwejRw8yO3Z0dkUi5Z4CV0TyWr4cNm4ET0/zwfIictUUuCJi7+xZePpp88+jR4MevSdSKhS4ImJv/HhISoJGjTRQSqQUKXBF5JKffrp0r+0bb2iglEgpUuCKiOniQKncXLj3XujSxdkViVQoClwRMS1bBps2mfMka6CUSKlT4IoIpKdroJTINabAFREs48dDcjI0bgx/TuUoIqVLgStSyVX5+WdzgBRooJTINaTAFanMDINqo0djyc2FXr2gc2dnVyRSYSlwRSqzpUvx+O47DA2UErnmnB64c+bMITQ0FE9PT8LDw9m8efMV+2/atInw8HA8PT257rrriIuLs1s+f/58oqKiqFGjBjVq1KBjx45s27btqvcrUuGcOYPl2WcBMMaMgQYNnFyQSMXm1MBdsWIFTz31FGPGjGHXrl1ERUXRpUsXEhMT8+1/6NAhunbtSlRUFLt27WL06NGMHDmSDz/80NZn48aN9O/fnw0bNpCQkEBISAjR0dEcP368xPsVqZDGjcOSnEzOdddBbKyzqxGp8JwauNOnT2fIkCE8+uijNG3alBkzZlC/fn3mzp2bb/+4uDhCQkKYMWMGTZs25dFHH2Xw4MFMmzbN1ue9994jJiaGVq1aERYWxvz587Faraxfv77E+xWpcH74wTZQKv2VV8DDw8kFiVR8TgvcrKwsdu7cSXR0tF17dHQ0W7duzXedhISEPP07derEjh07yM7OznedP/74g+zsbGrWrFni/QJkZmaSnp5u9xIplwwDRoyA3FyMXr3I7NDB2RWJVApOC9y0tDRyc3MJCAiwaw8ICCA5OTnfdZKTk/Ptn5OTQ1paWr7rPP/889StW5eOfz7PsyT7BZg8eTJ+fn62V/369Qv9jCJl0nvvwbffQtWqGNOnO7sakUrD6YOmLBaL3XvDMPK0FdY/v3aAKVOmsGzZMlatWoXnZfcWFne/L7zwAmfOnLG9jh49WmBfkTLrzJlLE1u89BKEhDi3HpFKpIqzdlyrVi1cXV3znFWmpqbmOfu8KDAwMN/+VapUwd/f36592rRpvPrqq6xbt46WLVte1X4BPDw88NDvXFJe5eTAJ5/A1KmQkgJNmmiglIiDOe0M193dnfDwcNauXWvXvnbtWtq2bZvvOpGRkXn6f/XVV0RERODm5mZrmzp1Kq+88gpffvklERERV71fkXIrNRUmTYLQUOjbF/7zH3B3h7g4DZQScTCnneECxMbGMmDAACIiIoiMjGTevHkkJiYyfPhwwLyMe/z4cRYvXgzA8OHDmTVrFrGxsQwdOpSEhAQWLFjAsmXLbNucMmUKL730EkuXLqVhw4a2M1kfHx98fHyKtF+Rcs0wcNu5E8uyZfDBB5CVZbbXqgWPPQbDhulSsogTODVw+/Xrx8mTJ5kwYQJJSUk0b96c+Ph4Gvx5A35SUpLdvbGhoaHEx8czatQoZs+eTXBwMDNnzqRPnz62PnPmzCErK4u+ffva7Wvs2LGMGzeuSPsVKZcuXIAVK7DMmkWtnTsvtbdpY45Kvu8+zZMs4kRODVyAmJgYYmJi8l22cOHCPG3t2rXj+++/L3B7hw8fvur9ipQrhw+bl4jfegtOnsQCGB4e0K8flieegFtvdXaFIkIZCFwRKQHDgHXrYNYs+OwzsFrN9pAQrMOH83v37tRu1gyLi9NvRBCRPylwRcqT9HRYtAhmz4b9+y+1d+wITzwB99wDFgvWlBTn1Sgi+VLgipQHe/aYIfvuu3DunNnm6wuDBkFMDDRteqnvxbNdESlTFLgiZVVODqxaBXPmwIYNl9qbNjXPZgcMMENXRMoFBa5IWZOaCvPnU2f2bFySksw2Fxfo2dMM2g4d4AqzoolI2aTAFSkLDAO2bTMHQa1cicuf984atWph0b2zIhWCAlfEmTIyYMUKM2h37LA1G23acOahh6g2ZAgWLy8nFigipUWBK+IMR47A3Lm2e2cBc6rFBx6AESMwwsO5kJJCNU1UIVJhKHBFHMUwYP1682x29Wq7e2d5/HEYMgRq1zbbNNJYpMJR4Ipca0W5d9bV1Xn1iYhDKHBFrpW9e82QXby48HtnRaTCU+CKlKYLF8zLxW++CV9/fald986KVHoKXJGrZbXi/t13WD77zHwcXnq62a57Z0XkLxS4IiW1fz+8+y6WJUvwP3LkUntIiHkm+9hjundWRGwUuCLFkZYGy5ebcxpv2waABbD6+mK57z4sAwdCVJR5disi8hcKXJHCZGSYj8BbvBi++MKc4xjMkcWdO2N96CFS2rQhoGFDPQ5PRAqkwBXJj2HAli1myK5cCWfOXFoWHm5eMu7fH+rUMe+Z1ePwRKQQClyRvzpwwLxcvGQJHDp0qb1+fXjoITNomzVzXn0iUm4pcEVOnjTnM373Xfjuu0vtPj5w331myLZrp99lReSqKHClcsrMhM8/N0P2888hO9tsd3GBTp3MkO3ZE/TgABEpJQpcqTwMAxISLv0u+7//XVp2882XfpcNDHRejSJSYSlwpeI7ePDS77IHD15qr1v30u+yzZs7rz4RqRQUuFIxnTplnsW++y5s3Xqp3dsb+vSBgQOhfXs9NEBEHEaBKxVHVhbEx5sh+9ln5nswf5ft2NEM2XvvNUNXRMTBFLhSvhmGObJ48WJzpPGpU5eWtWxphmz//hAc7LwaRURQ4Ep5k5VlTjJx7Bg+q1Zh+eQT897Zi4KCLv0u27Kl8+oUEbmMAlfKhnPn4Phx3PfuNadSTEmB5GRISsJy4gS1jh3D8vvv5j2zgAtge8idlxf07m2G7N/+pt9lRaRMUuDKtWO1mpd4k5IuvU6coNpvv2E5c8YWqCQlwfnzuAD++WzGArj9taFKFYzAQDIbN8Z90CBc+vY1J6kQESnDFLjO9N//wssvg6fn1b3c3XE9dw5yc82zvYvt12pmpOxs+P13+yBNSoLkZCwnTuB/9CiWtDTzLPXihBJ/cgEKGrJkeHuTW6cOrvXqYQkKMi8PBwZiDQjgf56e1GjWDJfgYPD3xwD+l5JCQECAZoASkXJBgetMSUnw6adXvRkXoE5+C9zcrhjUFg8PalgsWPz8oGrVvH08PMxLvX8GqiUpiTrHj+Py14FJl7EA7pc31qplTiYRFIQRGMj5atXwatwYl4uh+ufL8PLi9z9D1O6pO1YrWSkp8NdwtVqv6jsTEXE0Ba4zNWsGb75p/mZ5+SszM//2fF5GZibGhQtYMjKw/DWIsrPN19mz+e7eAngWo1wLYPt1tEoVW4ja/jcoCGtAAKerVqV6WBgudeuaIel+KYINq5WzKSl45XdmqhAVkQpMgetMISHw2GNXvRnDaiXl4pmh1Vp4SP8Z5tY//iA9NZVqHh64FBTwXl52YXrSzQ3/5s1xqV07/0u5ViuZl5+NioiIArfCqVLFHEBUlEFEVisXUlKoVtRwtFrJSUmBgsJWREQKpP9qioiIOIACV0RExAEUuCIiIg6gwBUREXEABa6IiIgDKHBFREQcQLcFlZBhGACkp6cXaz2r1crZs2epWrUqLqV0a01Jt1nc9YrSv7A+V1pe0LL82q/F91hSZemYFnddHdOClXY9jjqmRe1fGY/rtfq3eu7cOeBSLhREgVtCZ/+cval+/fpOrkRERMqCs2fP4ufnV+Byi1FYJEu+rFYrJ06cwNfXF4vFUqx1b731VrZv316q9ZR0m8Vdryj9C+tzpeUFLbu8PT09nfr163P06FGqVatWxOqvnbJ0TIu7ro5pwUr7uDrqmBa1f2U8rtfi32pERARff/01wcHBVzxz1hluCbm4uFCvXr0Srevq6lrqf/FKus3irleU/oX1udLygpYV1F6tWrUy8Y+4LB3T4q6rY1qw0j6ujjqmRe1fGY/rtfi3WqVKlSLlgfN/KKmERowYUWa2Wdz1itK/sD5XWl7QsmvxnZWmsnRMi7uujmnBSrtGRx3TovavjMfVmf9WdUlZyqX09HT8/Pw4c+ZMmfh/zXL1dEwrJh3XS3SGK+WSh4cHY8eOxcPDw9mlSCnRMa2YdFwv0RmuiIiIA+gMV0RExAEUuCIiIg6gwBUREXEABa6IiIgDKHBFREQcQIErFdrRo0dp3749zZo1o2XLlrz//vvOLklKSa9evahRowZ9+/Z1dilSQp999hk33HAD119/PW+99Zazy7nmdFuQVGhJSUmkpKTQqlUrUlNTueWWW9i/fz/e3t7OLk2u0oYNGzh37hyLFi3igw8+cHY5Ukw5OTk0a9aMDRs2UK1aNW655Rb+85//ULNmTWeXds3oDFcqtKCgIFq1agVAnTp1qFmzJqdOnXJuUVIqOnTogK+vr7PLkBLatm0bN954I3Xr1sXX15euXbuyZs0aZ5d1TSlwxam++eYbunfvTnBwMBaLhY8//jhPnzlz5hAaGoqnpyfh4eFs3ry5RPvasWMHVqtVj1R0AEceV3GOqz3GJ06coG7durb39erV4/jx444o3WkUuOJU58+f56abbmLWrFn5Ll+xYgVPPfUUY8aMYdeuXURFRdGlSxcSExNtfcLDw2nevHme14kTJ2x9Tp48ycCBA5k3b941/0ziuOMqznO1xzi/XzOL+6jTcscQKSMA46OPPrJra926tTF8+HC7trCwMOP5558v8nYzMjKMqKgoY/HixaVRphTTtTquhmEYGzZsMPr06XO1JcpVKskx3rJli3Hvvffalo0cOdJ47733rnmtzqQzXCmzsrKy2LlzJ9HR0Xbt0dHRbN26tUjbMAyDhx9+mLvuuosBAwZcizKlmErjuErZVpRj3Lp1a3766SeOHz/O2bNniY+Pp1OnTs4o12H0AHops9LS0sjNzSUgIMCuPSAggOTk5CJtY8uWLaxYsYKWLVvafmN69913adGiRWmXK0VUGscVoFOnTnz//fecP3+eevXq8dFHH3HrrbeWdrlSAkU5xlWqVOHf//43HTp0wGq18uyzz+Lv7++Mch1GgStl3uW/6xiGUeTfeu644w6sVuu1KEuu0tUcV6DCj2itCAo7xj169KBHjx6OLstpdElZyqxatWrh6uqa56wnNTU1z/9zlvJDx7Xi0zHOnwJXyix3d3fCw8NZu3atXfvatWtp27atk6qSq6XjWvHpGOdPl5TFqc6dO8evv/5qe3/o0CF2795NzZo1CQkJITY2lgEDBhAREUFkZCTz5s0jMTGR4cOHO7FqKYyOa8WnY1wCzh0kLZXdhg0bDCDPa9CgQbY+s2fPNho0aGC4u7sbt9xyi7Fp0ybnFSxFouNa8ekYF5/mUhYREXEA/YYrIiLiAApcERERB1DgioiIOIACV0RExAEUuCIiIg6gwBUREXEABa6IiIgDKHBFREQcQIErIoVq2LAhM2bMcHYZIuWaZpoSKSMefvhhTp8+bXtub1ny+++/4+3tjZeXl7NLyVdZ/u5ELtIZrkgllp2dXaR+tWvXdkrYFrU+kfJAgStSTuzdu5euXbvi4+NDQEAAAwYMIC0tzbb8yy+/5I477qB69er4+/tzzz33cPDgQdvyw4cPY7FYWLlyJe3bt8fT05MlS5bw8MMPc++99zJt2jSCgoLw9/dnxIgRdmF3+SVli8XCW2+9Ra9evfDy8uL666/n008/tav3008/5frrr6dq1ap06NCBRYsWYbFYOH36dIGf0WKxEBcXR8+ePfH29mbixInk5uYyZMgQQkNDqVq1KjfccAOvv/66bZ1x48axaNEiPvnkEywWCxaLhY0bNwJw/Phx+vXrR40aNfD396dnz54cPny4ZAdA5CopcEXKgaSkJNq1a0erVq3YsWMHX375JSkpKdx///22PufPnyc2Npbt27ezfv16XFxc6NWrF1ar1W5bzz33HCNHjmTfvn106tQJgA0bNnDw4EE2bNjAokWLWLhwIQsXLrxiTePHj+f+++/nhx9+oGvXrjz00EOcOnUKMMO9b9++3HvvvezevZthw4YxZsyYIn3WsWPH0rNnT3788UcGDx6M1WqlXr16rFy5kr179/Lyyy8zevRoVq5cCcAzzzzD/fffT+fOnUlKSiIpKYm2bdvyxx9/0KFDB3x8fPjmm2/49ttv8fHxoXPnzmRlZRX1qxcpPc59WJGIXDRo0CCjZ8+e+S576aWXjOjoaLu2o0ePGoCxf//+fNdJTU01AOPHH380DMMwDh06ZADGjBkz8uy3QYMGRk5Ojq3tvvvuM/r162d736BBA+O1116zvQeMF1980fb+3LlzhsViMb744gvDMAzjueeeM5o3b263nzFjxhiA8b///S//L+DP7T711FMFLr8oJibG6NOnj91nuPy7W7BggXHDDTcYVqvV1paZmWlUrVrVWLNmTaH7ECltOsMVKQd27tzJhg0b8PHxsb3CwsIAbJeNDx48yIMPPsh1111HtWrVCA0NBSAxMdFuWxEREXm2f+ONN+Lq6mp7HxQURGpq6hVratmype3P3t7e+Pr62tbZv38/t956q13/1q1bF+mz5ldfXFwcERER1K5dGx8fH+bPn5/nc11u586d/Prrr/j6+tq+s5o1a5KRkWF3qV3EUao4uwARKZzVaqV79+7861//yrMsKCgIgO7du1O/fn3mz59PcHAwVquV5s2b57l86u3tnWcbbm5udu8tFkueS9HFWccwDCwWi91yo4g3RFxe38qVKxk1ahT//ve/iYyMxNfXl6lTp/Kf//znituxWq2Eh4fz3nvv5VlWu3btItUiUpoUuCLlwC233MKHH35Iw4YNqVIl7z/bkydPsm/fPt58802ioqIA+Pbbbx1dpk1YWBjx8fF2bTt27CjRtjZv3kzbtm2JiYmxtV1+huru7k5ubq5d2y233MKKFSuoU6cO1apVK9G+RUqTLimLlCFnzpxh9+7ddq/ExERGjBjBqVOn6N+/P9u2beO3337jq6++YvDgweTm5tpG4c6bN49ff/2Vr7/+mtjYWKd9jmHDhvHzzz/z3HPP8csvv7By5UrbIKzLz3wL07hxY3bs2MGaNWv45ZdfeOmll9i+fbtdn4YNG/LDDz+wf/9+0tLSyM7O5qGHHqJWrVr07NmTzZs3c+jQITZt2sSTTz7JsWPHSuujihSZAlekDNm4cSM333yz3evll18mODiYLVu2kJubS6dOnWjevDlPPvkkfn5+uLi44OLiwvLly9m5cyfNmzdn1KhRTJ061WmfIzQ0lA8++IBVq1bRsmVL5s6daxul7OHhUaxtDR8+nN69e9OvXz/atGnDyZMn7c52AYYOHcoNN9xg+513y5YteHl58c033xASEkLv3r1p2rQpgwcP5sKFCzrjFafQTFMi4hCTJk0iLi6Oo0ePOrsUEafQb7gick3MmTOHW2+9FX9/f7Zs2cLUqVN54oknnF2WiNMocEXkmjhw4AATJ07k1KlThISE8PTTT/PCCy84uywRp9ElZREREQfQoCkREREHUOCKiIg4gAJXRETEARS4IiIiDqDAFRERcQAFroiIiAMocEVERBxAgSsiIuIAClwREREH+H9ALcE6FboA4AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the results\n",
    "print(f\"Number of loss values collected: {len(gpu_multisurv.lr_test.losses)}\")\n",
    "print(f\"Number of learning rates tested: {len(gpu_multisurv.lr_test.lrs)}\")\n",
    "print(f\"LR range: {min(gpu_multisurv.lr_test.lrs):.2e} to {max(gpu_multisurv.lr_test.lrs):.2e}\")\n",
    "print(f\"Loss range: {min(gpu_multisurv.lr_test.losses):.6f} to {max(gpu_multisurv.lr_test.losses):.6f}\")\n",
    "\n",
    "# Plot the results to find optimal learning rate\n",
    "gpu_multisurv.plot_lr_range(trim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Fix for the _predictions_to_pycox method in coach.py\n",
    "\n",
    "def fixed_predictions_to_pycox(self, preds, time_points=None):\n",
    "    \"\"\"Fixed version that properly handles the DataFrame structure.\"\"\"\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    \n",
    "    # preds shape is [num_patients, num_intervals] e.g., [864, 18]\n",
    "    # Convert predictions to DataFrame\n",
    "    df = pd.DataFrame(preds.cpu().numpy())\n",
    "    \n",
    "    if time_points is None:\n",
    "        # Get the number of intervals from the model output\n",
    "        n_intervals = preds.shape[1]  # Should be 18 for your model\n",
    "        \n",
    "        # Use the model's actual output intervals\n",
    "        if hasattr(self, 'output_intervals') and self.output_intervals is not None:\n",
    "            # Use the midpoints of the output intervals\n",
    "            intervals = self.output_intervals\n",
    "            if len(intervals) > n_intervals:\n",
    "                # Take the first n_intervals midpoints\n",
    "                midpoints = (intervals[:-1] + intervals[1:]) / 2\n",
    "                time_points = midpoints[:n_intervals]\n",
    "            else:\n",
    "                # Fallback to evenly spaced points\n",
    "                time_points = torch.linspace(0.5, intervals[-1].item() / 365, n_intervals)\n",
    "        else:\n",
    "            # Fallback: create time points that match the output size\n",
    "            time_points = torch.arange(0.5, 0.5 + n_intervals, 1.0)\n",
    "    \n",
    "    # Ensure time_points matches the prediction dimensions\n",
    "    if len(time_points) != preds.shape[1]:\n",
    "        print(f\"Warning: Adjusting time_points from {len(time_points)} to {preds.shape[1]}\")\n",
    "        time_points = torch.linspace(time_points[0], time_points[-1], preds.shape[1])\n",
    "    \n",
    "    # FIXED: The DataFrame structure should be transposed\n",
    "    # We want columns to be time points, rows to be patients\n",
    "    df = df.T  # Transpose so shape becomes [num_intervals, num_patients]\n",
    "    \n",
    "    # Convert time_points to numpy if it's a tensor\n",
    "    if torch.is_tensor(time_points):\n",
    "        time_points = time_points.cpu().numpy()\n",
    "    \n",
    "    # Set the index to time points\n",
    "    df.index = time_points\n",
    "    df.index.name = 'time'\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the fix to ModelCoach\n",
    "from coach import ModelCoach\n",
    "ModelCoach._predictions_to_pycox = fixed_predictions_to_pycox\n",
    "print(\"Applied corrected fix to ModelCoach._predictions_to_pycox method\")\n",
    "\n",
    "# Debug info\n",
    "print(f\"Model output intervals shape: {len(gpu_multisurv.output_intervals)}\")\n",
    "print(f\"Model risk layer output: {gpu_multisurv.model.risk_layer[0].out_features}\")\n",
    "print(f\"Expected: {len(gpu_multisurv.output_intervals) - 1} intervals for {len(gpu_multisurv.output_intervals)} breakpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Applied GPU-compatible fix to ModelCoach._predictions_to_pycox method\n",
      "Model output intervals shape: 18\n",
      "Model output intervals device: cuda:0\n",
      "Model risk layer output: 17\n",
      "Expected: 17 intervals for 18 breakpoints\n"
     ]
    }
   ],
   "source": [
    "# GPU-compatible fix for _predictions_to_pycox method\n",
    "def gpu_fixed_predictions_to_pycox(self, preds, time_points=None):\n",
    "    \"\"\"GPU-compatible version that properly handles device transfers.\"\"\"\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    \n",
    "    # preds shape is [num_patients, num_intervals] e.g., [864, 18]\n",
    "    # Convert predictions to DataFrame (move to CPU first)\n",
    "    df = pd.DataFrame(preds.detach().cpu().numpy())  # Added .detach() for GPU tensors\n",
    "    \n",
    "    if time_points is None:\n",
    "        # Get the number of intervals from the model output\n",
    "        n_intervals = preds.shape[1]  # Should be 18 for your model\n",
    "        \n",
    "        # Use the model's actual output intervals\n",
    "        if hasattr(self, 'output_intervals') and self.output_intervals is not None:\n",
    "            # Handle GPU tensors properly\n",
    "            intervals = self.output_intervals\n",
    "            if torch.is_tensor(intervals):\n",
    "                intervals = intervals.detach().cpu()  # Move to CPU for calculations\n",
    "            \n",
    "            if len(intervals) > n_intervals:\n",
    "                # Take the first n_intervals midpoints\n",
    "                midpoints = (intervals[:-1] + intervals[1:]) / 2\n",
    "                time_points = midpoints[:n_intervals]\n",
    "            else:\n",
    "                # Fallback to evenly spaced points\n",
    "                last_interval = intervals[-1].item() if torch.is_tensor(intervals) else intervals[-1]\n",
    "                time_points = torch.linspace(0.5, last_interval / 365, n_intervals)\n",
    "        else:\n",
    "            # Fallback: create time points that match the output size\n",
    "            time_points = torch.arange(0.5, 0.5 + n_intervals, 1.0)\n",
    "    \n",
    "    # Ensure time_points matches the prediction dimensions\n",
    "    if len(time_points) != preds.shape[1]:\n",
    "        print(f\"Warning: Adjusting time_points from {len(time_points)} to {preds.shape[1]}\")\n",
    "        first_point = time_points[0].item() if torch.is_tensor(time_points) else time_points[0]\n",
    "        last_point = time_points[-1].item() if torch.is_tensor(time_points) else time_points[-1]\n",
    "        time_points = torch.linspace(first_point, last_point, preds.shape[1])\n",
    "    \n",
    "    # FIXED: The DataFrame structure should be transposed\n",
    "    # We want columns to be time points, rows to be patients\n",
    "    df = df.T  # Transpose so shape becomes [num_intervals, num_patients]\n",
    "    \n",
    "    # Convert time_points to numpy if it's a tensor (ensure CPU)\n",
    "    if torch.is_tensor(time_points):\n",
    "        time_points = time_points.detach().cpu().numpy()\n",
    "    \n",
    "    # Set the index to time points\n",
    "    df.index = time_points\n",
    "    df.index.name = 'time'\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the GPU-compatible fix to ModelCoach\n",
    "from coach import ModelCoach\n",
    "ModelCoach._predictions_to_pycox = gpu_fixed_predictions_to_pycox\n",
    "print(\"âœ… Applied GPU-compatible fix to ModelCoach._predictions_to_pycox method\")\n",
    "\n",
    "# Debug info with GPU handling\n",
    "print(f\"Model output intervals shape: {len(gpu_multisurv.output_intervals)}\")\n",
    "print(f\"Model output intervals device: {gpu_multisurv.output_intervals.device}\")\n",
    "print(f\"Model risk layer output: {gpu_multisurv.model.risk_layer[0].out_features}\")\n",
    "print(f\"Expected: {len(gpu_multisurv.output_intervals) - 1} intervals for {len(gpu_multisurv.output_intervals)} breakpoints\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import utils\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# 2. Set up training parameters\n",
    "picked_lr = 5e-3\n",
    "\n",
    "# Add timestamp to make unique run tags\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "run_tag = utils.compose_run_tag(\n",
    "    model=gpu_multisurv, \n",
    "    lr=picked_lr,\n",
    "    dataloaders=gpu_multisurv.dataloaders,\n",
    "    log_dir='./training_logs/',\n",
    "    suffix=f'_breast_cancer_{timestamp}'  # Add timestamp\n",
    ")\n",
    "\n",
    "# 3. Create log directory (now it will be unique)\n",
    "log_dir = os.path.join('./training_logs/', run_tag)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "print(f\"Log directory created: {log_dir}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "fit_args = {\n",
    "    'lr': picked_lr,\n",
    "    'num_epochs': 75,\n",
    "    'info_freq': 5,\n",
    "    'lr_factor': 0.5,\n",
    "    'scheduler_patience': 10,\n",
    "    'log_dir': log_dir,  # Use the verified directory\n",
    "}\n",
    "\n",
    "print(f\"Starting training with LR: {picked_lr}\")\n",
    "\n",
    "# 4. Train the model\n",
    "gpu_multisurv.fit(**fit_args)\n",
    "\n",
    "# 5. Calculate and display elapsed time\n",
    "hrs, mins, secs = utils.elapsed_time(start_time)\n",
    "print(f\"Training completed in {hrs}h {mins}m {secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRE-TRAINING GPU VERIFICATION ===\n",
      "GPU model device: cuda:0\n",
      "GPU intervals device: cuda:0\n",
      "GPU device setting: cuda:0\n",
      "GPU memory before training: 0.25 GB\n",
      "Run tag: \"clinical_lr0.0005_breast_cancer_gpu_20250705_140211\"\n",
      "âœ… Log directory created: ./training_logs/clinical_lr0.0005_breast_cancer_gpu_20250705_140211\n",
      "âœ… Setup complete - ready for GPU training!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup with GPU verification\n",
    "import os\n",
    "import time\n",
    "import utils\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# 2. Verify GPU setup before training\n",
    "print(\"=== PRE-TRAINING GPU VERIFICATION ===\")\n",
    "print(f\"GPU model device: {next(gpu_multisurv.model.parameters()).device}\")\n",
    "print(f\"GPU intervals device: {gpu_multisurv.output_intervals.device}\")\n",
    "print(f\"GPU device setting: {gpu_multisurv.device}\")\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"GPU memory before training: {allocated:.2f} GB\")\n",
    "\n",
    "# 3. Set up training parameters\n",
    "picked_lr = 5e-4\n",
    "picked_num_epochs = 30\n",
    "picked_info_freq = 5\n",
    "picked_lr_factor = 0.5\n",
    "picked_scheduler_patience = 5\n",
    "picked_weight_decay = 5e-4\n",
    "\n",
    "# Add timestamp to make unique run tags\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "run_tag = utils.compose_run_tag(\n",
    "    model=gpu_multisurv, \n",
    "    lr=picked_lr,\n",
    "    dataloaders=gpu_multisurv.dataloaders,\n",
    "    log_dir='./training_logs/',\n",
    "    suffix=f'_breast_cancer_gpu_{timestamp}'  # Added 'gpu' to indicate GPU training\n",
    ")\n",
    "\n",
    "# 4. Create log directory\n",
    "log_dir = os.path.join('./training_logs/', run_tag)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "print(f\"âœ… Log directory created: {log_dir}\")\n",
    "\n",
    "print(\"âœ… Setup complete - ready for GPU training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting GPU training with LR: 0.0005\n",
      "ðŸ“Š Expected to see GPU utilization spike during training\n",
      "ðŸ“ Logs will be saved to: ./training_logs/clinical_lr0.0005_breast_cancer_gpu_20250705_140211\n",
      "\n",
      "=== GPU STATUS BEFORE TRAINING ===\n",
      "ðŸŽ¯ GPU Memory: 0.25 GB allocated, 0.36 GB reserved\n",
      "\n",
      "â° Training started at: 14:02:11\n",
      "Instantiating MultiSurv model...\n",
      "(loading pretrained unimodal model weights...)\n",
      "\n",
      "------------------------------------------\n",
      "             Training        Validation\n",
      "           ------------     ------------\n",
      " Epoch     Loss     Ctd     Loss     Ctd\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/multisurv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/30     0.0174   0.600   0.0153   0.617\n",
      " 5/30     0.0169   0.604   0.0148   0.429\n",
      " 10/30    0.0158   0.656   0.0143   0.535\n",
      " 15/30    0.0158   0.677   0.0143   0.582\n",
      " 20/30    0.0152   0.680   0.0136   0.600\n",
      " 25/30    0.0153   0.643   0.0137   0.634\n",
      " 30/30    0.0148   0.705   0.0139   0.608\n",
      "\n",
      ">>>>> Training completed in 1m 36s\n",
      ">>>>> Best validation C-indices:\n",
      "     0.6982968369829684 (epoch3)\n",
      "     0.6861313868613139 (epoch17)\n",
      "     0.6909975669099757 (epoch18)\n",
      "\n",
      "âœ… Training completed successfully!\n",
      "\n",
      "=== GPU STATUS AFTER TRAINING ===\n",
      "ðŸŽ¯ GPU Memory: 0.45 GB allocated, 0.60 GB reserved\n",
      "\n",
      "â° Training completed in 0h 1m 37s\n",
      "ðŸ Training finished at: 14:03:48\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Training execution with GPU monitoring\n",
    "fit_args = {\n",
    "    'lr': picked_lr,\n",
    "    'num_epochs': picked_num_epochs,\n",
    "    'info_freq': picked_info_freq,\n",
    "    'lr_factor': picked_lr_factor,\n",
    "    'scheduler_patience': picked_scheduler_patience,\n",
    "    'log_dir': log_dir,\n",
    "    'weight_decay':picked_weight_decay\n",
    "}\n",
    "\n",
    "print(f\"ðŸš€ Starting GPU training with LR: {picked_lr}\")\n",
    "print(f\"ðŸ“Š Expected to see GPU utilization spike during training\")\n",
    "print(f\"ðŸ“ Logs will be saved to: {log_dir}\")\n",
    "\n",
    "# Function to monitor GPU during training\n",
    "def check_gpu_status():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        print(f\"ðŸŽ¯ GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "    else:\n",
    "        print(\"âŒ CUDA not available\")\n",
    "\n",
    "\n",
    "# Check GPU before training\n",
    "print(\"\\n=== GPU STATUS BEFORE TRAINING ===\")\n",
    "check_gpu_status()\n",
    "\n",
    "# Clear any cached memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Start training\n",
    "print(f\"\\nâ° Training started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "try:\n",
    "    # This is where the actual training happens\n",
    "    gpu_multisurv.fit(**fit_args)\n",
    "    \n",
    "    print(f\"\\nâœ… Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Training failed with error: {e}\")\n",
    "    print(\"This might be due to GPU memory issues or device mismatches\")\n",
    "    # Print more debug info\n",
    "    print(f\"Model device: {next(gpu_multisurv.model.parameters()).device}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Check GPU after training\n",
    "print(\"\\n=== GPU STATUS AFTER TRAINING ===\")\n",
    "check_gpu_status()\n",
    "\n",
    "# Calculate and display elapsed time\n",
    "hrs, mins, secs = utils.elapsed_time(start_time)\n",
    "print(f\"\\nâ° Training completed in {hrs}h {mins}m {secs}s\")\n",
    "print(f\"ðŸ Training finished at: {datetime.now().strftime('%H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "from collections import deque\n",
    "import utils\n",
    "\n",
    "class AutoTrainer:\n",
    "    \"\"\"Automated training system with intelligent decision making.\"\"\"\n",
    "    \n",
    "    def __init__(self, gpu_model, device, models_dir='./models/', log_base_dir='./training_logs/'):\n",
    "        self.model = gpu_model\n",
    "        self.device = device\n",
    "        self.models_dir = models_dir\n",
    "        self.log_base_dir = log_base_dir\n",
    "        \n",
    "        # Training parameters\n",
    "        self.base_lr = 1e-5\n",
    "        self.num_epochs = 60\n",
    "        self.info_freq = 5\n",
    "        self.lr_factor = 0.5\n",
    "        self.scheduler_patience = 5\n",
    "        self.weight_decay = 1e-4\n",
    "        \n",
    "        # Decision thresholds\n",
    "        self.overfitting_gap_warning = 0.15\n",
    "        self.overfitting_gap_critical = 0.25\n",
    "        self.overfitting_loss_ratio = 2.0\n",
    "        self.convergence_variance = 0.02\n",
    "        self.convergence_loss_variance = 0.001\n",
    "        self.min_acceptable_performance = 0.55\n",
    "        self.smoothing_window = 3\n",
    "        \n",
    "        # Training phases\n",
    "        self.exploration_epochs = 20\n",
    "        self.refinement_epochs = 40\n",
    "        \n",
    "        # Tracking\n",
    "        self.checkpoint_candidates = {}  # epoch -> metrics\n",
    "        self.training_history = {\n",
    "            'train_loss': [], 'train_concord': [],\n",
    "            'val_loss': [], 'val_concord': [],\n",
    "            'smoothed_val_concord': []\n",
    "        }\n",
    "        self.lr_reductions = 0\n",
    "        self.overfitting_warnings = 0\n",
    "        \n",
    "    def train(self, dataloaders):\n",
    "        \"\"\"Main training orchestration with automated decisions.\"\"\"\n",
    "        print(\"ðŸš€ Starting Automated Training Process\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Setup training\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        run_tag = utils.compose_run_tag(\n",
    "            model=self.model, \n",
    "            lr=self.base_lr,\n",
    "            dataloaders=dataloaders,\n",
    "            log_dir=self.log_base_dir,\n",
    "            suffix=f'_auto_train_{timestamp}'\n",
    "        )\n",
    "        log_dir = os.path.join(self.log_base_dir, run_tag)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "        # Initial setup\n",
    "        current_lr = self.base_lr\n",
    "        patience_counter = 0\n",
    "        best_smoothed_concord = 0\n",
    "        convergence_history = deque(maxlen=10)\n",
    "        \n",
    "        print(f\"ðŸ“ Logs: {log_dir}\")\n",
    "        print(f\"ðŸŽ¯ Initial LR: {current_lr}\")\n",
    "        \n",
    "        # Training loop with checkpoints\n",
    "        epoch = 0\n",
    "        while epoch < self.num_epochs:\n",
    "            # Determine training phase\n",
    "            phase = self._get_training_phase(epoch)\n",
    "            \n",
    "            # Train for a batch of epochs\n",
    "            batch_epochs = min(self.info_freq, self.num_epochs - epoch)\n",
    "            \n",
    "            fit_args = {\n",
    "                'lr': current_lr,\n",
    "                'num_epochs': batch_epochs,\n",
    "                'info_freq': self.info_freq,\n",
    "                'lr_factor': self.lr_factor,\n",
    "                'scheduler_patience': self.scheduler_patience,\n",
    "                'log_dir': log_dir,\n",
    "                'weight_decay': self.weight_decay\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                print(f\"\\nðŸ“Š Training epochs {epoch+1}-{epoch+batch_epochs} (Phase: {phase})\")\n",
    "                self.model.fit(**fit_args)\n",
    "                \n",
    "                # Extract metrics from tensorboard logs\n",
    "                metrics = self._extract_metrics_from_logs(log_dir, up_to_epoch=epoch+batch_epochs)\n",
    "                \n",
    "                # Update tracking\n",
    "                self._update_tracking(metrics)\n",
    "                \n",
    "                # Get current performance\n",
    "                current_metrics = self._get_current_metrics(epoch+batch_epochs)\n",
    "                smoothed_concord = current_metrics['smoothed_val_concord']\n",
    "                \n",
    "                print(f\"\\nðŸ“ˆ Epoch {epoch+batch_epochs} Summary:\")\n",
    "                print(f\"   Val Concord: {current_metrics['val_concord']:.4f} (smoothed: {smoothed_concord:.4f})\")\n",
    "                print(f\"   Generalization gap: {current_metrics['gap']:.4f}\")\n",
    "                print(f\"   Loss ratio: {current_metrics['loss_ratio']:.2f}\")\n",
    "                \n",
    "                # Make decisions\n",
    "                decision = self._make_decision(\n",
    "                    current_metrics, phase, patience_counter, \n",
    "                    best_smoothed_concord, convergence_history\n",
    "                )\n",
    "                \n",
    "                # Execute decision\n",
    "                if decision['action'] == 'stop':\n",
    "                    print(f\"\\nðŸ›‘ Stopping: {decision['reason']}\")\n",
    "                    break\n",
    "                    \n",
    "                elif decision['action'] == 'reduce_lr':\n",
    "                    current_lr *= self.lr_factor\n",
    "                    self.lr_reductions += 1\n",
    "                    patience_counter = 0\n",
    "                    print(f\"\\nðŸ“‰ Reducing LR to {current_lr:.6f}\")\n",
    "                    \n",
    "                elif decision['action'] == 'continue':\n",
    "                    if smoothed_concord > best_smoothed_concord:\n",
    "                        best_smoothed_concord = smoothed_concord\n",
    "                        patience_counter = 0\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                \n",
    "                # Track checkpoint candidates\n",
    "                if self._is_good_checkpoint(current_metrics, phase):\n",
    "                    self._save_checkpoint_candidate(epoch+batch_epochs, current_metrics)\n",
    "                \n",
    "                # Update convergence history\n",
    "                convergence_history.append(smoothed_concord)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nâŒ Training error: {e}\")\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(\"ðŸ’¡ Consider reducing batch size\")\n",
    "                break\n",
    "            \n",
    "            epoch += batch_epochs\n",
    "        \n",
    "        # Final checkpoint selection and evaluation\n",
    "        print(\"\\nðŸŽ¯ Selecting best checkpoint...\")\n",
    "        best_checkpoint = self._select_best_checkpoint(dataloaders)\n",
    "        \n",
    "        # Final evaluation\n",
    "        print(\"\\nðŸ“Š Final Evaluation:\")\n",
    "        final_results = self._final_evaluation(best_checkpoint, dataloaders)\n",
    "        \n",
    "        # Training summary\n",
    "        hrs, mins, secs = utils.elapsed_time(start_time)\n",
    "        print(f\"\\nâœ… Training completed in {hrs}h {mins}m {secs}s\")\n",
    "        print(f\"ðŸ† Best checkpoint: Epoch {best_checkpoint['epoch']}\")\n",
    "        print(f\"   Val Ctd: {final_results['val_ctd']:.4f}\")\n",
    "        print(f\"   Test Ctd: {final_results['test_ctd']:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'best_checkpoint': best_checkpoint,\n",
    "            'final_results': final_results,\n",
    "            'run_tag': run_tag,\n",
    "            'training_history': self.training_history\n",
    "        }\n",
    "    \n",
    "    def _get_training_phase(self, epoch):\n",
    "        \"\"\"Determine current training phase.\"\"\"\n",
    "        if epoch < self.exploration_epochs:\n",
    "            return 'exploration'\n",
    "        elif epoch < self.refinement_epochs:\n",
    "            return 'refinement'\n",
    "        else:\n",
    "            return 'fine-tuning'\n",
    "    \n",
    "    def _extract_metrics_from_logs(self, log_dir, up_to_epoch):\n",
    "        \"\"\"Extract metrics from tensorboard logs.\"\"\"\n",
    "        event_files = glob.glob(os.path.join(log_dir, \"events.out.tfevents.*\"))\n",
    "        if not event_files:\n",
    "            return None\n",
    "            \n",
    "        event_file = sorted(event_files, key=os.path.getmtime)[-1]\n",
    "        ea = EventAccumulator(event_file, size_guidance={\"scalars\": 0})\n",
    "        ea.Reload()\n",
    "        \n",
    "        metrics = {}\n",
    "        for tag in ['train_loss', 'train_concord', 'val_loss', 'val_concord']:\n",
    "            data = [(e.step, e.value) for e in ea.Scalars(tag) if e.step <= up_to_epoch]\n",
    "            metrics[tag] = data\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def _update_tracking(self, metrics):\n",
    "        \"\"\"Update training history tracking.\"\"\"\n",
    "        if not metrics:\n",
    "            return\n",
    "            \n",
    "        for key in self.training_history:\n",
    "            if key != 'smoothed_val_concord' and key in metrics:\n",
    "                # Add new values\n",
    "                for step, value in metrics[key]:\n",
    "                    if step >= len(self.training_history[key]):\n",
    "                        self.training_history[key].append(value)\n",
    "        \n",
    "        # Update smoothed values\n",
    "        if len(self.training_history['val_concord']) >= self.smoothing_window:\n",
    "            vals = np.array(self.training_history['val_concord'])\n",
    "            smoothed = np.convolve(vals, np.ones(self.smoothing_window)/self.smoothing_window, mode='valid')\n",
    "            self.training_history['smoothed_val_concord'] = smoothed.tolist()\n",
    "    \n",
    "    def _get_current_metrics(self, epoch):\n",
    "        \"\"\"Get current training metrics.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Get latest values\n",
    "        if self.training_history['train_concord']:\n",
    "            metrics['train_concord'] = self.training_history['train_concord'][-1]\n",
    "        else:\n",
    "            metrics['train_concord'] = 0.5\n",
    "            \n",
    "        if self.training_history['val_concord']:\n",
    "            metrics['val_concord'] = self.training_history['val_concord'][-1]\n",
    "        else:\n",
    "            metrics['val_concord'] = 0.5\n",
    "            \n",
    "        if self.training_history['train_loss']:\n",
    "            metrics['train_loss'] = self.training_history['train_loss'][-1]\n",
    "        else:\n",
    "            metrics['train_loss'] = 1.0\n",
    "            \n",
    "        if self.training_history['val_loss']:\n",
    "            metrics['val_loss'] = self.training_history['val_loss'][-1]\n",
    "        else:\n",
    "            metrics['val_loss'] = 1.0\n",
    "        \n",
    "        # Smoothed concordance\n",
    "        if self.training_history['smoothed_val_concord']:\n",
    "            metrics['smoothed_val_concord'] = self.training_history['smoothed_val_concord'][-1]\n",
    "        else:\n",
    "            metrics['smoothed_val_concord'] = metrics['val_concord']\n",
    "        \n",
    "        # Derived metrics\n",
    "        metrics['gap'] = metrics['train_concord'] - metrics['val_concord']\n",
    "        metrics['loss_ratio'] = metrics['val_loss'] / max(metrics['train_loss'], 1e-6)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _make_decision(self, metrics, phase, patience_counter, best_smoothed, convergence_history):\n",
    "        \"\"\"Make training decision based on current metrics and phase.\"\"\"\n",
    "        decision = {'action': 'continue', 'reason': ''}\n",
    "        \n",
    "        # Check for convergence\n",
    "        if len(convergence_history) >= 10:\n",
    "            variance = np.var(list(convergence_history))\n",
    "            if variance < self.convergence_variance and self.lr_reductions > 0:\n",
    "                decision = {'action': 'stop', 'reason': 'Model converged'}\n",
    "                return decision\n",
    "        \n",
    "        # Check for severe overfitting\n",
    "        if metrics['gap'] > self.overfitting_gap_critical:\n",
    "            decision = {'action': 'stop', 'reason': f'Severe overfitting (gap={metrics[\"gap\"]:.3f})'}\n",
    "            return decision\n",
    "        \n",
    "        # Check for poor performance\n",
    "        if phase != 'exploration' and metrics['smoothed_val_concord'] < self.min_acceptable_performance:\n",
    "            decision = {'action': 'stop', 'reason': f'Performance below threshold ({metrics[\"smoothed_val_concord\"]:.3f})'}\n",
    "            return decision\n",
    "        \n",
    "        # Phase-specific decisions\n",
    "        if phase == 'exploration':\n",
    "            # Be patient in exploration\n",
    "            if patience_counter > 10:\n",
    "                decision = {'action': 'reduce_lr', 'reason': 'No improvement in exploration'}\n",
    "                \n",
    "        elif phase == 'refinement':\n",
    "            # More aggressive in refinement\n",
    "            if patience_counter > 5:\n",
    "                decision = {'action': 'reduce_lr', 'reason': 'No improvement in refinement'}\n",
    "            elif metrics['gap'] > self.overfitting_gap_warning:\n",
    "                self.overfitting_warnings += 1\n",
    "                if self.overfitting_warnings > 3:\n",
    "                    decision = {'action': 'reduce_lr', 'reason': 'Repeated overfitting warnings'}\n",
    "                    \n",
    "        else:  # fine-tuning\n",
    "            # Very aggressive in fine-tuning\n",
    "            if patience_counter > 3:\n",
    "                decision = {'action': 'stop', 'reason': 'No improvement in fine-tuning'}\n",
    "            elif metrics['loss_ratio'] > self.overfitting_loss_ratio:\n",
    "                decision = {'action': 'stop', 'reason': f'High loss ratio ({metrics[\"loss_ratio\"]:.2f})'}\n",
    "        \n",
    "        return decision\n",
    "    \n",
    "    def _is_good_checkpoint(self, metrics, phase):\n",
    "        \"\"\"Determine if current epoch is a good checkpoint candidate.\"\"\"\n",
    "        # Always save in exploration\n",
    "        if phase == 'exploration':\n",
    "            return True\n",
    "            \n",
    "        # Save if good performance and not overfitting\n",
    "        return (metrics['smoothed_val_concord'] > 0.6 and \n",
    "                metrics['gap'] < self.overfitting_gap_warning and\n",
    "                metrics['loss_ratio'] < 1.5)\n",
    "    \n",
    "    def _save_checkpoint_candidate(self, epoch, metrics):\n",
    "        \"\"\"Save checkpoint candidate information.\"\"\"\n",
    "        # Get actual model checkpoints\n",
    "        if hasattr(self.model, 'best_model_weights'):\n",
    "            available_epochs = list(self.model.best_model_weights.keys())\n",
    "            \n",
    "            # Find closest available epoch\n",
    "            closest_epoch = None\n",
    "            min_diff = float('inf')\n",
    "            for avail_epoch in available_epochs:\n",
    "                epoch_num = int(avail_epoch.replace('epoch', ''))\n",
    "                diff = abs(epoch_num - epoch)\n",
    "                if diff < min_diff:\n",
    "                    min_diff = diff\n",
    "                    closest_epoch = avail_epoch\n",
    "            \n",
    "            if closest_epoch and min_diff <= 2:  # Within 2 epochs\n",
    "                self.checkpoint_candidates[closest_epoch] = metrics.copy()\n",
    "    \n",
    "    def _select_best_checkpoint(self, dataloaders):\n",
    "        \"\"\"Select best checkpoint by evaluating candidates on full validation set.\"\"\"\n",
    "        print(\"\\nðŸ” Evaluating checkpoint candidates...\")\n",
    "        \n",
    "        best_checkpoint = None\n",
    "        best_score = 0\n",
    "        \n",
    "        # Get available checkpoints\n",
    "        available_checkpoints = []\n",
    "        if hasattr(self.model, 'best_model_weights'):\n",
    "            available_checkpoints.extend(list(self.model.best_model_weights.keys()))\n",
    "        if hasattr(self.model, 'current_concord'):\n",
    "            available_checkpoints.append('current')\n",
    "        \n",
    "        for checkpoint_epoch in available_checkpoints:\n",
    "            # Load checkpoint\n",
    "            try:\n",
    "                # Save current state\n",
    "                current_state = self.model.model.state_dict().copy()\n",
    "                \n",
    "                # Load checkpoint weights\n",
    "                if checkpoint_epoch == 'current':\n",
    "                    # Current weights are already loaded\n",
    "                    pass\n",
    "                else:\n",
    "                    # Load from best_model_weights\n",
    "                    checkpoint_state = self.model.best_model_weights[checkpoint_epoch]\n",
    "                    self.model.model.load_state_dict(checkpoint_state)\n",
    "                \n",
    "                # Evaluate on full validation set\n",
    "                performance = utils.Evaluation(\n",
    "                    model=self.model,\n",
    "                    dataset=dataloaders['val'].dataset,\n",
    "                    device=self.device\n",
    "                )\n",
    "                performance.compute_metrics()\n",
    "                \n",
    "                # Get metrics\n",
    "                val_ctd = performance.c_index_td if performance.c_index_td else 0\n",
    "                val_ibs = performance.ibs if performance.ibs else 1.0\n",
    "                \n",
    "                # Composite score (prioritize Ctd, penalize high IBS)\n",
    "                score = val_ctd - 0.1 * val_ibs\n",
    "                \n",
    "                print(f\"  {checkpoint_epoch}: Ctd={val_ctd:.4f}, IBS={val_ibs:.4f}, Score={score:.4f}\")\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_checkpoint = {\n",
    "                        'epoch': checkpoint_epoch,\n",
    "                        'val_ctd': val_ctd,\n",
    "                        'val_ibs': val_ibs,\n",
    "                        'score': score\n",
    "                    }\n",
    "                \n",
    "                # Restore original state\n",
    "                self.model.model.load_state_dict(current_state)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error evaluating {checkpoint_epoch}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Load best checkpoint\n",
    "        if best_checkpoint:\n",
    "            if best_checkpoint['epoch'] == 'current':\n",
    "                # Already using current weights\n",
    "                pass\n",
    "            else:\n",
    "                checkpoint_state = self.model.best_model_weights[best_checkpoint['epoch']]\n",
    "                self.model.model.load_state_dict(checkpoint_state)\n",
    "            print(f\"\\nâœ… Selected {best_checkpoint['epoch']} as best checkpoint\")\n",
    "        \n",
    "        return best_checkpoint\n",
    "    \n",
    "    def _final_evaluation(self, checkpoint, dataloaders):\n",
    "        \"\"\"Perform final evaluation on val and test sets.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Validation set evaluation\n",
    "        print(\"\\nðŸ“Š Validation Set Performance:\")\n",
    "        val_performance = utils.Evaluation(\n",
    "            model=self.model,\n",
    "            dataset=dataloaders['val'].dataset,\n",
    "            device=self.device\n",
    "        )\n",
    "        val_performance.compute_metrics()\n",
    "        val_performance.show_results()\n",
    "        \n",
    "        results['val_ctd'] = val_performance.c_index_td if val_performance.c_index_td else 0\n",
    "        results['val_ibs'] = val_performance.ibs if val_performance.ibs else 1.0\n",
    "        \n",
    "        # Test set evaluation\n",
    "        print(\"\\nðŸ“Š Test Set Performance:\")\n",
    "        test_performance = utils.Evaluation(\n",
    "            model=self.model,\n",
    "            dataset=dataloaders['test'].dataset,\n",
    "            device=self.device\n",
    "        )\n",
    "        test_performance.compute_metrics()\n",
    "        test_performance.show_results()\n",
    "        \n",
    "        results['test_ctd'] = test_performance.c_index_td if test_performance.c_index_td else 0\n",
    "        results['test_ibs'] = test_performance.ibs if test_performance.ibs else 1.0\n",
    "        \n",
    "        # Save best weights\n",
    "        if checkpoint:\n",
    "            save_name = f\"best_{checkpoint['epoch']}\"\n",
    "            self.model.save_weights(\n",
    "                saved_epoch=checkpoint['epoch'], \n",
    "                prefix=f\"auto_trained_{save_name}\", \n",
    "                weight_dir=self.models_dir\n",
    "            )\n",
    "            print(f\"\\nðŸ’¾ Saved best weights as: auto_trained_{save_name}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "def run_automated_training(gpu_multisurv, dataloaders, device, models_dir='./models/'):\n",
    "    \"\"\"Run automated training with intelligent decision making.\"\"\"\n",
    "    \n",
    "    # Initialize auto trainer\n",
    "    trainer = AutoTrainer(\n",
    "        gpu_model=gpu_multisurv,\n",
    "        device=device,\n",
    "        models_dir=MODELS\n",
    "    )\n",
    "    \n",
    "    # You can customize parameters if needed\n",
    "    # trainer.base_lr = 5e-4\n",
    "    # trainer.num_epochs = 80\n",
    "    # trainer.min_acceptable_performance = 0.60\n",
    "    \n",
    "    # Run training\n",
    "    results = trainer.train(dataloaders)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# To use:\n",
    "# results = run_automated_training(gpu_multisurv, dataloaders, device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "results = run_automated_training(gpu_multisurv, dataloaders, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model weights\n",
    "\n",
    "If desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using event file: /app/training_logs/clinical_mRNA_lr0.001_breast_cancer_gpu_20250705_134207/events.out.tfevents.1751722932.a9fecd5a8321.63207.1\n",
      "Available scalar tags: ['train_loss', 'train_concord', 'val_loss', 'val_concord']\n",
      "Best smoothed val_concord = 0.6715 at epoch 38\n",
      "\n",
      " epoch â”‚ raw val_concord â”‚ smoothed\n",
      "â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   3   â”‚      0.4440     â”‚  0.5112\n",
      "   4   â”‚      0.5694     â”‚  0.5231\n",
      "   5   â”‚      0.4923     â”‚  0.5019\n",
      "   6   â”‚      0.4866     â”‚  0.5161\n",
      "   7   â”‚      0.5977     â”‚  0.5255\n",
      "   8   â”‚      0.6767     â”‚  0.5870\n",
      "   9   â”‚      0.3755     â”‚  0.5500\n",
      "  10   â”‚      0.4325     â”‚  0.4949\n",
      "  11   â”‚      0.4885     â”‚  0.4322\n",
      "  12   â”‚      0.3841     â”‚  0.4350\n",
      "  13   â”‚      0.4933     â”‚  0.4553\n",
      "  14   â”‚      0.4349     â”‚  0.4374\n",
      "  15   â”‚      0.5211     â”‚  0.4831\n",
      "  16   â”‚      0.6494     â”‚  0.5351\n",
      "  17   â”‚      0.5081     â”‚  0.5595\n",
      "  18   â”‚      0.5694     â”‚  0.5757\n",
      "  19   â”‚      0.5738     â”‚  0.5504\n",
      "  20   â”‚      0.6034     â”‚  0.5822\n",
      "  21   â”‚      0.6456     â”‚  0.6076\n",
      "  22   â”‚      0.6782     â”‚  0.6424\n",
      "  23   â”‚      0.6743     â”‚  0.6660\n",
      "  24   â”‚      0.6341     â”‚  0.6622\n",
      "  25   â”‚      0.6437     â”‚  0.6507\n",
      "  26   â”‚      0.6772     â”‚  0.6517\n",
      "  27   â”‚      0.6226     â”‚  0.6478\n",
      "  28   â”‚      0.5910     â”‚  0.6303\n",
      "  29   â”‚      0.6552     â”‚  0.6229\n",
      "  30   â”‚      0.5958     â”‚  0.6140\n",
      "  31   â”‚      0.6293     â”‚  0.6268\n",
      "  32   â”‚      0.6772     â”‚  0.6341\n",
      "  33   â”‚      0.6619     â”‚  0.6561\n",
      "  34   â”‚      0.6293     â”‚  0.6561\n",
      "  35   â”‚      0.6264     â”‚  0.6392\n",
      "  36   â”‚      0.6628     â”‚  0.6395\n",
      "  37   â”‚      0.6935     â”‚  0.6609\n",
      "  38   â”‚      0.6580     â”‚  0.6715\n",
      "  39   â”‚      0.6590     â”‚  0.6702\n",
      "  40   â”‚      0.5910     â”‚  0.6360\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# 1) Path to your events file:\n",
    "events_path = os.path.join('/app', 'training_logs/' ,run_tag)\n",
    "event_files = glob.glob(os.path.join(events_path, \"events.out.tfevents.*\"))\n",
    "if not event_files:\n",
    "    raise FileNotFoundError(f\"No TensorBoard event files found in {events_path}\")\n",
    "event_file = sorted(event_files, key=os.path.getmtime)[-1]  # use the newest\n",
    "print(f\"Using event file: {event_file}\")\n",
    "\n",
    "# 2) Load all scalar data\n",
    "ea = EventAccumulator(event_file, size_guidance={\"scalars\": 0})\n",
    "ea.Reload()\n",
    "\n",
    "# 3) Check available scalar tags\n",
    "print(\"Available scalar tags:\", ea.Tags()[\"scalars\"])\n",
    "# => ['train_loss', 'train_concord', 'val_loss', 'val_concord']\n",
    "\n",
    "# 4) Extract (epoch, value) pairs for val_concord\n",
    "val_concord = [(e.step, e.value) for e in ea.Scalars(\"val_concord\")]\n",
    "\n",
    "# 5) Separate epochs and values\n",
    "epochs = [step for step, _ in val_concord]\n",
    "vals   = np.array([v for _, v in val_concord])\n",
    "\n",
    "# 6) Compute 3-epoch moving average\n",
    "window = 3\n",
    "# 'valid' mode produces len(vals) - window + 1 points\n",
    "smoothed = np.convolve(vals, np.ones(window)/window, mode=\"valid\")\n",
    "smoothed_epochs = epochs[window - 1 :]  # first smoothed point corresponds to epoch=window\n",
    "\n",
    "# 7) Find epoch with highest smoothed val_concord\n",
    "best_idx = np.argmax(smoothed)\n",
    "best_epoch = smoothed_epochs[best_idx]\n",
    "best_smoothed_concord = smoothed[best_idx]\n",
    "print(f\"Best smoothed val_concord = {best_smoothed_concord:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "# 8) (Optional) print raw vs smoothed for inspection\n",
    "print(\"\\n epoch â”‚ raw val_concord â”‚ smoothed\")\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "for ep, raw, sm in zip(smoothed_epochs, vals[window - 1 :], smoothed):\n",
    "    print(f\"  {ep:>2d}   â”‚      {raw:.4f}     â”‚  {sm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch26', 'epoch32', 'epoch37'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_multisurv.best_model_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch26': 0.6772030651340997,\n",
       " 'epoch32': 0.6772030651340997,\n",
       " 'epoch37': 0.6934865900383141}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_multisurv.best_concord_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch40': 0.5909961685823755}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_multisurv.current_concord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model weights to file:\n",
      "    /mnt/data/multisurv_models/clinical_mRNA_lr0.001_breast_cancer_gpu_20250705_134207_epoch37_concord0.69.pth\n"
     ]
    }
   ],
   "source": [
    "gpu_multisurv.save_weights(saved_epoch='epoch37', prefix=run_tag, weight_dir=MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data modalities:\n",
      "   clinical\n",
      "\n",
      "Dataset sizes (# patients):\n",
      "   train: 710\n",
      "   val: 165\n",
      "   test: 219\n",
      "\n",
      "Batch size: 128\n"
     ]
    }
   ],
   "source": [
    "dataloaders = utils.get_dataloaders(data_location=DATA,\n",
    "                                    labels_file='/app/data/labels.tsv',\n",
    "                                    modalities=data_modalities.value,\n",
    "                                    wsi_patch_size=299,\n",
    "                                    n_wsi_patches=5,\n",
    "#                                     exclude_patients=exclude_cancers,\n",
    "                                    return_patient_id=True,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Applied fix to _BaseEvaluation._predictions_to_pycox\n",
      "âœ… Applied fix to Evaluation._predictions_to_pycox\n",
      "ðŸ’¡ Note: There's a syntax warning in evaluation.py line 187\n",
      "   Change 'self.type is not 'MultiSurv'' to 'self.type != 'MultiSurv''\n",
      "   This doesn't affect functionality but should be fixed in the source code\n",
      "\n",
      "ðŸ”§ Evaluation fixes applied! Now try running your evaluation again.\n"
     ]
    }
   ],
   "source": [
    "# Fix for evaluation.py _predictions_to_pycox method\n",
    "\n",
    "def fixed_evaluation_predictions_to_pycox(self, data, time_points=None):\n",
    "    \"\"\"Fixed evaluation version that handles the correct dimensions.\"\"\"\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    \n",
    "    # Extract predictions from data\n",
    "    # data should be a list of tuples: (predictions, times, events, patient_ids)\n",
    "    predictions_list = []\n",
    "    for item in data:\n",
    "        pred = item[0]  # The prediction tensor\n",
    "        if torch.is_tensor(pred):\n",
    "            pred = pred.detach().cpu().numpy()\n",
    "        predictions_list.append(pred)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    predictions = np.array(predictions_list)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(predictions)\n",
    "    \n",
    "    if time_points is None:\n",
    "        # Get the number of intervals from the model output\n",
    "        n_intervals = predictions.shape[1]  # Should be 18 for your model\n",
    "        \n",
    "        # Use the model's actual output intervals if available\n",
    "        if hasattr(self, 'model') and hasattr(self.model, 'output_intervals') and self.model.output_intervals is not None:\n",
    "            # Use the midpoints of the output intervals\n",
    "            intervals = self.model.output_intervals\n",
    "            if torch.is_tensor(intervals):\n",
    "                intervals = intervals.detach().cpu().numpy()\n",
    "            \n",
    "            if len(intervals) > n_intervals:\n",
    "                # Take the first n_intervals midpoints\n",
    "                midpoints = (intervals[:-1] + intervals[1:]) / 2\n",
    "                time_points = midpoints[:n_intervals]\n",
    "            else:\n",
    "                # Fallback to evenly spaced points\n",
    "                last_interval = intervals[-1] if not torch.is_tensor(intervals) else intervals[-1].item()\n",
    "                time_points = np.linspace(0.5, last_interval / 365, n_intervals)\n",
    "        else:\n",
    "            # Fallback: create time points that match the output size\n",
    "            time_points = np.arange(0.5, 0.5 + n_intervals, 1.0)\n",
    "    \n",
    "    # Ensure time_points matches the prediction dimensions\n",
    "    if len(time_points) != predictions.shape[1]:\n",
    "        print(f\"Warning: Adjusting time_points from {len(time_points)} to {predictions.shape[1]}\")\n",
    "        first_point = time_points[0] if len(time_points) > 0 else 0.5\n",
    "        last_point = time_points[-1] if len(time_points) > 0 else n_intervals + 0.5\n",
    "        time_points = np.linspace(first_point, last_point, predictions.shape[1])\n",
    "    \n",
    "    # FIXED: The DataFrame structure should be transposed\n",
    "    # We want columns to be time points, rows to be patients\n",
    "    df = df.T  # Transpose so shape becomes [num_intervals, num_patients]\n",
    "    \n",
    "    # Convert time_points to numpy if it's a tensor\n",
    "    if torch.is_tensor(time_points):\n",
    "        time_points = time_points.detach().cpu().numpy()\n",
    "    \n",
    "    # Set the index to time points\n",
    "    df.index = time_points\n",
    "    df.index.name = 'time'\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the fix to the evaluation module\n",
    "import evaluation\n",
    "\n",
    "# Check if the _BaseEvaluation class exists and patch it\n",
    "if hasattr(evaluation, '_BaseEvaluation'):\n",
    "    evaluation._BaseEvaluation._predictions_to_pycox = fixed_evaluation_predictions_to_pycox\n",
    "    print(\"âœ… Applied fix to _BaseEvaluation._predictions_to_pycox\")\n",
    "else:\n",
    "    print(\"âŒ _BaseEvaluation class not found\")\n",
    "\n",
    "# Also check for Evaluation class\n",
    "if hasattr(evaluation, 'Evaluation'):\n",
    "    evaluation.Evaluation._predictions_to_pycox = fixed_evaluation_predictions_to_pycox\n",
    "    print(\"âœ… Applied fix to Evaluation._predictions_to_pycox\")\n",
    "\n",
    "# Fix the syntax warning too\n",
    "def patch_syntax_warning():\n",
    "    \"\"\"Fix the syntax warning in evaluation.py if possible.\"\"\"\n",
    "    print(\"ðŸ’¡ Note: There's a syntax warning in evaluation.py line 187\")\n",
    "    print(\"   Change 'self.type is not 'MultiSurv'' to 'self.type != 'MultiSurv''\")\n",
    "    print(\"   This doesn't affect functionality but should be fixed in the source code\")\n",
    "\n",
    "patch_syntax_warning()\n",
    "\n",
    "print(\"\\nðŸ”§ Evaluation fixes applied! Now try running your evaluation again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Applied clinical data shape fix to Evaluation._get_patient_predictions\n"
     ]
    }
   ],
   "source": [
    "# Fix the data shape issue in get_patient_predictions\n",
    "def fixed_get_patient_predictions(self, idx):\n",
    "    \"\"\"Fixed version that handles 1D continuous tensor correctly.\"\"\"\n",
    "    patient_data = self.dataset[idx]\n",
    "    data_dict, time, event = patient_data\n",
    "    \n",
    "    # Create batch data\n",
    "    batch_data = {}\n",
    "    for key, value in data_dict.items():\n",
    "        if key == 'clinical' and isinstance(value, tuple):\n",
    "            cat, cont = value\n",
    "            # Fix: ensure continuous is 2D [1, n_features]\n",
    "            if cont.dim() == 1:\n",
    "                cont = cont.unsqueeze(0)\n",
    "            batch_data[key] = (cat.unsqueeze(0).to(self.device), \n",
    "                              cont.unsqueeze(0).to(self.device))\n",
    "        else:\n",
    "            batch_data[key] = value.unsqueeze(0).to(self.device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        _, risk = self.model.model(batch_data)\n",
    "    \n",
    "    return risk.squeeze(0).cpu()\n",
    "\n",
    "# Apply the fix\n",
    "import evaluation\n",
    "evaluation.Evaluation._get_patient_predictions = fixed_get_patient_predictions\n",
    "print(\"âœ… Applied clinical data shape fix to Evaluation._get_patient_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Applied final fix for nested dictionary structure\n"
     ]
    }
   ],
   "source": [
    "# Final fix for _predictions_to_pycox\n",
    "def fixed_evaluation_predictions_to_pycox_final(self, data, time_points=None):\n",
    "    \"\"\"Fixed version that handles the nested dictionary structure.\"\"\"\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    \n",
    "    # Extract predictions from the nested dictionary structure\n",
    "    predictions_list = []\n",
    "    times_list = []\n",
    "    events_list = []\n",
    "    patient_ids = []\n",
    "    \n",
    "    for patient_id, patient_data in data.items():\n",
    "        if isinstance(patient_data, dict):\n",
    "            # Extract probabilities from the dictionary\n",
    "            prob = patient_data['probabilities']\n",
    "            if torch.is_tensor(prob):\n",
    "                prob = prob.detach().cpu().numpy()\n",
    "            predictions_list.append(prob)\n",
    "            \n",
    "            # Also collect times and events for verification\n",
    "            times_list.append(patient_data['time'])\n",
    "            events_list.append(patient_data['event'])\n",
    "            patient_ids.append(patient_id)\n",
    "        else:\n",
    "            # Fallback for other formats\n",
    "            if torch.is_tensor(patient_data):\n",
    "                predictions_list.append(patient_data.detach().cpu().numpy())\n",
    "            else:\n",
    "                predictions_list.append(patient_data[0])\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    predictions = np.array(predictions_list)\n",
    "    print(f\"Extracted predictions shape: {predictions.shape}\")\n",
    "    \n",
    "    # Create DataFrame with patients as rows, time intervals as columns\n",
    "    df = pd.DataFrame(predictions)\n",
    "    \n",
    "    if time_points is None:\n",
    "        n_intervals = predictions.shape[1]\n",
    "        \n",
    "        # Use the model's actual output intervals if available\n",
    "        if hasattr(self, 'model') and hasattr(self.model, 'output_intervals') and self.model.output_intervals is not None:\n",
    "            intervals = self.model.output_intervals\n",
    "            if torch.is_tensor(intervals):\n",
    "                intervals = intervals.detach().cpu().numpy()\n",
    "            \n",
    "            # Create midpoints for time intervals\n",
    "            if len(intervals) == n_intervals + 1:\n",
    "                # intervals are boundaries, we need midpoints\n",
    "                time_points = (intervals[:-1] + intervals[1:]) / 2\n",
    "            else:\n",
    "                # Fallback\n",
    "                time_points = np.linspace(0.5, 20, n_intervals)\n",
    "        else:\n",
    "            # Fallback: create evenly spaced time points\n",
    "            time_points = np.arange(0.5, 0.5 + n_intervals, 1.0)\n",
    "    \n",
    "    # Ensure time_points matches prediction dimensions\n",
    "    if len(time_points) != predictions.shape[1]:\n",
    "        time_points = np.linspace(0.5, 20, predictions.shape[1])\n",
    "    \n",
    "    # Transpose so time is the index (required by pycox)\n",
    "    df = df.T\n",
    "    df.index = time_points\n",
    "    df.index.name = 'time'\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the final fix\n",
    "import evaluation\n",
    "evaluation.Evaluation._predictions_to_pycox = fixed_evaluation_predictions_to_pycox_final\n",
    "print(\"âœ… Applied final fix for nested dictionary structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect patient predictions: 165/165\n",
      "\n",
      "Extracted predictions shape: (165, 17)\n",
      "C-index   0.764\n",
      "Ctd       0.764\n",
      "IBS       0.237\n",
      "INBLL     1.235\n"
     ]
    }
   ],
   "source": [
    "performance = utils.Evaluation(\n",
    "    model=gpu_multisurv, dataset=dataloaders['val'].dataset,\n",
    "    device=device)\n",
    "performance.compute_metrics()\n",
    "performance.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect patient predictions: 219/219\n",
      "\n",
      "Extracted predictions shape: (219, 17)\n",
      "C-index   0.671\n",
      "Ctd       0.672\n",
      "IBS       0.325\n",
      "INBLL     1.742\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on TEST set (not validation)\n",
    "performance = utils.Evaluation(\n",
    "    model=gpu_multisurv, \n",
    "    dataset=dataloaders['test'].dataset,  # Note: 'test' not 'val'\n",
    "    device=device\n",
    ")\n",
    "performance.compute_metrics()\n",
    "performance.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watermark <a class='tocSkip'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark --iversions\n",
    "%watermark -v\n",
    "print()\n",
    "%watermark -u -n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of the page](#Top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "197px",
    "width": "372px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "236px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
